{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五章 关键词提取算法\n",
    "\n",
    "## 5.1 关键词提取技术概述\n",
    "\n",
    "关键词提取算法一般也可以分为有监督和无监督两类：\n",
    "\n",
    "+ 有监督的关键词提取：主要是通过分类的方式进行，通过构建一个较为丰富和完善的词表，然后通过判断每个文档与词表中每个词的匹配程度，以类似打标签的方式，达到关键词提取的效果 。 \n",
    "\n",
    "+ 无监督的关键词提取： 既不需要一张人工生成、维护的词表，也不需要人工标准语料辅助进行训练。本章主要介绍TF-IDF算法、TextRank算法和主题模型算法(包括LSA、LSI、LDA 等)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 TF/IDF 算法\n",
    "\n",
    "TF-IDF算法由两部分组成:TF算法以及IDF算法：\n",
    "\n",
    "+ TF算法是统计一个词在一篇文档中出现的频次，其基本思想是，一个词在文档中出现的次数越多，则其对文档的表达能力也就越强。 \n",
    "\n",
    "+ IDF 算法则是统计一个词在文档集的多少个文档中出现，其基本的思想是，如果一个词在越少的文档中出现，则其对文档的区分能力也就越强 。\n",
    "\n",
    "TF-IDF算法结合使用的原因：TF仅衡量词的出现频次，但是没有考虑到词的对文档的区分能力，IDF则是相反，强调的是词的区分能力，但是一个词既然能在一篇文档中频繁出现，那说明这个词能够很好地表现该篇文档的特征。于是，学者们将这两种算法综合进行使用构成 TF-IDF 算法，从词频、逆文档频次两个角度对词的重要性进行衡量。举例如下：\n",
    "\n",
    "世界献血日，学校团体、献血服务志愿者等可到血液中心参观检验加工过程，我们会对检验结果进行公示，同时血液的价格也将进行公示 。\n",
    "\n",
    "“献血” “血液” “进行” “公示”等词出现的频次均为2，如果从TF算法的角度，他们对于这篇文档的重要性是一样的。但是实际上明显“血液” “献血”对这篇文档来说更关键。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF计算公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tf_{ij}=\\frac {n_{ij}}{\\sum_k n_{kj}}$$\n",
    "\n",
    "$$tf(word)=\\frac {(word在文档中出现的次数)}{文档总词数}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n_{ij}$表示词$i$在文档$j$中的出现频次，但是仅用频次来表示， 分母部分就是统计文档中每个词出现次数的总和，也就是文档的总词数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“献血”一词出现次数为2，文档的总词数为30，则$tf(献血)=\\frac {n(献血)}{n(总)}=\\frac {2}{30} \\approx 0.067$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF计算公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$idf_{i}=log (\\frac {|D|}{1+|D_i|})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$|D|$为文档集中的总文档，$|D_i|$为文档集中出现词$i$的文档数量。分母加1是采用了拉普拉斯平滑避免有部分新的词没有在语料库中出现过而导致分母为零的情况出现，增强算法的健壮性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现假设我们具有的文档集有1000篇文档，其中出现“献血” “血液” “进行” “公示”的文档数分别为10、15、100、50，idf (献血) = log (1000/10) = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF计算公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tf\\times idf(i,j)=tf_{ij} \\times idf_i= \\frac {n_{ij}}{\\sum_k n_{kj}} \\times log(\\frac {|D|}{1+|D_i|})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank算法\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank 算法的基本思想来源于Google的PageRank算法。\n",
    "\n",
    "PageRank算法是Google创始人拉里·佩奇和谢尔盖·布林于1997年构建早期的搜索系统原型时提出的链接分析算法，该算法是他们用来评价搜索系统过覆盖网页重要性的一种重要方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PageRank 算法是一种网页排名算法，基本思想：\n",
    "\n",
    "+ 链接数量。一个网页被越多的其他网页链接，说明这个网页越重要。\n",
    "\n",
    "+ 链接质量。一个网页被一个越高权值的网页链接，也能表明这个网页越重要 。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PageRank算法计算公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ S(V_i) = (1 - d) + d * \\sum_{j \\varepsilon In(V_i)} \\frac{1}{|Out(V_j)|} S(V_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $S(V_i)$:第i个网页的价值\n",
    "\n",
    "+ $In(V_i)$：由链接到i的网页组成的集合\n",
    "\n",
    "+ $Out(V_j)$从j网页出去的网页组成的集合\n",
    "\n",
    "+ $|Out(V_j)|$：集合的网页数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In(Vi)表示网页Vi的所有入链的集合，Vi表示某个网页，Vj表示链接到Vi的网页（即Vi的入链），S(Vi)表示网页Vi的PR值，Out(Vj)表示$V_j$指向的链接，d表示阻尼系数，是用来克服这个公式中“d *”后面的部分的固有缺陷用的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PageRank算法的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Markdown](http://i2.tiimg.com/611786/c6467b9753ed4eb7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图中，可以认为是A-E 5个网页构成的图，节点与节点之间存在着边，图中存在箭头，此时的图称为“有向图”。\n",
    "\n",
    "B到C的箭头表示B网页有到C网页的链接，而A、B之间的箭头表示A、B网页之间相互链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W= \\begin{bmatrix} \n",
    "0 & 1/3 & 0 & 0 & 0 \\\\\n",
    "1/2 & 0 & 0 & 0 & 1 \\\\ \n",
    "0 & 1/3 & 0 & 0 & 0\\\\\n",
    "1/2 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1/3 & 0 & 1 & 0 \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第i行表示进入到第i个节点的概率分布，而第j列，表示第j个节点的出节点概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以用一个5维列向量S表示5个节点的概率初始值,也就是一个随机向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(S)^0=(1/5 \\quad 1/5 \\quad 1/5 \\quad 1/5 \\quad 1/5)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "S(V_A)=1-d+d·\\sum_{j\\in {In(V_A)}} \\frac {1}{|Out(V_j)|}S(V_j)&=1-0.85+0.85*(\\frac{1}{|Out(V_B)|}S(V_B))\\\\\n",
    "&=0.15+0.85*(\\frac {1}{3}*\\frac{1}{5})=0.207\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为何此处计算的是$S(V_B)$？因为此处算的是指向A的网页，即B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "S(V_B)&=1-d+d·\\sum_{j \\in {In(V_B)}} \\frac{1}{|Out(V_j)|}S(V_j)\\\\\n",
    "&=1-0.85+0.85*(\\frac{1}{|Out(V_A)|}+\\frac{1}{|Out(V_E)|}S(V_E))\\\\\n",
    "&=0.15+0.85*(\\frac{1}{2}*\\frac{1}{5}+1*\\frac {1}{5})=0.405\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一轮："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(S)^1=(0.207 \\quad 0.405 \\quad 0.207 \\quad 0.235 \\quad 0.377)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与利用公式分别计算的一致，迭代100轮："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(S)^{100}=(0.405 \\quad 0.898 \\quad 0.405 \\quad 0.322 \\quad 0.678)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代200轮："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(S)^{200}=(0.405 \\quad 0.898 \\quad 0.405 \\quad 0.322 \\quad 0.678)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经收敛了，标准化后："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S=(0.149 \\quad 0.322 \\quad 0.149 \\quad 0.119 \\quad 0.251)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank算法计算公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ S(V_i) = (1 - d) + d * \\sum_{j \\varepsilon In(V_i)} \\frac{\\omega_{ij}}{\\sum_{V_j \\varepsilon Out(V_j) }\\omega_{ij} } WS(V_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该公式仅仅比PageRank多了一个权重项Wji，用来表示两个节点之间的边连接有不同的重要程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jieba分词TextRank：\n",
    "\n",
    "+ 对每个句子进行分词和词性标注处理。\n",
    "\n",
    "+ 过滤掉除指定词性外的其他单词，过滤掉出现在停用词表的单词，过滤掉长度小于2的单词。\n",
    "\n",
    "+ 将剩下的单词中循环选择一个单词，将其与其后面4个单词分别组合成4条边。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如：         ['有','媒体', '曝光','高圆圆', '和', '赵又廷','现身', '台北', '桃园','机场','的', '照片']\n",
    "        对于‘媒体‘这个单词，就有（'媒体', '曝光'）、（'媒体', '高圆圆'）、（'媒体', '和'）、（'媒体', '赵又廷'）4条边，且每条边权值为1，当这条边在之后再次出现时，权值再在基础上加1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 把给定的文本T按照完整句子进行分割，$T=[S_1,S_2,......,S_m]$，\n",
    "\n",
    "2. 对于每个句子，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即，Si=$[pi_1，pi_2，...，pi_n]$\n",
    "\n",
    "3. 构建候选关键词图G = (V,E)，其中V为节点集，由2生成的候选关键词组成，然后采用共现关系（co-occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，即：$[p_1,p_2,...,p_k][p_2,p_3,...,p_{k+1}]$等都是一个个的窗口，在一个窗口中如果两个单词同时出现，则认为对应单词节点间存在一个边，\n",
    "\n",
    "4. 根据PageRank原理中的衡量重要性的公式，初始化各节点的权重，然后迭代计算各节点的权重，直至收敛，\n",
    "\n",
    "5. 对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词，\n",
    "\n",
    "6. 由（5）得到最重要的T个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。例如，文本中有句子“Matlab code for plotting ambiguity function”，如果“Matlab”和“code”均属于候选关键词，则组合成“Matlab code”加入关键词序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.4 LSA/LSI/LDA 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在某些场景，基于文裆本身的关键词提取还不是非常足够，有些关键词并不一定会显式地出现在文档当中，如一篇讲动物生存环境的科普文，通篇介绍了狮子老虎鲸鱼等各种动物的情况，但是文中并没有显式地出现动物二字，这种情况下，前面的两种算法显然不能提取出动物这个隐含的主题信息，这时候就需要用到主题模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 LSA/LSI 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA  ( Latent Semantic Analysis ，潜在语义分析 ) 和 LSI ( Latent Semantic Index ， 潜在语义索引)， 二者通常被认为是同一种算法，只是应用的场景略有不同， LSA 是在需要构建的相关任务中的叫法 。 可以说， LSA 和 LSI 都是对文档的潜在语义进行分析，但是潜在语义索引在分析后，还会利用分析的结果建立相关的索引 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA 的主要步骤如下: \n",
    "\n",
    "+  使用 BOW 模型将每个文档表示为向量 。\n",
    "\n",
    "+ 将所有的文档词向量拼接起来构成词一文档矩阵$(m \\times n)$\n",
    "\n",
    "+  对词 一文档矩 阵进行奇异值分解( SVD ) 操作$ ([m \\times r] ·  [r \\times r]  ·  [r\\times n] ) $。\n",
    "\n",
    "+ 根据SVD的结果，将词-文档矩阵映射到一个更低维度k ( [m x k]  ·  [k x k]  ·  [k x n] ,  0  < k  < r ) 的近似 SVD 结果，每个词和文档都可以表示为 k 个主题构成的空 间中的一个点，通过计算每个词和文档的相似度(相似度计算可以通过余弦相似度或者是 KL 相似度进行)，可以得到每个文档 中对每个词的相似度结果，取相似度最高的一个词 即为文档的关键词 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA存在着对词的频率分布不敏感、物理解释性薄弱等问题，有学者对此进行改进，提出LDA (Latent Dirichlet Allocation ，隐含狄利克雷分布)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 LDA算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出,是一种主题模型，它可以将文档中的每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。\n",
    "\n",
    "一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人类怎么生成文档的呢？LDA的这三位作者在原始论文中给了一个简单的例子。比如假设事先给定了这几个主题：Arts、Budgets、Children、Education，然后通过学习训练，获取每个Topic对应的词语。如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Markdown](http://i2.tiimg.com/611786/91c9c213ad2f626f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后以一定的概率选取上述某个主题，再以一定的概率选取那个主题下的某个单词，不断地重复这两部，最终生成如下图所示地一篇文章（其中不同颜色地词语对应上图中不同主题下的词）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Markdown](http://i2.tiimg.com/611786/fb3d9329c83df520.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而当我们看到一篇文章后，往往喜欢推测这篇文章是如何生成的，我们可能会认为作者先确定这篇文章的几个主题，然后围绕这几个主题遣词造句，表达成文。\n",
    "LDA就是要干这事：根据给定的一篇文档，推测其主题分布。\n",
    "通俗来说，可以假定认为人类是根据上述文档生成过程写成了各种各样的文章，现在某小撮人最想让计算机利用LDA干一件事：你计算机给我推测分析网络上歌各篇文章分别都写了些啥，且各篇文章中各个主题出现的概率大小（主题分布）是啥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 四个分布 ： 二项分布、多项分布、beta分布、Dirichlet分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二项分布：二项分布是从伯努利分布推进的。伯努利分布，又称两点分布或0-1分布，是一个离散型的随机分布，其中的随机变量只有两类取值，非正即负。⽽二项分布即重复 n 次的伯努利试验。简⾔言之，只做一次实验，是伯努利利分布，重复做了了 n 次，是二项分布。二项分布的概率密度函数为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(K=k)=\\begin{pmatrix} n \\\\ k \\\\ \\end{pmatrix} p^k (1-p)^{n-k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多项分布：是二项分布扩展到多维的情况。\n",
    "\n",
    "多项分布是指单次试验中的随机变量的取值不再是0-1 的，而是有多种离散值可能（ 1,2,3...,k ）。比如投掷 6 个⾯面的骰⼦子实验， N 次实验结果服从 K=6 的多项分布。当然啦，他们加起来的 P 应该是等于 1 的。\n",
    "\n",
    "多项分布的密度函数为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_1,x_2,\\dots,x_k;n,p_1,p_2,\\dots,p_k)=\\frac {n!}{x_1! \\dots x_k!} p_1^{x_1} \\dots p_k^{x_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beta分布，二项分布的共轭先验分布，给定参数a>0和b>0，取值范围为[0,1]的随机变量x的概率密度函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x;\\alpha,\\beta)=\\frac{1}{B(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{B(\\alpha,\\beta)}=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)},\\Gamma(z)=\\int_0^{\\infty}{t^{z-1}e^{-t}}\\,{\\rm d}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dirichlet分布，是beta分布在高维度上的推广"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x_1,x_2,\\dots,x_k;\\alpha_1,\\alpha_2,\\dots,\\alpha_k)=\\frac{1}{B(\\alpha)} \\prod_{i=1}^k x_i^{\\alpha^i-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$B(\\alpha)=\\frac{\\prod_{i=1}^k \\Gamma(\\alpha^i)}{\\Gamma(\\sum_{i=1}^k \\alpha^i)},\\sum x_i=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 几个主题模型（循序渐进）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于文档$w=(w_1,w_2,\\dots,w_N)$，用$p(w_n)$表示词$w_n$的先验概率，生成文档w的概率为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(w)=\\prod_{n=1}^N p(w_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixture of unigrams model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该模型的生成过程是：给某个文档先选择一个主题 z ，再根据该主题生成文档，该文档中的所有词都来自一个主题。假设主题有 z1,z2,z3,...zk ，生成文档的概率为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(w)=p(z_1)\\prod_{n=1}^Np(w_n|z_1)+\\dots+p(z_k)\\prod_{n=1}^Np(w_n|z_k)=\\sum_zp(z)\\prod_{n=1}^Np(w_n|z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 贝叶斯模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\theta)=P(\\theta)P(y|\\theta)$$\n",
    "$$P(\\theta|y)=\\frac {P(\\theta,y)}{P(y)}=\\frac {P(y|\\theta)P(\\theta)}{P(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pLSA模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "刚刚的 mix unigram 模型里面，一篇文章只给了一个主题。但是现实⽣生活中，一篇⽂文章可能有多个主题，只不过是『出现的几率』不一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定一共有K个可选的主题，有V个可选的词，咱们来玩一个扔骰子的游戏。\n",
    "\n",
    "+ 1.假设你没一篇文档会制作一颗K面的“文档-主题”骰子（仍此骰子能得到K个主题中的任意一个），和K个V面的“主题-词项”骰子（每个骰子对应一个主题，K个骰子对应之前的K的主题，且骰子的每一面对应要选择的词项，V个面对应着V个可选的词）。如图所示：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i2.tiimg.com/611786/717f8397b9e8c01c.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + 2.每写一个词，先扔“文档-主题”骰子选择主题，得到主题的结果后，使用和主题对应结果1的那颗“主题-词项”骰子，仍该骰子选择要写的词。\n",
    " \n",
    " 先仍“文档-主题”的骰子，假设（以一定的概率）得到的主题是教育，所以下一步便是仍教育主题骰子，（以一定的概率）得到教育主题骰子对应的某个词：大学。为何偏偏选取教育这个主题呢？其实是随机选取的，只是这个随机遵循一定的概率分布。比如这3个主题的概率分布便是{教育：0.5，经济：0.3，交通：0.2}，我们把各个主题z在文档d中出现的概率分布称之为主题分布，且是一个多选分布。\n",
    " \n",
    "  从主题分布中随机抽取出教育主题后，依然面对着3个词：大学、老师、课程。这3个词的概率分布便是{大学：0.5，老师：0.3，课程：0.2}，我们把各个词语w在主题下出现的概率分布称之为词分布，这个词分布也是一个多项分布。\n",
    " \n",
    "  所以，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+  3.最后不停的重复仍“文档-主题”骰子和“主题-词项”骰子，重复N次（产生N个词），完成一篇文档，重复这产生一篇文档的方法M次，则完成M篇文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述过程抽象出来即是PLSA的文档生成模型。我们定义："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $P(d_i)$表示海量文档中某篇文档被选中的概率。\n",
    "\n",
    "+ $P(w_j|d_j)$表示词$w_j$在给定文档$d_i$中出现的概率。对于每个词语，用它在文档中出现的次数初一文档中词语总的数目便是它在文档中出现的概率$P(w_j|d_i)$。\n",
    "\n",
    "+ $P(z_k|d_i)$表示具体某个主题$z_k$在给定文档$d_i$下出现的概率。\n",
    "\n",
    "+ $P(w_j|z_k)$表示具体某个词$w_j$在给定主题$z_k$下出现的概率，与主题关系越密切的词。其条件概率$P(w_j|z_k)$越大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用上述的第1、3、4个概率，我们便可以按照如下的步骤得到“文档-词项”的生成模型：\n",
    "\n",
    "+ 1.按照概率$P(d_i)$选择一篇文档$d_i$\n",
    "\n",
    "+ 2.选的文档$d_i$后，从主题分布中按照概率$P(z_k|d_i)$选择一个隐含的主题类别$z_k$\n",
    "\n",
    "+ 3.选的$z_k$后，从词项分布中按照概率$P(w_j|z_k)$选择一个词$w_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过观测，得到了『知道主题是什么，我就用什么单词』的文本⽣生成模型，那么，根据贝叶斯定律，我们就可以反过来推出『看见用了什么单词，我就知道主题是什么』\n",
    "\n",
    "如下图所示（图中被染色的d、w表示可观测变量，未被染色的z表示未知的隐变量，N表示一篇文档中总共N个单词，M表示M篇文档）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i2.tiimg.com/611786/9b8276aa989ed347.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图中，文档d和词w是我们得到的样本，可观测得到，所以对于任意一篇文档，其$P(w_j|d_i)$是已知的。\n",
    "\n",
    "从而可以根据大量已知的文档-词项信息$P(w_j|d_i)$，训练出文档-主题$P(z_k|d_i)$和主题-词项$P(w_j|z_k)$，如下公式所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_j|d_i)=\\sum _{k=1}^K P(w_j|z_k)P(z_k|d_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "故得到文档中每个词的生成概率为：\n",
    "$$P(d_i,w_j)=P(d_i)P(w_j|d_i)=P(d_i)\\sum_{k=1}^KP(w_j|z_k)P(z_k|d_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$P(d_i)$可事先计算求出，而$P(w_j|z_k)$和$P(z_k|d_i)$未知，所以$\\theta = (P(w_j|z_k),P(z_k|d_i))$就是我们要估计的参数值，通俗点来说，就是要最大化这个$\\theta$\n",
    "\n",
    "因为待估计的参数中含有隐变量z，所以我们可以考虑EM算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1.由于$P(w_j|z_k)$和$P(z_k|d_i)$未知，所以我们用EM算法去估计$\\theta = (P(w_j|z_k),P(z_k|d_i))$这个参数的值。\n",
    "\n",
    "+ 2.而后，用$\\Phi _{k,j}$表示词项$w_j$出现在主题$z_k$中的概率，即$P(w_j|z_k)=\\Phi_{k,j}$，用$\\theta_{i,k}$表示主题$z_k$出现在文档$d_i$中的概率，即$P(z_k|d_i)=\\theta_{i,k}$，从而把$P(w_j|z_k)$转换成了“主题-词项”矩阵$\\Phi$（主题生成词），把$P(z_k|d_i)$转换成了“文档-主题”矩阵$\\theta$（文档生成主题）。\n",
    "\n",
    "+  3.最终求解出$\\Phi_{k,j}$、$\\theta_{i,k}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM算法求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单词之间相互独立，因此文档$d_i$中所有单词的生成概率为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(d_i,\\vec{w})=\\prod_{j=1}^N p(d_i,w_j)^{n(d_i,w_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$\\vec{w}$是一个N维向量$(n(d_i,w_1),n(d_i,w_2),\\cdots,n(d_i,w_N))$，每个分量是单词$w_j$在文档$d_i$中的出现次数，N是词汇表V的大小。\n",
    "\n",
    "文档之间同样是相互独立的，因此语料的生成概率为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(D)&=\\prod_{i=1}^M p(d_i,\\vec{w})\\\\\n",
    "&=\\prod_{i=1}^M \\prod_{j=1}^N p(d_i,w_j)^{n(d_i,w_j)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用最大似然估计，最大化$\\log p(D)$，对应的$p(z_k|d_i)$和$p(w_j|z_k)$就是我们要找的最优参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\iota  =\\log p(D)&=\\log \\prod_{i=1}^M \\prod_{j=1}^N p(d_i,w_j)^{d_i,w_j}\\\\\n",
    "&=\\sum_{i=1}^M \\sum_{j=1}^N n(d_i,w_j) \\log p(d_i,w_j)\\\\\n",
    "&=\\sum_{i=1}^M \\sum_{j=1}^N n(d_i,w_j) \\log[p(d_i) \\sum_{k=1}^K p(w_j|z_k) p(z_k|d_i)]\\\\\n",
    "&=\\sum_{i=1}^M \\sum_{j=1}^N n(d_i,w_j)[\\log p(d_i)+\\log \\sum_{k=1}^K p(w_j|z_k) p(z_k|d_i)]\\\\\n",
    "&=\\sum_{i=1}^M(\\sum_{j=1}^N n(d_i,w_j) \\log p(d_i) + \\sum n(d_i,w_j) \\log \\sum_{k=1}^K p(w_j|z_k) p(z_k|d_i))\\\\\n",
    "&=\\sum_{i=1}^M(n(d_i) \\log p(d_i) + \\sum_{j=1}^N n(d_i,w_j) \\log \\sum_{k=1}^K p(w_j|z_k) p(z_k|d_i))\\\\\n",
    "&=\\sum_{i=1}^M n(d_i)[\\log p(d_i) + \\sum_{j=1}^N \\frac {n(d_i,w_j)}{n(d_i)} \\log \\sum_{k=1}^K p(w_j|z_k) p(z_k|d_i)]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为文档长度$n(d_i)$和文档概率$p(d_i)$可以单独计算，将其去掉不会影响似然函数的最优化。所以在后面部分，我们统一将$\\iota$写成如下形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\iota = \\sum_{i=1}^M \\sum_{j=1}^N n(d_i,w_j) \\log \\sum_{k=1}^K p(w_j|z_k)p(z_k|d_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将对数函数包含参数和的情况进行一般化，首先目标函数可以写成下式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = arg \\max_{\\theta} \\sum_{i=1}^m \\log p(x^{(i)};\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(x^{(i)};\\theta)$是在给定参数$\\theta$下第i个样本的产生概率，有些情况下这个概率值收到一些隐含因素的影响，如主题模型中文档生成过程中的主题，将其用$z^{(i)}$表示，此时$p(x^{i};\\theta)$需要累加所有的$p(x^{(i)},z^{(i)};\\theta)$，而优化目标也变成："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = arg \\max_{\\theta} \\sum_{i=1}^m \\log p(x^{(i)};\\theta) = arg \\max_{\\theta} \\sum_{i=1}^m \\log \\sum_   {z^{(i)}} p(x^{(i)},z^{(i)};\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jensen不等式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E[f(x)] \\geq f(E(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若函数$f$是凹函数，上述不等式符号相反。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Jensen不等式对上面的对数似然函数进行缩放如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\sum_{i=1}^m \\sum_{z^{(i)}} p(x^{(i)},z^{(i)};\\theta) &=\\sum_{i=1}^m \\log \\sum_{z^{(i)}}Q(z^{(i)}) \\frac {p(x^{(i)},z^{(i)};\\theta)}{Q(z^{(i)}) }  (1)\\\\\n",
    "& \\geq \\sum_{i=1}^m \\sum_{z^{(i)}}Q(z^{(i)}) \\frac {p(x^{(i)},z^{(i)};\\theta)}{Q(z^{(i)}) }  (2)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$\\sum_{z^{(i)} Q(z^{(i)})=1$，即隐变量$z^{(i)}$的概率分别。在上式中，将$Q(z^{(i)})$看作随机变量$\\frac {p(x^{(i)},z^{(i)};\\theta)}{Q(z^{(i)})}$的概率，函数$f$对应$\\log$函数，根据Jensen不等式得到上面不等式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要满足Jensen不等式的等号。需要："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac {p(x^{(i)},z^{(i)};\\theta)}{Q(z^{(i)}) }=c ,c为常量$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\implies p(x^{(i)},z^{(i)};\\theta)=cQ(z^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\implies \\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\\theta)=c\\sum_{z^{(i)}}Q(z^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\implies \\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\\theta)=c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，我们可以得到等号成立需要满足的条件是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(z^{(i)})=\\frac {p(x^{(i)},z^{(i)};\\theta)}{\\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\\theta)}=p(z^{(i)}|x^{(i)};\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即隐变量的后验概率，将$Q(z^{(i)})$带入(2)式，找出使其最大的$\\theta$，这时我们确定该$\\theta$值处的一个下界，同样我们调整$Q(z^{(i)})$使这个下界最大化，同时也在尝试提升我们的对数似然函数，即我们需要最大化下式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&arg \\max_{\\theta}\\sum_{i=1}^m \\sum_{z^{(i)}}Q(z^{(i)}) \\frac {p(x^{(i)},z^{(i)};\\theta)}{Q(z^{(i)}) }\\\\\n",
    "&\\implies arg \\max_{\\theta}  \\sum_{i=1}^m \\sum_{z^{(i)}}Q(z^{(i)}) p(x^{(i)},z^{(i)};\\theta)\\\\\n",
    "&=arg \\max_{\\theta}  \\sum_{i=1}^m \\sum_{z^{(i)}}) p(z^{(i)}|x^{(i)};\\theta) p(x^{(i)},z^{(i)};\\theta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式可以理解为对数似然函数$p(x^{(i)},z^{(i)};\\theta)$基于条件概率$p(z^{(i)}|x^{(i)};\\theta) $的期望，正好对应EM算法的E步，而最大化过程，正好对应M步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\iota &= \\sum_{i=1}^M \\sum_{j=1}^N n(d_i,w_j) \\log \\sum_{k=1}^K p(w_j|z_k)p(z_k|d_i)\\\\\n",
    "& \\geq \\sum_{i=1}^M \\sum_{j=1}^N n(d_i,w_j) \\sum_{k=1}^K Q(z^{(i)}) \\log \\frac{p(w_j|z_k)p(z_k|d_i)}{Q(z_k)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面的推导得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(z_k)=p(z_k|d_i.w_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后开始EM算法迭代过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E步：计算联合概率分布的条件概率期望"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）计算隐变量的后验概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(z_k|d_i,w_j)&=\\frac{p(z_k,d_i,w_j)}{\\sum_{k=1}^K p(z_k,d_i,w_j)}\\\\\n",
    "&=\\frac{p(w_j|z_k)p(z_k|d_i)}{\\sum_{k=1}^K p(w_j|z_k)p(z_k|d_i)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）将隐变量的后验概率带入，得到期望函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\iota^c&=\\sum_{i=1}^M \\sum_{j=1}^N n(d_i,w_j) \\sum_{k=1}^K p(z_k|d_i,w_j)\\log p(z_k,d_i,w_j)\\\\\n",
    "&=\\sum_{i=1}^M \\sum_{j=1}^N n(d_i,w_j) \\sum_{k=1}^K p(z_k|d_i,w_j)\\log p(w_j|z_k)p(z_k|d_i)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M步：最大化期望函数，这是一个带约束的最优化问题："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{j=1}^N p(w_j|z_k)=1$$\n",
    "$$\\sum_{k=1}^K p(z_k|d_i)=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以采用1拉格朗日乘法求解，拉格朗日函数如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H=\\iota^c + \\sum_{k=1}^K \\tau_k(1-\\sum_{j=1}^N p(w_j|z_k))+\\sum_{i=1}^M \\rho(1-\\sum_{k=1}^Kp(z_k|d_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别求偏导，令其结果等于0，最终可以估计出参数$p(w_j|z_k)$和$p(z_k|d_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(w_j|z_k)=\\frac {\\sum_{i=1}^M n(d_i,w_j)p(z_k|d_i,w_j)}{\\sum_{j=1}^N\\sum_{i=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}$$\n",
    "\n",
    "$$p(z_k|d_i)=\\frac {\\sum_{j=1}^N n(d_i,w_j)p(z_k|d_i,w_j)}{n(d_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ p(z_{k}|d_{i},w_{j};\\theta_{t}) & =\\frac {p(z_{k},d_{i},w_{j})} {p(d_{i},w_{j})}\\\\\n",
    "&= \\frac {p(z_k,d_i)·p(w_j,z_k)}{p(d_i,w_j)}\\\\\n",
    "& = \\frac {p(z_k|d_i)·p(w_j|z_k)·p(d_i)}{\\sum_z p(z|d_i)·p(w_j|z)·p(d_i)}\\\\\n",
    "&=\\frac {p(z_k|d_i)·p(w_j|z_k)}{\\sum_z p(z|d_i)·p(w_j|z)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个步骤中，我们假定所有的$p(z_k|d_i)$和$p(w_j|z_k)$都是已知的，初始时随机赋值，后面迭代的过程中取前一轮M步骤中得到的参数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M步：最大化Complete data对数似然函数的期望（即把其中与z相关的部分积分掉），"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1.按照先验概率$P(d_i)$选择一篇文档$d_i$\n",
    "\n",
    "+ 2.从狄利克雷分布(即Dirichlet)$\\alpha$中选取生成文档$d_i$的主题分布$\\theta_i$，换言之，主题分布$\\theta_i$由超参数为$\\alpha$的Dirichlet分布生成\n",
    "\n",
    "+ 3.从主题的多项式分布$\\theta_i$中取样生成文档$d_i$第j个词的主题$z_{i.j}$\n",
    "\n",
    "+ 4.从狄利克雷分布(即Dirichlet分布)$\\beta$中取样生成主题$z_{i,j}$对应的词语分布$\\Phi_{z_{i,j}}$，换言之，词语分布$\\Phi_{z_{i,j}}$由参数$\\beta$的Dirichlet分布生成\n",
    "\n",
    "+ 5.从词语的多项式分布$\\Phi_{z_{i,j}}$中采样最终词语$w_{i,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gibbs采样算法求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Gibbs采样算法求解LDA的方法中，我们的$\\alpha$，$\\beta$是已知的先验输入,我们的目标是得到各个$z_{dn}$，$w_{kn}$对应的整体$\\vec z$，$\\vec w$的概率分布，即文档主题的分布和主题词的分布。由于我们是采用Gibbs采样法，则对于要求的目标分布，我们需要得到对应分布各个特征维度的条件概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体到我们的问题，我们的所有文档联合起来形成的词向量$\\vec w$是已知的数据，不知道的是语料库主题$\\vec z$ 的分布。假如我们可以先求出w，z的联合分布$p(\\vec w,\\vec z)$，进而可以求出某一个词$w_i$对应主题特征$z_i$的条件概率分别$p(z_i=k|\\vec w,\\vec z_{\\lnot i})$。其中，$\\vec z_{\\lnot i}$代表去掉下标为i的词后的主题分别，有了条件概率分别$p(z_i=k|\\vec w,\\vec z_{\\lnot i})$，我们就可以进行Gibbs采样，最终在Gibbs采样收敛后得到第i个词的主题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们通过采样得到了所有词的主题,那么通过统计所有词的主题计数，就可以得到各个主题的词分布。接着统计各个文档对应词的主题计数，就可以得到各个文档的主题分布。\n",
    "\n",
    "以上就是Gibbs采样算法求解LDA的思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们简化下Dirichlet分布的表达式,其中$\\Delta (\\alpha)$是归一化参数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Dirichlet(\\vec p|\\vec \\alpha)=\\frac {\\Gamma(\\sum_{k=1}^K \\alpha_k)}{\\prod_{k=1}^K \\Gamma (\\alpha_k)} \\prod_{k=1}^K p_k^{\\alpha_k-1}=\\frac{1}{\\Delta(\\vec \\alpha)} \\prod_{k=1}^K p_k^{\\alpha_k-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们先计算下第d个文档的主题的条件分布$p(\\vec z_d|\\alpha)$，$\\alpha \\to \\theta_d \\to \\vec z_d$组成了Dirichlet-multi共轭,利用这组分布，计算$p(\\vec z_d|\\vec \\alpha)$如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(\\vec z_d|\\vec \\alpha)&=\\int p(\\vec z_d|\\vec theta_d)p(\\theta_d|\\vec \\alpha)d\\vec \\theta_d\\,\\\\\n",
    "&=\\int \\prod_{k=1}^K p_k^{n_d^{(k)}} Dirichlet(\\vec \\alpha) d\\vec \\theta_d\\,\\\\\n",
    "&=\\int \\prod_{k=1}^{n_d^{(k)}} \\frac {1}{\\Delta (\\vec \\alpha)} \\prod_{k=1}^K p_k^{\\alpha_k -1} d \\vec \\theta_d \\,\\\\\n",
    "&= \\frac{1}{\\Delta (\\vec \\alpha)} \\int \\prod_{k=1}^K p_k^{n_d^{(k)}+\\alpha_k -1} d\\vec \\theta_d \\,\\\\\n",
    "&=\\frac {\\Delta (\\vec n_d + \\vec \\alpha)}{\\Delta (\\vec \\alpha)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，在第d个文档中，第k个主题的词的个数表示为：$n_d^{(k)}$，对应的多项分布的计数可以表示为 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vec n_d=(n_d^{(1)},n_d^{(2)},\\ldots,n_d^{(K)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了单一一个文档的主题条件分布，则可以得到所有文档的主题条件分布为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\vec z|\\vec \\alpha)=\\prod_{d=1}^M p(\\vec z_d|\\vec \\alpha)=\\prod_{d=1}^M \\frac {\\Delta(\\vec n_d+\\vec \\alpha)}{\\Delta (\\vec \\alpha)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的方法，可以得到，第k个主题对应的词的条件分布$p(\\vec w|\\vec z,\\vec \\eta)$为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\vec w|\\vec z,\\vec \\eta)=\\prod_{k=1}^K p(\\vec w|\\vec z,\\vec \\eta)=\\prod_{k=1}^K \\frac{\\Delta(\\vec n_k+\\vec \\eta)}{\\Delta(\\vec \\eta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，第k个主题中，第v个词的个数表示为：$n_k^{(v)}$， 对应的多项分布的计数可以表示为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vec n_k=(n_k^{(1)},n_k^{(2)},\\ldots,n_k^{(V)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　最终我们得到主题和词的联合分布$p(\\vec w,\\vec z|\\vec \\alpha,\\vec \\eta)$如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\vec w|\\vec z) \\propto p(\\vec w,\\vec z|\\vec \\alpha,\\vec \\eta)=p(\\vec z|\\vec \\alpha)p(\\vec w|\\vec z,\\vec \\eta)=\\prod_{d=1}^M \\frac{\\Delta(\\vec n_d+\\vec \\alpha)}{\\Delta(\\vec \\alpha)} \\prod_{k=1}^K \\frac{\\Delta(\\vec n_k+\\vec \\eta)}{\\Delta(\\vec \\eta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了联合分布，现在我们就可以求Gibbs采样需要的条件分布$p(z_i=k|\\vec w,\\vec z_{\\lnot i})$了。需要注意的是这里的i是一个二维下标，对应第d篇文档的第n个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于下标i，由于它对应的词$w_i$是可以观察到的，因此我们有："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(z_i=k|\\vec w,\\vec z_{\\lnot i}) \\propto p(z_i=k,w_i=t|\\vec w_{\\lnot i},\\vec z_{\\lnot i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$z_i=k,w_i=t$，它只涉及到第d篇文档和第k个主题两个Dirichlet-multi共轭，即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vec \\alpha \\to \\vec \\theta_d \\to \\vec z_d$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vec \\eta \\to \\vec \\beta_k \\to \\vec w_{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其余的M+K−2个Dirichlet-multi共轭和它们这两个共轭是独立的。如果我们在语料库中去掉$z_i,w_i$，并不会改变之前的M+K个Dirichlet-multi共轭结构，只是向量的某些位置的计数会减少，因此对于$\\vec \\theta_d,\\vec \\theta_k$，对应的后验分布为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\vec \\theta_d|\\vec w_{\\lnot i},\\vec z_{\\lnot i})=Dirichlet(\\vec \\theta_d|\\vec n_{d,\\lnot i}+\\vec \\alpha)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\vec \\theta_k|\\vec w_{\\lnot i},\\vec z_{\\lnot i})=Dirichlet(\\vec \\beta_k|\\vec n_{k,\\lnot i}+\\vec \\eta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在开始计算Gibbs采样需要的条件概率："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "p(z_i=k|\\vec w,\\vec z_{\\lnot i}) \\propto p(z_i=k,w_i=t|\\vec w_{\\lnot i},\\vec z_{\\lnot i})\n",
    "&=  \\int p(z_i=k,w_i=t,\\vec \\theta_d,\\vec \\beta_k|\\vec w_{\\lnot i},\\vec z_{\\lnot i}) d \\vec \\theta_d d \\vec \\beta_k \\,\\\\\n",
    "&=\\int p(z_i=k,\\vec \\theta_d|\\vec w_{\\lnot i},\\vec z_{\\lnot i})p(w_i=t,\\vec \\beta_k|\\vec w_{\\lnot i},\\vec z_{\\lnot i}) d \\vec \\theta_d d \\vec \\beta_k \\,\\\\\n",
    "&=\\int p(z_i=k|\\vec \\theta_d) p(\\vec \\theta_d|\\vec w_{\\lnot i},\\vec z_{\\lnot i}) p(w_i=t|\\vec \\beta_k)p(\\vec \\beta_k|\\vec w_{\\lnot i},\\vec z_{\\lnot i}) d \\vec \\theta_d d \\vec \\beta_k \\,\\\\\n",
    "&=\\int p(z_i=k|\\vec \\theta_d) Dirichlet(\\vec \\theta_d|\\vec n_{d,\\lnot i}+\\vec \\alpha) d \\vec \\theta_d \\,\\\\\n",
    "& * \\: \\int p(w_i=t|\\vec \\theta_k)Dirichlet(\\vec \\theta_k|\\vec n_{k,\\lnot i}+\\vec \\eta) d \\vec \\beta_k\\,\\\\\n",
    "&=\\int \\vec \\theta_{dk} Dirichlet(\\vec \\theta_d|\\vec n_{d,\\lnot i}+\\vec \\alpha) d \\vec \\theta_d \\int \\theta_{kt} Dirichlet(\\vec \\theta_k|\\vec n_{k,\\lnot i}+\\vec \\eta) d \\vec \\theta_k\\,\\\\\n",
    "&=E_{Dirichlet(\\theta_{dk})}(\\theta_{dk})E_{Dirichlet(\\theta_k)}(\\theta_{kt})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dirichlet分布的期望公式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_{Dirichlet(\\theta_d)}(\\theta_{dk})=\\frac {n_{d,\\lnot i}^k+\\alpha_k}{\\sum_{s=1}^K n_{d,\\lnot i}^s+\\alpha_s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_{Dirichlet}(\\theta_{kt})=\\frac{n_{k,\\lnot i}+\\eta}{\\sum_{f=1}^V n_{k,\\lnot i}^f+\\eta_f}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终我们得到每个词对应主题的Gibbs采样的条件概率公式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(z_i=k|\\vec w,\\vec z_{\\lnot i})=\\frac {n_{d,\\lnot i}^k+\\alpha_k}{\\sum_{s=1}^K n_{d,\\lnot i}^s+\\alpha_s} \\frac{n_{k,\\lnot i}^t+\\eta_t}{\\sum_{f=1}^V n_{k,\\lnot i}^f+\\eta_f}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了这个公式，我们就可以用Gibbs采样去采样所有词的主题，当Gibbs采样收敛后，即得到所有词的采样主题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用所有采样得到的词和主题的对应关系，我们就可以得到每个文档词主题的分布$\\theta_d$和每个主题中所有词的分布$\\beta_k$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面两个过程可以看到，LDA在PSLA的基础上，为主题分布和词分布分别加了两个Dirichlet先验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLSA与LDA的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ PLSA中，主题分布和词分布是唯一确定的，能明确的指出主题分布可能就是{教育：0.5，经济：0.3，交通：0.2}，词分布可能就是{大学：0.5，老师：0.3，课程：0.2}。\n",
    "\n",
    "+ 但在LDA中，主题分布和词分布不再是唯一确定不变，即无法确切给出。例如主题分布可能是{教育0.5，经济：0.2，交通：0.2}，到底是哪个我们不确定，因为它是随机的可变化。但再怎么变化，也依然服从一定的分布，即主题分布跟词分布由Dirichlet先验随机确定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLSA**中，主题分布和词分布确定后，以一定的概率（$P(z_k|d_i)、P(w_j|z_k)$）分别选取具体的主题和词项，生成好文档。而后根据生成好的文档反推其主题分布、词分布时，最终用EM算法（极大似然估计思想）求解出了两个未知但固定的参数的值：$\\Phi_{k,j}$（由$P(w_j|z_k)$转换而来）和$\\theta_{i,k}$（由$P(z_k|d_i)$转换而来）。\n",
    "\n",
    "文档d产生主题z的概率，由主题z产生单词w的概率都是两个固定的值。\n",
    "\n",
    "举个文档d产生主题z的例子。给定一篇文档d，主题分布是一定的，比如${P(z_i|d),i=1,2,3}$，可能就是{0.4，0.5，0.1}，表示$z_1、z_2、z_3$，这3个主题被文档d选中的概率都是个固定的值：$P(z_1|d)=0.4,P(z_2|d)=0.5,P(z_3|d)=0.1$，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i1.fuimg.com/611786/a39af47232980168.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在**LDA**中，主题分布和词分布不是唯一确定的，有很多种可能，但一篇文档总得对于一个主题分布和词分布，LDA为它们弄了两个Dirichlet先验参数，这个Dirichlet先验为某篇文档随机抽取1出某个主题分布个词分布。\n",
    "\n",
    "文档d产生主题z的概率，主题z产生单词w的概率都不再是某两个确定的值，而是随机变量。\n",
    "\n",
    "举个文档d具体产生主题z的例子。给定一篇文档d，现在有多个主题$z_1,z_2,z_3$，他们的主题分布${P(z_i|d),i=1,2,3}$可能是{0.4，0.5，0.1}，也可能是{0.2,0.2,0.6}，即这些主题被d选中的概率都不再认为是确定的值，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i2.tiimg.com/611786/9ec371c66f8bf8a8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF模型结果：\n",
      "晋江市/ 城市/ 大会/ 爱心/ 中华/ 基金会/ 陈健倩/ 重庆市/ 许嘉璐/ 巡视员/ \n",
      "TextRank模型结果：\n",
      "城市/ 爱心/ 救助/ 中国/ 社会/ 晋江市/ 基金会/ 大会/ 介绍/ 公益活动/ \n",
      "LSI模型结果：\n",
      "中国/ 中华/ 爱心/ 项目/ 基金会/ 社会/ 城市/ 公益活动/ 全国/ 国家/ \n",
      "LDA模型结果：\n",
      "晋江市/ 年度/ 公益活动/ 大会/ 人大常委会/ 重庆市/ 陈健倩/ 许嘉璐/ 巡视员/ 价值/ \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import math\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from gensim import corpora, models\n",
    "from jieba import analyse\n",
    "import functools\n",
    "\n",
    "\n",
    "# 停用词表加载方法\n",
    "def get_stopword_list():\n",
    "    # 停用词表存储路径，每一行为一个词，按行读取进行加载\n",
    "    # 进行编码转换确保匹配准确率\n",
    "    stop_word_path = 'D:\\\\319\\\\studying file\\\\NLP\\\\learning-nlp-master\\\\chapter-5\\\\stopword.txt'\n",
    "    stopword_list = [sw.replace('\\n', '') for sw in open(stop_word_path,encoding='utf-8').readlines()]\n",
    "    return stopword_list\n",
    "\n",
    "\n",
    "# 分词方法，调用结巴接口\n",
    "def seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        # 不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        # 进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "\n",
    "\n",
    "# 去除干扰词\n",
    "def word_filter(seg_list, pos=False):\n",
    "    stopword_list = get_stopword_list()\n",
    "    filter_list = []\n",
    "    # 根据POS参数选择是否词性过滤\n",
    "    ## 不进行词性过滤，则将词性都标记为n，表示全部保留\n",
    "    for seg in seg_list:\n",
    "        if not pos:\n",
    "            word = seg\n",
    "            flag = 'n'\n",
    "        else:\n",
    "            word = seg.word\n",
    "            flag = seg.flag\n",
    "        if not flag.startswith('n'):\n",
    "            continue\n",
    "        # 过滤停用词表中的词，以及长度为<2的词\n",
    "        if not word in stopword_list and len(word) > 1:\n",
    "            filter_list.append(word)\n",
    "\n",
    "    return filter_list\n",
    "\n",
    "\n",
    "# 数据加载，pos为是否词性标注的参数，corpus_path为数据集路径\n",
    "def load_data(pos=False, corpus_path='D:\\\\319\\\\studying file\\\\NLP\\\\learning-nlp-master\\\\chapter-5\\\\corpus.txt'):\n",
    "    # 调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词\n",
    "    doc_list = []\n",
    "    for line in open(corpus_path, 'r',encoding='utf-8'):\n",
    "        content = line.strip()\n",
    "        seg_list = seg_to_list(content, pos)\n",
    "        filter_list = word_filter(seg_list, pos)\n",
    "        doc_list.append(filter_list)\n",
    "\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "# idf值统计方法\n",
    "def train_idf(doc_list):\n",
    "    idf_dic = {}\n",
    "    # 总文档数\n",
    "    tt_count = len(doc_list)\n",
    "\n",
    "    # 每个词出现的文档数\n",
    "    for doc in doc_list:\n",
    "        for word in set(doc):\n",
    "            idf_dic[word] = idf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "    # 按公式转换为idf值，分母加1进行平滑处理\n",
    "    for k, v in idf_dic.items():\n",
    "        idf_dic[k] = math.log(tt_count / (1.0 + v))\n",
    "\n",
    "    # 对于没有在字典中的词，默认其仅在一个文档出现，得到默认idf值\n",
    "    default_idf = math.log(tt_count / (1.0))\n",
    "    return idf_dic, default_idf\n",
    "\n",
    "\n",
    "#  排序函数，用于topK关键词的按值排序\n",
    "def cmp(e1, e2):\n",
    "    import numpy as np\n",
    "    res = np.sign(e1[1] - e2[1])\n",
    "    if res != 0:\n",
    "        return res\n",
    "    else:\n",
    "        a = e1[0] + e2[0]\n",
    "        b = e2[0] + e1[0]\n",
    "        if a > b:\n",
    "            return 1\n",
    "        elif a == b:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "# TF-IDF类\n",
    "class TfIdf(object):\n",
    "    # 四个参数分别是：训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量\n",
    "    def __init__(self, idf_dic, default_idf, word_list, keyword_num):\n",
    "        self.word_list = word_list\n",
    "        self.idf_dic, self.default_idf = idf_dic, default_idf\n",
    "        self.tf_dic = self.get_tf_dic()\n",
    "        self.keyword_num = keyword_num\n",
    "\n",
    "    # 统计tf值\n",
    "    def get_tf_dic(self):\n",
    "        tf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            tf_dic[word] = tf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "        tt_count = len(self.word_list)\n",
    "        for k, v in tf_dic.items():\n",
    "            tf_dic[k] = float(v) / tt_count\n",
    "\n",
    "        return tf_dic\n",
    "\n",
    "    # 按公式计算tf-idf\n",
    "    def get_tfidf(self):\n",
    "        tfidf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            idf = self.idf_dic.get(word, self.default_idf)\n",
    "            tf = self.tf_dic.get(word, 0)\n",
    "\n",
    "            tfidf = tf * idf\n",
    "            tfidf_dic[word] = tfidf\n",
    "\n",
    "        tfidf_dic.items()\n",
    "        # 根据tf-idf排序，去排名前keyword_num的词作为关键词\n",
    "        for k, v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            print(k + \"/ \", end='')\n",
    "        print()\n",
    "\n",
    "\n",
    "# 主题模型\n",
    "class TopicModel(object):\n",
    "    # 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量\n",
    "    def __init__(self, doc_list, keyword_num, model='LSI', num_topics=4):\n",
    "        # 使用gensim的接口，将文本转为向量化表示\n",
    "        # 先构建词空间\n",
    "        self.dictionary = corpora.Dictionary(doc_list)\n",
    "        # 使用BOW模型向量化\n",
    "        corpus = [self.dictionary.doc2bow(doc) for doc in doc_list]\n",
    "        # 对每个词，根据tf-idf进行加权，得到加权后的向量表示\n",
    "        self.tfidf_model = models.TfidfModel(corpus)\n",
    "        self.corpus_tfidf = self.tfidf_model[corpus]\n",
    "\n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "        # 选择加载的模型\n",
    "        if model == 'LSI':\n",
    "            self.model = self.train_lsi()\n",
    "        else:\n",
    "            self.model = self.train_lda()\n",
    "\n",
    "        # 得到数据集的主题-词分布\n",
    "        word_dic = self.word_dictionary(doc_list)\n",
    "        self.wordtopic_dic = self.get_wordtopic(word_dic)\n",
    "\n",
    "    def train_lsi(self):\n",
    "        lsi = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lsi\n",
    "\n",
    "    def train_lda(self):\n",
    "        lda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lda\n",
    "\n",
    "    def get_wordtopic(self, word_dic):\n",
    "        wordtopic_dic = {}\n",
    "\n",
    "        for word in word_dic:\n",
    "            single_list = [word]\n",
    "            wordcorpus = self.tfidf_model[self.dictionary.doc2bow(single_list)]\n",
    "            wordtopic = self.model[wordcorpus]\n",
    "            wordtopic_dic[word] = wordtopic\n",
    "        return wordtopic_dic\n",
    "\n",
    "    # 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词\n",
    "    def get_simword(self, word_list):\n",
    "        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]\n",
    "        senttopic = self.model[sentcorpus]\n",
    "\n",
    "        # 余弦相似度计算\n",
    "        def calsim(l1, l2):\n",
    "            a, b, c = 0.0, 0.0, 0.0\n",
    "            for t1, t2 in zip(l1, l2):\n",
    "                x1 = t1[1]\n",
    "                x2 = t2[1]\n",
    "                a += x1 * x1\n",
    "                b += x1 * x1\n",
    "                c += x2 * x2\n",
    "            sim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0\n",
    "            return sim\n",
    "\n",
    "        # 计算输入文本和每个词的主题分布相似度\n",
    "        sim_dic = {}\n",
    "        for k, v in self.wordtopic_dic.items():\n",
    "            if k not in word_list:\n",
    "                continue\n",
    "            sim = calsim(v, senttopic)\n",
    "            sim_dic[k] = sim\n",
    "\n",
    "        for k, v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            print(k + \"/ \", end='')\n",
    "        print()\n",
    "\n",
    "    # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法\n",
    "    def word_dictionary(self, doc_list):\n",
    "        dictionary = []\n",
    "        for doc in doc_list:\n",
    "            dictionary.extend(doc)\n",
    "\n",
    "        dictionary = list(set(dictionary))\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "    def doc2bowvec(self, word_list):\n",
    "        vec_list = [1 if word in word_list else 0 for word in self.dictionary]\n",
    "        return vec_list\n",
    "\n",
    "\n",
    "def tfidf_extract(word_list, pos=False, keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    idf_dic, default_idf = train_idf(doc_list)\n",
    "    tfidf_model = TfIdf(idf_dic, default_idf, word_list, keyword_num)\n",
    "    tfidf_model.get_tfidf()\n",
    "\n",
    "\n",
    "def textrank_extract(text, pos=False, keyword_num=10):\n",
    "    textrank = analyse.textrank\n",
    "    keywords = textrank(text, keyword_num)\n",
    "    # 输出抽取出的关键词\n",
    "    for keyword in keywords:\n",
    "        print(keyword + \"/ \", end='')\n",
    "    print()\n",
    "\n",
    "\n",
    "def topic_extract(word_list, model, pos=False, keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    topic_model = TopicModel(doc_list, keyword_num, model=model)\n",
    "    topic_model.get_simword(word_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = '6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。' + \\\n",
    "           '中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄' + \\\n",
    "           '办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,' + \\\n",
    "           '重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局' + \\\n",
    "           '领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城' + \\\n",
    "           '市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市' + \\\n",
    "           '、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加' + \\\n",
    "           '这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动' + \\\n",
    "           '的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理' + \\\n",
    "           '事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大' + \\\n",
    "           '常委会主任陈健倩介绍了大会的筹备情况。'\n",
    "\n",
    "    pos = True\n",
    "    seg_list = seg_to_list(text, pos)\n",
    "    filter_list = word_filter(seg_list, pos)\n",
    "\n",
    "    print('TF-IDF模型结果：')\n",
    "    tfidf_extract(filter_list)\n",
    "    print('TextRank模型结果：')\n",
    "    textrank_extract(text)\n",
    "    print('LSI模型结果：')\n",
    "    topic_extract(filter_list, 'LSI', pos)\n",
    "    print('LDA模型结果：')\n",
    "    topic_extract(filter_list, 'LDA', pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
