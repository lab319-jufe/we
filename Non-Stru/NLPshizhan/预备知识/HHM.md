# 隐马尔可夫模型
隐马尔可夫模型（Hidden Markov Model，HMM），是一个关于时序的概率模型，描述了由隐藏的马尔科夫链随机生成一个不可观测的状态随机序列，再由状态随机序列中的各个状态生成一个可观测的随机序列的过程。在中文分词中，HMM是一种字标注法：词语是由字构造的，每个字在词语里都有自己的构词位置，中文分词中规定每个字最多只有四个构词位置————B（词首）、M（词中）、E（词尾）、S（独立成词），HMM中文分词就是在给每个字标上B、M、E、S中的其中一个标签


HMM 由以下5个参数确定：

**状态转移概率矩阵 A**: 
 
$$
\begin{bmatrix}
a_{11} & a_{12} &\cdots&a_{1j} \\
a_{21} & a_{22} &\cdots&a_{2j} \\
\vdots & \vdots & &\vdots \\
a_{i1} & a_{i2} &\cdots&a_{ij}\\
\end{bmatrix}
$$

**发射概率矩阵 B**: 

$$
\begin{bmatrix}
b_{11} & b_{12} &\cdots&b_{1n} \\
b_{21} & b_{22} &\cdots&b_{2n} \\
\vdots & \vdots & &\vdots \\
b_{m1} & b_{m2} &\cdots&b_{mn}\\
\end{bmatrix}
$$

**初始状态分布**:  $\pi$=($\pi_1$,$\pi_2$,$\cdots$,$\pi_i$)

**观测值序列**：O={$o_1,o_2,..,o_T$},其中，$o$可以取Q集合中的任何值

**状态值序列**：$I$={$i_1,i_2,...,i_T$},其中，$i$可以取V集合中的任何值

其中，

**观测值集合**:  Q={$q_1$,$q_2$,...,$q_N$}，是所有可能的观测状态的集合，观测值序列中的$o_t$也可以从集合Q中取

**状态值集合**:  V={$v_1$,$v_2$,...,$v_M$}，是所有可能的隐藏状态的集合，状态值序列中的$i_t$均可以从集合V中取

**先描述一个真实的场景，更好地描述下面HMM的应用：**

甲和乙是好朋友，每天都要煲电话粥。

甲已知三点信息：1.乙所在的地方天气不太好，下雨天多过晴天。2.天气会影响乙当天做什么事。3.乙每天做了什么事。

如果甲想通过乙对日常事务的描述推断出乙所在地的天气，就可以使用HMM来解决。

为简化模型，我们假设乙所在地的天气只有两种——rainy、sunny，乙每天只会做三件事中的其中一件——walk,shop,clean，此时观测结果O(日常事务)是实际隐藏状态Q(乙所在地的天气)的概率函数。

![Markdown](http://i2.tiimg.com/611786/8a98587d8085884a.png)


**使用HMM时，我们的问题一般有这两个特征：**

1.问题是基于序列的，比如时间序列，或者状态序列。状态序列，简称状态序列。
在这里观测序列是乙的日常事务————下楼散步、出门采购、打扫房间，隐藏状态序列是乙所在地的天气

2.问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏序列

## HMM要解决的三个基本问题:
1.评估问题。已知模型$\lambda$=(A,B,$\pi$)的参数和观测序列O={$o_1$,$o_2$,...,$o_T$}，那么该模型下观测序列O出现的概率P(O|$\lambda$)？

已知甲第一天购物，第二天打扫房间，第三天散步和初始概率、转移概率、发射概率，求出在已知的条件下，甲第一天购物，第二天打扫房间，第三天散步的概率。

2.学习问题。已知观测序列O={$o_1$,$o_2$,...,$o_T$}，估计模型$\lambda$=(A,B,$\pi$)的参数，使得在该模型下观测序列概率P(O|$\lambda$)最大

已知甲第一天购物，第二天打扫房间，第三天散步，求初始概率、转移概率、发射概率，使得甲第一天购物、第二天打扫房间、第三天散步的概率最大

3.预测问题，也称解码问题。已知模型$\lambda$=(A,B,$\pi$)和观测序列O={$o_1$,$o_2$,...,$o_T$}，求该给定观测序列下，最有可能的对应状态序列$I$={$i_1,i_2,...,i_T$}？

已知甲第一天购物，第二天打扫房间，第三天散步和初始概率、转移概率、发射概率，求这三天最有可能的天气

## HMM两个很重要的假设:
1.观测独立性假设。假设任意时刻的观测只依赖于当前时刻的隐藏的马尔科夫链的状态，与其他观测及状态无关，即$o_t$由$i_t$决定。这也是一个为了简化模型的假设，也就是说，B每天选择散步、采购还是打扫卫生只依赖于当天的天气如何。

2.齐次马尔科夫链假设。假设隐藏的马尔科夫链在任意时刻t的状态只依赖于它前一个时刻的隐藏状态，即$i_t$由$i_{t-1}$决定。也就是说，今天的天气只依赖于前一天的天气。

**对应三个基本问题，有三个算法能分别解决三个基本问题**

## 评估问题算法
### 前向算法
首先,定义前向概率$\alpha(i)$=P($o_1$, $o_2$, $\cdots$ ,$o_t$ ,$i_t$=$q_i$|$\lambda$)

输入：隐马尔可夫模型$\lambda$，观测序列O

输出：观测序列的概率P(O|$\lambda$)

步骤：

(1)初值   $\alpha_1(i)$=P($o_1$,$i_t$=$q_i$|$\lambda$)

![Markdown](http://i1.fuimg.com/611786/ba4c300cb5681756.png)

我们要求的观测序列O出现的条件概率可利用前向概率得到：
$P(O|\lambda)=\sum_{i=1}^{N} P(O,i_t=q_i|\lambda)=\sum_{i=1}^{N}\alpha_T(i)$

(2)递推得到$\alpha_t(j)$，对于t=1,2,...,T-1,

$\alpha_{t+1}(i)=P(o_1,o_2,\cdots,o_t,o_{t+1},i_{t+1}=q_j|\lambda)$

$=\sum_{i=1}^{N}P(o_1,o_2,\cdots,o_t,o_{t+1},i_t=q_i,i_{t+1}=q_j|\lambda)$

$=\sum_{i=1}^{N}P(o_{t+1}|o_1,o_2,\cdots,o_t,i_t=q_i,i_{t+1}=q_j,\lambda)P(o_1,o_2,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)$

$=\sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_j）P(o_1,o_2,\cdots,o_t,i_t=q_j|\lambda)$

$=\sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_j)P(i_{t+1}=q_j|o_1,o_2,\cdots,o_t,i_t=q_i,\lambda)P(o_1,o_2,\cdots,o_t,i_t=q_j|\lambda)$

$=\sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_j)P(i_{t+1}=q_j|i_t=q_j)\alpha_t(i)$

$=\sum_{i=1}^Nb_j(o_{t+1})a_{ij}\alpha_t(i)$

即 $\alpha_{t+1}(i)=b_j(o_{t+1})[ \sum_{i=1}^Na_{ij}\alpha_t(i) ]$              

(3)终止  P(O|$\lambda$)=$\sum_{i=1}^N\alpha_T(i)$


### 后向算法
与前向算法类似，可观测序列取的是($o_{t+1}$,$o_{t+2}$,...,$o_T$)
![Markdown](http://i2.tiimg.com/611786/79e7f3f92d701087.png)
## 学习问题算法
### 监督学习方法
用极大似然估计法来估计HMM的参数，具体方法为：

（1）转移概率$a_{ij}$的估计

$\hat{a}_{ij}$=$\frac{A_{ij}}{{\sum_{j=1}^N}A_{ij}}$

其中，i=1,2,...,N；j=1,2,...,N；$A_{ij}$为样本中时刻t处于状态i，时刻t+1转移到状态j的频数

（2）观测概率$b_j(k)$的估计

$\hat{b}_j(k)$=$\frac{B_{jk}}{{\sum_{k=1}^M}B_{jk}}$

其中，j=1,2,...,N；k=1,2,...,M；$B_{jk}$是样本中状态为j并观测为k的频数

（3）初始状态概率$\pi_i$的估计为样本中初始状态为$q_i$的频率

**由于该方法需要使用训练数据，而人工标注训练数据的代价过高**

### Baum-Welch算法
Baum-Welch算法其实是EM算法在隐马尔可夫模型学习中的具体实现，期望最大化算法(Expectation maximization)算法，简称EM算法，是用期望替代实际的算法，基本思路：

求出联合分布$P(O,I|\lambda)$基于条件概率$P(O,I|\bar{\lambda})$的期望，其中$\bar{\lambda}$为当前的模型参数，然后在最大化这个期望，得到更新的模型参数$\lambda$，接着不断地迭代，直到$\bar{\lambda}$收敛为止。

分为两步进行：

（1）E步：
Q函数：$Q（\lambda,\lambda^{(i)}) = E_I(log P(O,I|\lambda)P(I|O,\lambda^{(i)})) = \sum_I log P(O,I|\lambda)P(I|O,\lambda^{(i)})$

对应进HMM中，观测序列O，隐藏状态I，模型参数$\lambda$，且I为离散序列，得出HMM中的Q函数

Q($\lambda$,$\bar{\lambda}$)=$\sum_IlogP(O,I|\lambda)P(O,I|\bar{\lambda})$

（2）M步：极大化Q函数Q($\lambda$,$\bar{\lambda}$)，求极大化时的模型参数A,B,$\pi$（用拉格朗日乘数法，对其求偏导并令为0，得出A,B,$\pi$）

抛硬币的例子：有两枚硬币A和B，硬币是有偏的，即这两枚硬币抛出正反面的概率不再是0.5，H表示正面向上，T表示反面向上，参数θ表示正面朝上的概率，且A、B两硬币的抛出正面的概率不相同(这里先假设$\theta_A=0.6$,$\theta_B=0.5$，这是已知的条件)。现做5组实验，每组随机选一个硬币，连续抛10次，所有的观测结果如下：

**1  H T T T H H T H T H**  
**2  H H H H H T H H H H** 
**3  H T H H H H H T H H** 
**4  H T H T T T H H T T** 
**5  T H H H T H H H T H** 

根据这些观测结果O，我们想知道5次实验中什么硬币更有可能导致现有的观测结果，使用EM算法：

第一步:给$\theta_A$,$\theta_B$一个初始值：$\theta_A^{(0)}$=0.6,$\theta_B^{(0)}$=0.5

第二步（E-step）：
+ 计算A、B硬币的概率可以用二项分布$p(k)=C_n^k\theta^k(1-\theta)^{n-k}$，为了计算方便，$C_n^k$也不会影响整体的结果和后续计算，可以省略。
+ 第一组：估计每组实验是硬币A、硬币B的概率（本组实验是硬币B的概率=1-本组实验是硬币A的概率）:分别计算每组实验中，选择A硬币且正面朝上次数的期望值，选择B硬币且正面朝上次数的期望值，用期望值代替概率
P(A) = $p_A(h)^h(1-p_A(h))^{10-h}=0.6^{5}*(1-0.6^{5})$=0.0007962624 
P(B) = $p_B(h)^h(1-p_B(h))^{10-h}=0.5^{5}*(1-0.5^{5})$=0.0009765625
P（硬币是A硬币） = $\left(\dfrac{P(A)}{P(A)+P(B)}\right)$= 0.45
P（硬币是B硬币） = $\left(\dfrac{P(B)}{P(A)+P(B)}\right)$= 0.55
+ 剩下的4组都以此类推，分别计算出每组A、B硬币且正面朝上次数的期望值

第三步（M-step）：
+ 利用第二步求得的期望值重新计算$\theta_A$,$\theta_B$：第一组有45%的概率来自A，因此用0.45乘以5个H得到2.2个H是来自A的，再用0.45乘以5个T得到2.2个T是来自A的,同理55%的概率来自B，5个H里有2.8个H来自B,2.8个T来自B，以此类推，可以得到50个观测结果中，有21.3个H和8.6个T来自A，有11.7个H和8.4个T来自B。
  $\theta_A^{(1)}=\left(\dfrac{21.3}{21.3+8.6}\right)\approx0.71$
  $\theta_B^{(1)}=\left(\dfrac{11.7}{11.7+8.4}\right)\approx0.58$
+ 将更新后的$\theta_A^{(1)}$,$\theta_B^{(1)}$再一次代入E步中进行计算，$\theta$不断变化，直到某一时刻$\theta$值稳定下来或者$\theta$的值在足够小可以忽略的范围内变动时，$\theta$值就会在这个值的附近稳定下来。



![Markdown](http://i1.fuimg.com/611786/e9713d39c1b2c5db.png)

## 预测问题算法
### 近似算法
在每个时刻t选择在该时刻最有可能出现的状态$i_t^*$,从而得到一个状态序列$I^*=(i_1^*,i_2^*,..,i_t^*)$,将它作为预测的结果。这个算法的优点是计算简单，但是却不能保证预测的状态序列的整体是最可能的状态序列，因为预测的状态序列中某些相邻的隐藏状态可能存在转移概率为0的情况。

### Viterbi算法
输入：模型$\lambda=(A,B,\pi)$和观测$O=(o_1,o_2,...,o_T)$
输出：最优路径$I^*=(i_1^*,i_2^*,...,i_T^*)$
(1)初始化

$\delta_1(i)=\pi_ib_i(o_1) ,i=1,2,...,N$
$\Psi_1(i)=0,i=1,2,...,N$

(2)递推。对t=2，3，...,$T$

$\delta_t(i)=\mathop{\max}_{1<j<N}[\delta_{t-1}(j)a_{ji}]b_i(o_t) ,i=1,2,...,N$
$\Psi_t(i)=\mathop{\arg\max}_{1<j<N}[\delta_{t-1}(j)a_{ji}] ,i=1,2,...,N$

(3)终止

$P^*={\max}_{1<i<N}\delta_T(i)$
$i_T^*=\arg\max_{1<j<N}[\delta_T(i)]$

### Viterbi算法在中文分词里的应用

#### 针对中文分词，直接给HMM的五元组参数赋予具体含义： ####
**状态值集合V**={$v_1$,$v_2$,$v_3$,$v_4$},其中，$v_1$为B，$v_2$为M，$v_3$为E，$v_4$为S，即V={B, M, E, S}，分别代表每个状态代表的是该字在词语中的位置，B代表该字是词语中的起始字(begin)，M代表是词语中的中间字(middle)，E代表是词语中的结束字(end)，S则代表是单字成词(single)。

**观察值集合Q**为所有汉字(东南西北你我他…)，甚至包括标点符号所组成的集合。

在HMM模型中文分词中，我们的输入是一个句子(也就是观察值序列)，输出是这个句子中每个字的状态值。比如，输入观察值序列———我爱中国，这个序列中得每个字，观察值集合中都有，输出的状态序列为BMS。
根据这个状态序列我们可以进行切词:B/M/S ，切词结果:我/爱/中国

**初始状态概率分布$\pi$** 是输入的句子的第一个字属于{B,E,M,S}这四种状态的概率，容易理解，开头的第一个字只可能是词语的首字(B)，或者是单字成词(S)，E和M的概率都是0。

**转移概率矩阵 A**  矩阵的横坐标和纵坐标顺序是BEMS x BEMS，比如A[0][2]代表的含义就是从状态B转移到状态M的概率。由状态各自的含义可知，状态B后面只能接M和E，状态M后面只能接M和E，状态E后面只能接B和S，状态S后面只能接B和S。

**发射概率矩阵 B**  矩阵中的发射概率$b_{ij}$其实一个条件概率，观测值完全依赖于当前的状态值（观测独立性假设）

**tips：HMM模型里所需的三个概率都是通过对指定语料库里的分词语料进行训练得出的**

#### HMM中文分词之Viterbi算法 

输入样例: 我爱北京

Viterbi算法计算过程如下： 

**1.通过初始概率、转移概率、发射概率，计算出二维数组weight[4][4]**，4是状态数(0:B,1:E,2:M,3:S)，4是输入句子的字数。例如 weight[0][2] 代表 状态B的条件下，出现’北’这个字的可能性。

![Markdown](http://i2.tiimg.com/611786/f590e171cfc612f7.png)
![Markdown](http://i2.tiimg.com/611786/5b040ab4493790c5.png)

**2.在weight里取最大概率，计算得到二维数组path**，4是状态数(0:B,1:E,2:M,3:S)，4是输入句子的字数。例如 path[0][2] 代表 weight[0][2]取到最大时，前一个字的状态，比如 path[0][2] = 1, 则代表 weight[0][2]取到最大时，前一个字(也就是爱)的状态是E。

![Markdown](http://i1.fuimg.com/611786/71ca686d4b51d61f.png)
**3.从最后一个字向前回溯**：
能得出“京”字最大的概率为E状态下的0.00192，所以最优路径下，“京”字是在E的位置；再往前推，“京”的E状态下的0.00192是由前一个字“北”的第三条路线得到的，所以最优路径经过“北”的第三个位置，即最优路径下，“北”应为M状态；而“北”能在M状态达到最大概率是由于前一个字“爱”的第三条路线得出的，所以在最优路径下，“爱”的位置为M；再向前推，“爱”在M状态室友“我”的第一条路线得出的，所以在最优路径下，“我”的位置为B。所以最优路径为“B-M-M-E”。