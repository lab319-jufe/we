---
title: "linear regression"
output: html_document
---

> 文件创建：杨钰萍 2017级   2017年秋
---

#第二讲 线性回归

##1 前言

下面不详述违反各项经典假设的原因及可能造成的后果，请参见计量经济学或者其它相关书籍

关于偏残差系数的数理意义请参考：赵松山《关于偏残差系数的讨论》，在这里不展开讨论

---

##2 线性回归

###2.1 简单线性回归

```{r warning=FALSE}
#Boston数据集在MASS包里面
library(MASS)
#fix(Boston)  #看一下Boston是一张怎样的数据表
#names(Boston)#看一下Boston数据表中的各个变量名称

#用lm.fit2.1 <-lm(medv ~ lstat, data = Boston)可以得到跟以下两句同样的结果
attach(Boston)
lm.fit2.1 <- lm(medv ~ lstat) #得到线性回归函数

lm.fit2.1 #系数
summary(lm.fit2.1) #查看置信区间，p值，标准误，R^2，F统计量
```

**结果解读：**星号个数表示显著性的强弱，“*”越多表示显著性越强

```{r warning=FALSE}
confint(lm.fit2.1) #得到系数估计值的置信区间
predict(lm.fit2.1, data.frame(lstat = (c(2, 6, 9))), interval = "confidence") #用lstat预测medv得到的置信区间
predict(lm.fit2.1, data.frame(lstat = (c(2, 6, 9))), interval = "prediction") #用lstat预测medv得到的预测区间

#画出函数拟合图
plot(lstat, medv, col="blue")
abline(lm.fit2.1, lwd = 3, col = "red")

```

**注意：**lm（应变量~自变量），plot（自变量，应变量），abline()不能单独使用，上面必有一行画图的命令


###2.2 多元线性回归


```{r warning=F}
lm.fit2.2.1 <- lm(medv ~ lstat + age, data = Boston) #medv关于lstat和age的回归模型
summary(lm.fit2.2.1)
lm.fit2.2.2 <- lm(medv ~ ., data = Boston) #medv关于所有变量的回归模型
summary(lm.fit2.2.2)
lm.fit2.2.3 <- lm(medv ~ . - age, data = Boston) #medv关于除了age以外的所有变量的回归模型
summary(lm.fit2.2.3)
#用summary(lm(medv ~ lstat + age + lstat : age, data = Boston))可以得到跟以下语句一样的结果
summary(lm(medv ~ lstat+age, data = Boston)) #包含了lstat，age和lstat与age的交互项

#如果要把二次项引入线性回归 ，要用I()，不能用lstat^2， 因为 "^" 表示交互项达到某一个次数，或者用下面的函数
lm.fit2.2.4 <- lm(medv ~ poly(lstat, 5)) #把lstat的一阶二阶至五阶全部写进去了
summary(lm.fit2.2.4)

```
**注意：**如果自变量里面有定性变量，R会自动创建虚拟变量

###2.3 选择“最佳模型”

选择最佳模型跟自变量的选取可以放在一起，选择最佳模型是对已有模型去进行比较，然后选自变量相当于是还在建模，但都有关于要不要把某一个自变量引进来的问题

但是一个模型好不好，主要看拟合的怎么样或者预测能力如何，根据模型不同的用途有不同的标准，以及可能会出现某两个检测方法得出不同结论的情况，这个需要从R^2等多种信息量准则去综合判断

**方法1：anova检验**（这个函数可以比较两个嵌套模型的拟合优度）（（必须要是**嵌套模型**））

```{r , echo=TRUE,warning=FALSE}
#以下比较模型2.2.3与2.2.2
anova(lm.fit2.2.3, lm.fit2.2.2)

```
**结果解读：**这里第一个模型嵌套在第二个模型中，结果显示，p值为0.96，意味着检验不显著，即不拒绝原假设，也就是不需要把age引入线性模型中。

**方法2：用赤池信息量准则（AIC）**((寻找可以最好地解释数据但包含最少自由参数的模型))(((没有嵌套模型的要求)))

```{r,echo=TRUE}
AIC(lm.fit2.2.3, lm.fit2.2.2)

```

**结果解读：**这里表明lm.fit2.2.3更好，跟上面用ANOVA的结果一样

通过变量选择从大量的候选变量里面得到最终的自变量（较为流行以下两种）（（CH6的内容，下次再说））

**方法3：逐步回归法**（未展开）

**方法4：全子集回归**（未展开）

---
##3 判断是否满足假设

###3.1 总体判断

**标准诊断图形**
```{r,echo=TRUE,warning=FALSE}
#Auto数据集在ISLR包里面
library(ISLR)
lm.fit3.1.1 <- lm(mpg ~ horsepower, data = Auto)
par(mfrow = c(2, 2))
plot(lm.fit3.1.1)
```

**结果解读：**左上图是**“残差与拟合图”**，用来判断线性性。该图中显示出了一个曲线关系，说明因变量与自变量不满足线性性，可能要在回归模型中加入一个二次项；右上图是**“正态Q-Q图”**，可以用来判断是否满足正态性假设。图中的点均落在呈45°角的直线上，说明满足正态性假设；左下图是**“位置尺度图”**，用来判断同方差性。该图中水平线周围的点呈随机分布，说明满足同方差性；右下图是**“残差与杠杆图”**，可以用来鉴别异常值点（可读性不佳，不展开，后面补充更好的鉴别图像）。

```{r,echo=TRUE,warning=FALSE}
#加入二次项，与原来的回归诊断图进行比较
lm.fit3.1.2<- lm(mpg ~ horsepower + I(horsepower ^ 2), data = Auto)
par(mfrow = c(2, 2))
plot(lm.fit3.1.2)
```

**结果解读：**加入二次项后，**“残差与拟合图”**趋于一条直线，说明此时满足了线性性

**以下使用基础包中的state.x77数据集(这个数据集是一个矩阵)**

```{r,echo=TRUE,warning=FALSE}

states<-as.data.frame(state.x77[,c("Murder","Population","Illiteracy","Income","Frost")])#这边是为了把以矩阵形态存在的数据集变成数据框
lm.fit3.1.3 <- lm(Murder ~ ., data = states)
par(mfrow = c(2, 2))
plot(lm.fit3.1.3)
```

**基于gvlma包**

```{r,echo=TRUE,warning=FALSE}
library(gvlma)
gvmodel <- gvlma(lm.fit3.1.3)
summary(gvmodel)
```

**结果解读：**输出的结果除了summary(lm.fit3.1.3)的内容外，最后五行值得注意。它不仅给出了综合检验结果（Global Stat），还给出了偏度，峰度，异方差性的评价。当然输出结果表示，数据满足OLS回归模型的各项所有统计假设（Assumptions acceptable）

###3.2 自相关性

```{r,echo=TRUE}
library(car)
durbinWatsonTest(lm.fit3.1.3)
```

**结果解读：**P>0.05,不能拒绝原假设，即不存在自相关性

###3.3 线性性
```{r,echo=TRUE}
crPlots(lm.fit3.1.3) #具体可以再探究，跟标准诊断图形的左上图是吻合的，都表示满足线性性
```

**结果解读：**输出的是偏残差图，这边四幅图都显示拟合曲线和平滑拟合曲线都呈线性，表明满足线性性假设

###3.4 正态性
```{r,echo=TRUE,warning=FALSE}
#car包提供大量函数，大大增强拟合和评价回归模型的能力
qqPlot(lm.fit3.1.3, label=row.names(states),id.method="identify",simulate = TRUE, main = "学生化残差的Q-Q图")
```

**结果解读：**图中的点均散落在呈45°角的直线附近，且都落在置信区间内，说明较好地满足了正态性假设。但是发现存在一个异常值点（左上角），它是一个很大的正残差值（真实值-预测值），说明有数据被低估了。

```{r,echo=TRUE}
#自己编写一个函数用来画学生化残差图
rsdplot <- function(fit, nbreaks = 10)
{
	z <- rstudent(fit)
    hist(z, breaks = nbreaks, freq = FALSE, xlab = "studentized residual", main = "distribution of errors")
    rug(jitter(z), col = "brown")
    curve(dnorm(x, mean = mean(z), sd = sd(z)), add = TRUE, col = "blue", lwd = 2)
    lines(density(z)$x, density(z)$y, col = "red", lwd = 2, lty = 2)
    legend("topright", legend = c("normal curve", "kernel density cuve"), lty = c(1,2), col = c("blue", "red"), cex = 0.7)
}
rsdplot(lm.fit3.1.3)

```

**结果解读：**除了一个很明显的离群点 ，其他误差值都很好地服从了正态分布，而且密度曲线跟正态分布曲线很接近。此外，这个柱状图和密度测量分布图能够很方便地看出分布的斜度（比Q-Q图容易）

###3.5 同方差性

**方法1 图形法**

```{r,echo=TRUE}
spreadLevelPlot(lm.fit3.1.3)
```

**结果解读：**“残差拟合值图”显示，散点在水平的残差拟合曲线周围呈水平随机分布，说明满足同方差性假设。

**方法2 假设检验法**

```{r,echo=TRUE}
ncvTest(lm.fit3.1.3)
```

**结果解读：**p=0.19,说明不显著，即满足原假设，也就是说，不存在异方差性

###3.6 多重共线性

**非正规方法：**

方法1：增或减一个变量，回归系数大变

方法2：对重要自变量进行回归系数的单项检验，结果不显著

方法3：回归系数的代数符号与经验值相反

方法4：相关阵中两两自变量的相关系数较大

方法5：重要回归系数的置信区间较大

**正规方法：**计算方差膨胀因子（局限性：不能区别几个同时多重共线性）

```{r,echo=TRUE}
sqrt(vif(lm.fit3.1.3)) > 2 #先计算方差膨胀因子，再与2比较大小
```

**结果解读：**TRUE表明存在多重共线性，FALSE表明不存在多重共线性，所以可以看到这边不存在多重共线性

---

##4 判断是否存在异常值点

###4.1 三种点总的判断
```{r,echo=TRUE}
influencePlot(lm.fit3.1.3,id.method="identify",main="influence plot",sub="circle size is proportional to cook's distance")
```

**结果解读：**纵坐标看离群点(>2或<-2),横坐标看高杠杆值点（>0.2或0.3），圈圈大小看对模型参数估计的影响大小，太大的可能是强影响点。从图中看到三种点均有存在

###4.2 离群点

方法1：通过Q-Q图，落在置信区间外的是离群点

方法2：标准化残差值>2或<-2的可能是离群点

**方法3：**

```{r,echo=TRUE}
#下面这个函数，是通过！单个！最大（—or+）残差值的显著性来判断是否具有离群点，如果不显著，就说明没有离群点，如果显著，必须删除这个离群点，然后再次使用这个函数用来判断是不是还有其他离群点的存在
outlierTest(lm.fit3.1.3)
```

###4.3 高杠杆值点

```{r,echo=TRUE,warning=FALSE}
hatplot <- function(fit)
{
    p <- length(coefficients(fit)) #p是参数个数
    n <- length(fitted(fit))#n是样本量
    plot(hatvalues(fit), main = "Index Plot of Hat Values")
    abline(h = c(2, 3) * p / n, col = "red", lty = 2)
    identify(1:n, hatvalues(fit), names(hatvalues(fit)))#定位函数
}
hatplot(lm.fit3.1.3)
```

**结果解读：**只需要看图，水平线标注的是帽子均值2倍和3倍的位置，一般认为超过帽子均值2倍和3倍的位置的点是高杠杆值点。

###4.4 强影响值点

**方法1：计算Cook距离并绘图**（Cook's D图）（该图对于找强影响点很有用，但是不能反映这些点是怎么影响模型的）((这个图可以直接显示出来强影响点的名字))

```{r,echo=TRUE}
cutoff <- 4 / (nrow(states) - (length(lm.fit3.1.3$coefficients) - 1) - 1)
plot(lm.fit3.1.3, which = 4, cook.levels = cutoff)#？which=4表示前面的第四张图，但是第4张图并不是这样的(而且第1，2，3张图是一样的)#这里好像有点疑问
abline(h = cutoff, lty = 2, col = "red")
```

**结果解读：**从图中可以看到有三个强影响点（已被标出名字）

**方法2：绘制变量添加图**

```{r,echo=TRUE}
avPlots(lm.fit3.1.3,id.method="identify")
```

**结果解读：**变量添加图是，预测变量$X_k$与其余k-1个预测变量拟合的残差值（X轴）与响应变量Y与其余k-1个预测变量拟合的残差值（Y轴）画出的散点关系图，还给出了平滑拟合曲线。图中的直线表示相应预测变量的实际回归系数。如左下和右下两个图，直线为水平在O处，说明这两个预测变量是不显著的，最终的回归模型中，这两个变量前的系数应该是0。

---

##5 问题的解决

###5.1 违反线性性假设
```{r,echo=TRUE,warning=FALSE}
boxTidwell(Murder ~ Population + Illiteracy, data = states)
```

**结果解读：**结果表明使用变换，可以改善线性关系，但是由于计分检验的P值都>0.05，所以不能拒绝原假设，意味着不需要进行该变量变换

###5.2 违反正态性假设
```{r,echo=TRUE,warning=FALSE}
summary(powerTransform(states$Murder))
```

**结果解读：**结果表明，用murder^0.6来正态化变量murder，但是发现不能拒绝lambda=1的假设，意味着，不需要进行该变量变换

###5.3 违反同方差性

前面在<更好的方法（分散）>里面，检验同方差性的时候已经提到了

###5.4 存在多重共线性

方法1：删除存在多重共线性的变量（把<更好的方法（分散）>里面，检测多重共线性，出现TRUE的变量删除）（（局限性：得不到关于被剔除变量的直接信息，且模型中剩余变量回归系数大小受模型外的相关变量的影响））

方法2：在多项式回归模型中，把任意给定的自变量表示成与其均值离差的形式，以大大降低一阶，二阶和更高阶的项之间的多重共线性

方法3：岭回归（专门处理多重共线性）((下次再讲))

方法4：主成分回归（参考《应用回归分析》何晓群，刘文卿）、因子分析这些

###5.5 存在异常值点

如果能肯定地说明一个异常观察值是很大的测量误差的结果，剔除是合适的

如果异常观察值是准确的，他可能意味着模型的失败（遗漏重要自变量或选择了不正确的函数形式）

如果异常观察值是准确的，但找不到对于他的解释，那么可以抑制他的影响（而不是剔除）（（比如使用最小绝对离差法：通过极小化绝对离差之和来估计回归系数。这种方法具有对 异常数据和不合适模型并不敏感的性质，除了这个方法，还有其他稳健的方法可以参考《统计稳健性：关于当代应用问题的看法》罗伯特.V.霍格））



---

##6 补充（待补充）

###6.1 库克距离（待补充）

###6.2 帽子矩阵（待补充）

###6.3 异常值点的区分（待补充）

---

