---
title: "决策树"
author: "杨钰萍一稿，林为修改"
date: "2018年4月26日"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#决策树基本介绍

##决策树是什么？

决策树是基于树结构来进行决策，这恰是人类在面临决策问题时一种很自然的处理机制。例如，我们要对“这是好瓜吗？”这样的问题进行决策时，通常会进行一系列的判断或“子决策”：我们先看“它是什么颜色？”，如果是“青绿色”，则我们再看“它的根蒂是什么形态？”，如果是“蜷缩”，我们再判断“它敲起来是什么声音？”，最后我们得出决策：这是一个好瓜。这个决策如图所示：

![](http://i1.fuimg.com/611786/2bf6a55a694d5c20.png)


一棵决策树包含一个根结点、若干个内部结点和若干个叶结点   

树的内部结点（internal node）用来把预测变量空间分开    
树的分支（branch）用来把树内部各个结点的连接起来    
叶结点对应于决策结果  

##决策树能做什么？  

决策树能实现对数据的探索，能对数据轮廓进行描述，能进行预测和分类(决策树在回归中称为回归树，在分类中称为分类树 )，了解哪些变量最重要。决策树学习的目的是为了产生一棵泛化能力强，也就是能够处理未见实例的决策树。    
  
##决策树基本流程  

![](http://i1.fuimg.com/611786/50def6ef4fbcf3f1.png)


#经典决策树

##分类树

对某个给定观测值，用它所属区域中**训练集**的**众数**对其进行预测

###经典算法

**1、ID3**[Quinlan,1986]:用信息增益(information gain)来划分结点

我们希望，决策树划分过程中，分支结点所包含的样本尽可能同属一类，即结点的纯度越来越高。信息熵是度量样本集合纯度最常用的一种指标。假设样本集合D中响应变量有K类，第k类样本所占的比例为pk，那么信息熵的计算则为下面的计算方式： 

$$Ent(D)=-\sum_{k=1}^{K}p_{k}\log_2 p_{k}$$       

当这个Ent(D)的值越小，说明样本集合D的纯度就越高，有了信息熵，当我选择用样本的某一个属性a来划分样本集合D时，就可以得出用属性a对样本D进行划分所带来的“信息增益” ：

$$Gain(D,a)=Ent(D)-\sum_{m=1}^{M}\frac{\lvert D^m\rvert}{\lvert D\rvert}Ent(D^m)$$      

m是属性a可取值数目数，$\frac{\lvert D^m\rvert}{\lvert D\rvert}$的存在是因为不同分支结点所包含的样本数不同，所以要给分支结点赋权。
一般而言，信息增益越大，则意味着使用属性a进行划分所获得的纯度提升越大。因此，我们可用信息增益来进行决策树的划分属性选择。  

ID3决策树偏好于选择可取值数目较多的属性。这种划分方法会导致结点过纯，每个分支的样本量过小，具有高方差性，泛化能力差。

****

**2、C4.5**[Quinlan,1993]:用信息增益率(information gain ratio)来划分结点

C4.5决策树的提出完全是为了解决ID3决策树的一个缺点，当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱，不能够对新样本进行有效的预测。  

而C4.5决策树则不直接使用信息增益来作为划分样本的主要依据，而提出了另外一个概念，增益率:

$$Gain.ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$

其中$$IV(a)=-\sum_{m=1}^{M}\frac{\lvert D^m\rvert}{\lvert D\rvert}\log_2\frac{\lvert D^m\rvert}{\lvert D\rvert}$$

$IV(a)$称为属性a的"固有值"，当属性a可取值数目越多，也就是M越大，$IV(a)$越大，因此信息增益率偏好于选择可取值数目较少的属性。

需注意的是，增益率准则对数目较少的属性有所偏好，因此C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出增益率高于平均水平的属性，再从中选择增益率最高的。

****

**3、CART**(classification and regression tree)[Breiman et al.,1984]：用基尼系数(Gini index)来划分结点。

基尼系数被视为衡量结点纯度（purity）的指标，如果基尼系数较小，就意味着某个结点包含的观测值几乎都来自同一类别。基尼指数的通俗解释就是：表示一件事物的不确定性，基尼指数越大不确定性越大。我们要找基尼指数小的属性，这样的属性对于划分数据集的准确性会更高。

$$Gini.index(D,a)=\sum_{m=1}^{M}\frac{\lvert D^m\rvert}{\lvert D\rvert}G$$

其中$G=\sum_{k=1}^{K}p_{k}(1-p_{k})$是基尼值。如果$p_{k}$取值都接近0或者1，基尼值就会很小，相应的，基尼系数就会很小。

选择使基尼系数最小的属性作为最优化分属性。

****

###分类树举例

![](http://i2.tiimg.com/611786/eca2a86036a27d31.png)


****

##回归树

###理论概念

对某个给定观测值，用它所属区域中训练集的**平均值**对其进行预测。CART算法对预测变量空间的分割采用**递归二叉分裂**（recursive binary splitting）,这是一种自上而下的贪婪（greedy）算法，也就是**只考虑当前分裂最优，而不从全局考虑。**，在这里意味着每做一个分割点，都选择使RSS最小的那个分割点。

建立回归树的大致过程：

step1：把预测变量空间分割成J个互不重叠的区域$R_1,R_2...R_J$  
step2：对落入区域$R_j$的每个观测值做同样的预测，预测值等于这个空间上训练集的响应值的均值  

问题就在于，如何分割预测变量空间。理论上区域空间的形状是任意的，但是出于模型简化和增强可解释性的考虑，将预测变量空间划分成高维矩阵=盒子（box）。

划分区域的目标是使下式最下：$$\sum_{j=1}^{J}\sum_{i\in R_j}(y_i-\hat y_{R_i})^2$$

如果将空间无限分裂下去，那么所有训练集上的点都会被分到一个类，复杂而无用。需要规定某个停止准则，比如一个区域的观测个数小于5，这个区域就不再分裂了。

这种方法，虽然在训练集上RSS会比较小，分的比较好，但是树太多，太复杂可能会出现过拟合。而小一点的树具有更小的方差和更好的解释性，当然会增加些许偏差（方差-偏差权衡），所以我们要考虑剪枝。

****

##剪枝

目的：对付过拟合  

基本策略：预剪枝、后剪枝  

**预剪枝**是指在**决策树生成过程中**，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化能力提升，则停止划分并将当前结点标记为叶结点。

**优点**：预剪枝使得决策树的很多分支都没有展开，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间和测试时间。   
**缺点**：有些分支的当前划分虽不能提升泛化能力、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高；预剪枝基于贪婪的本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险。    


**后剪枝**则是**先从训练集生成一颗完整的决策树**，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化能力提升，则将该子树替换为叶结点。后剪枝决策树通常比预剪枝决策树保留了更多的分支。    

**优点**：一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。  
**缺点**：后剪枝决策树实在生成完全决策树之后进行的，并且要自底向上地对所有非叶结点进行逐一考察，因此其训练时间会比未剪枝决策树和预剪枝决策树要久很多。  


直观上看，剪枝的目的是选出使测试集预测误差最小的子树，子树的测试误差可以通过交叉验证或验证集来估计。但由于可能的子树数量及其庞大，对每一课子树都用交叉验证来估计误差太过复杂。因此需要从所有可能子树中选出一小部分再进行考虑。  

**代价复杂性剪枝（cost complexity pruning）**也称**最弱联系剪枝（weakest link pruning）**可以完成这个任务，这种方法不是考虑每一课可能的子树，而是考虑以非负调整参数$\alpha$标记一系列子树。每一个$\alpha$的取值对于一颗子树T属于T0，当$\alpha$一定时，其对应的子树使下式最小：

$$\sum_{m=1}^{\lvert T \rvert}\sum_{i:x_i\in R_m}(y_i-\hat y_{R_m})^2+\alpha\lvert T \rvert$$

其中，$\lvert T \rvert$表示树T的终端结点数，$R_m$是第m个终端结点对应的矩形，$\alpha$是调整系数，在子树的复杂性和与训练数据的契合度之间控制权衡。上式简单理解就是给分裂准则-基尼系数加上惩罚项。当$\alpha$增大时，如果树的终端结点数多，他就会为自己的复杂付出代价，那么使上式取到最小值的子树会变的更小。


****

###回归树与线性模型比较

如果预测变量和响应变量之间的关系能很好地用线性回归模型来拟合，那么拟合效果将优于不能揭示这种线性结构的回归树；如果两者之间的关系呈现出一种复杂的高度非线性，那么树方法可能更好。

![](http://i1.fuimg.com/611786/30fb9a7a34bca248.png)

**图片解读：**

上方：一个二维分类实例，它的真实决策边界是线性的。线性边界的传统方法(左图)比用平行线划分区域的决策树(右图)，效果更好。  
下方：这里的真实决策边界是非线性的，此时线性模型没有捕捉到真实的决策边界(左图),决策树则相对成功(右图)。  

****

##树的优缺点

**优点：**解释性强，更接近人的决策，图形模式使得非专业人士也能看得懂，可以直接处理定性的预测变量而不需要创建哑变量

**缺点：**预测准确性一般无法达到其他回归和分类方法的水平，太过复杂的树会存在过拟合问题。

不过，通过用**装袋法，随机森林，提升法**等方法组合大量决策树，可以显著提升数的预测效果。

****

##三种集群学习方法

###装袋法  

减小统计学习方法方差的通用做法：**自助法聚集（bootstrap aggregation）**或称**装袋法（bagging）**，可以改善多种回归模型的预测效果，对决策树尤其有用。  

**定量情况**：从训练集中重复抽样，生成B个不同的自助抽样训练集，用第b个自助抽样训练集拟合模型并求得预测值$\hat{f}^{*b}(x)$，最后对所有预测值求平均：$$\hat f_{bag}(x)=\frac{1}{B}\sum_{b=1}^{B}\hat{f}^{*b}(x)$$

B个不同的自助抽样训练集建立起B棵回归树，这些树都是未经剪枝的，方差高而偏差小。对这B棵树的B个拟合值求平均可以减小方差，通过成百上千棵树的组合，袋装法能大幅提升预测准确性。  

**定性情况**：B棵树给出B个预测类别，采用多数投票（majority vote）的方法：把B个预测中出现频率最高的类作为总体预测  

树的个数B不是一个对袋装法起决定作用的参数，B很大时，也不会产生过拟合。在实际中，取足够大的B时，误差能够稳定下来。  



**袋外误差估计**

装袋法每棵树大概可以用三分之二的观测值，剩余三分之一称为树的袋外（out of bag，OOB）观测值。可以用所有将第i个观测值作为OOB的树来预测第i个观测值的响应值，这样会生成B/3个对第i个观测值的预测，求平均或者执行多数投票，可以得到第i个观测值的OOB预测。以此求得每一个观测值的OOB预测，就可以计算得到总体的OOB均方误差（对回归问题）或分类误差（对分类问题），这个是对装袋法模型测试误差的有效估计。  

当B足够大的时候，OOB误差实质上与留一法交叉验证误差是等价的，在大数据集上使用装袋法时，使用交叉验证会让计算变得相当麻烦，此时用OOB方法估计测试误差就很方便。  

**问题**：与单棵树相比，袋装法可以提高预测的准确性，但是由此得到的模型可能很难解释，他对预测准确性的提升是以牺牲解释性为代价的。  

###随机森林

随机森林的过程可以看作是对树**去相关（decorrelate）**，这样的树得到的平均值有更小的方差，因而树的可信度也更高。  

在随机森林中需对自助抽样训练集建立一系列决策树，这与袋装法类似，不过在建立这些决策树时，每考虑树上的一个分裂点，都要从全部的p个预测变量中选出一个包含m个预测变量的随机样本作为候选变量，这个分裂点所用的预测变量只能从这m个变量中选择。在每个分裂点处都重新进行抽样，选出m个预测变量。  

当m=p时，等同于建立袋装法树。当许多预测变量相关时，取较小的m值建立随机森林通常很有效。通常取$m\approx\sqrt{p}$，也就是说，每个分裂点考虑的预测变量个数约等于预测变量总数的平方根，这就意味着，算法将大部分可用的预测变量排除在考虑范围之外。

在装袋法树的集合中，大多树都会将最强的预测变量用于顶部分裂点，这会造成所有的袋装法树看起来很相似，因为这些装袋法树中的预测变量是高度相关的。

但是  
1、对不相关的量求平均  
2、对许多高度相关的量求平均  

这两者带来的方差减小的程度，1比2大得多。随机森林通过强迫每个分裂点仅考虑预测变量的一个子集，克服这个相关的问题。  

![](http://i1.fuimg.com/611786/0cc8430049cbe27e.png)



###提升法

提升法(boosting)与装袋法类似，也能改进许多用于解决回归或分类问题的统计学习方法。

提升法的每棵树的生成要用到之前生成的树的信息。是一种**学习舒缓（learning slowly）**的统计学习方法，与一般的方法相比往往有较好的预测效果。

![](http://i1.fuimg.com/611786/11480331154a4536.png)

**三个参数**

1、树的总数B。B值过大可能会出现过拟合（与装袋法和随机森林不同）。用交叉验证来选择B

2、取极小正值的压缩参数$\lambda$。控制着提升法的学习速度。通常取0.001或者0.01，如果$\lambda$很小，则需要很大的B才能获得良好的预测效果。

3、每棵树的分裂点数d。控制着整个提升模型的复杂性。取1建模通常能得到上佳效果，此时每棵树都仅由一个分裂点构成。

![](http://i1.fuimg.com/611786/5e2348bdb2d11212.png)


##程序

###构建分类树

使用ISLR包中的Carseats数据集进行分类树的创建
```{r warning=FALSE,message=FALSE}
library(ISLR)
High<-ifelse(Carseats$Sales<=8,"No","Yes")
Carseats<-cbind(Carseats,High)
```

**建模**

```{r warning=FALSE,message=FALSE}
library(tree)
fit1<- tree(High ~.-Sales, Carseats)#用除了Sales 之外的所有变量预测High
summary(fit1)#summary列出了生成终端节点的所有变量 以及训练错误率
```

**结果解读**：用除了Sales之外的所有变量预测High，一共有400条记录，用于生成终端结点的有8个变量："ShelveLoc"，"Price"，"Income"，"CompPrice"，"Population"，"Advertising"，"Age"，"US"   ，终端结点个数$T_0=27$，训练错误率是9%

**画图**

```{r warning=FALSE,message=FALSE}
plot(fit1)
text(fit1,pretty=0,cex=0.4)#pretty=0用于输出所有定性预测变量的类别名，而非首字母
```

```{r warning=FALSE,message=FALSE}
fit1
```
**结果解读**：split（分裂规则）, n（这一分支上的观测数量）, deviance（偏差）, yval（这一分支的整体预测）, (yprob)（这一分支中取No和取Yes的比例），后面带“*”的是终端结点

![](http://i1.fuimg.com/611786/459dd55fc6e74ec2.png)

**分训练集和测试集**

```{r warning=FALSE,message=FALSE}
set.seed(2)#随机种子为了划分训练集和测试集
train<-sample(1:nrow(Carseats),200)
test<-Carseats[-train,]
High.test<-High[-train]

fit2<-tree(High~.-Sales,Carseats,subset = train)
pred<-predict(fit2,test,type="class")
tab<-table(pred,High.test)
tab
sum(diag(prop.table(tab)))
```

**结果解读**:能对测试集上71.5%的数据做出正确预测

**剪枝**

```{r warning=FALSE,message=FALSE}
set.seed(3)
fit3<-cv.tree(fit2,FUN = prune.misclass)
#FUN = prune.misclass意味着用分类错误率来控制交叉验证和剪枝过程
fit3
```
**结果解读**:"size"是所考虑的每棵树的终端结点数，"dev"是对应的交叉验证错误率，当size为9时，交叉验证错误率最低，共有50个交叉验证误差， "k" 是使用的成本复杂性参数(代价复杂性剪枝中的$\alpha$,可以理解为惩罚系数)

```{r warning=FALSE,message=FALSE}
par(mfrow=c(1,2))
plot(fit3$size,fit3$dev,type="b",main="错误率关于size的变化")
plot(fit3$k,fit3$dev,type="b",main="错误率关于k的变化")
```

**结果解读**：
左边错误率关于size的变化可以理解为树的终端结点增加，对真实决策边界更接近，错误率下降，size继续增加，导致过拟合，对验证集的泛化能力下降，错误率上升。    

右边错误率关于K(成本复杂性参数)的变化可以理解为，当惩罚系数增加时，降低了过拟合程度，使得错误率下降，但继续增加惩罚系数时，导致欠拟合，错误率快速上升。 

**用prune.misclass()剪枝**
```{r warning=FALSE,message=FALSE}
fit4<-prune.misclass(fit2,best=9)#best越大，也即要的分类树的终端结点个数越多，剪枝后的树会更大，分类的准确率会更低
plot(fit4)
text(fit4,pretty=0,cex=0.8)
```

**剪枝后的效果**
```{r warning=FALSE,message=FALSE}
pred2<-predict(fit4,test,type="class")
tab2<-table(pred2,High.test)
tab2
sum(diag(prop.table(tab2)))
```

**结果解读**:剪枝后生成了一棵更加易于解释的树，且提高了分类的准确率

###构建回归树

使用MASS包中的Boston数据集进行回归树的创建

```{r warning=FALSE,message=FALSE}
library(MASS)
set.seed(1)
train1<-sample(1:nrow(Boston),nrow(Boston)/2)
fit6<-tree(medv~.,Boston,subset = train1)
summary(fit6)
plot(fit6)
text(fit6,pretty=0)

pred<-predict(fit6,Boston[-train1,])
plot(pred,Boston[-train1,"medv"])
abline(0,1)
mean((pred-Boston[-train1,"medv"])^2)#计算测试均方误差
```
**结果解读**:lstat(社区财富水平)代表社会经济地位底的个体所占比例，这棵树表明lstat值低对应的房价高；回归树的测试均方误差为25.05，所以均方误差平方根是5.005，这意味着这个模型的测试预测值与郊区真实房价的中位数之差在5005美元之内

**剪枝**
```{r warning=FALSE,message=FALSE}
set.seed(3)
fit7<-cv.tree(fit6)#默认用偏差（deviance）来控制交叉验证和剪枝过程
plot(fit7$size,fit7$dev,type="b",main="错误率关于size的变化")
fit8<-prune.tree(fit6,best=5)
plot(fit8)
text(fit8,pretty=0)
```

 


###装袋法和随机森林

**装袋法**是m=p时的随机森林，是随机森林的一种特殊情况，仍旧使用MASS包中的Boston数据集

```{r warning=FALSE,message=FALSE}
library(randomForest)
set.seed(1)
fit61<-randomForest(medv~.,Boston,subset = train1,mtry=13,importance=TRUE)
#mtry=13即在树的每一个分裂点都要考虑所有预测变量，也就是执行装袋法
pred1<-predict(fit61,Boston[-train1,])
plot(pred1,Boston[-train1,"medv"])
abline(0,1)
mean((pred1-Boston[-train1,"medv"])^2)#计算测试均方误差
```

**结果解读**:测试均方误差为13.5，几乎是单棵回归树树的测试均方误差的一半

**随机森林**

```{r warning=FALSE,message=FALSE}
library(randomForest)
set.seed(1)
fit62<-randomForest(medv~.,Boston,subset = train1,mtry=4,importance=TRUE)
#mtry=4，即树上的每个分裂点都只考虑4个预测变量
pred2<-predict(fit62,Boston[-train1,])
plot(pred2,Boston[-train1,"medv"])
abline(0,1)
mean((pred2-Boston[-train1,"medv"])^2)#计算测试均方误差
```

**结果解读**:测试均方误差为11.6076，意味着在这种情况下，随机森林比装袋法有所提升

**注意**：mtry这个参数建立回归树时默认是p/3，建立分类树时默认取$\sqrt{p}$个变量;randomForest()中还有一个参数是ntree，用来改变生成的树的数目，默认生成500棵树。

###提升法

```{r warning=FALSE,message=FALSE}
library(gbm)
set.seed(1)
fit63<-gbm(medv~.,Boston[train1,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)

fit64<-gbm(medv~.,Boston[train1,],distribution="gaussian",n.trees=5000,interaction.depth=4)

par(mfrow=c(1,2))

pred3<-predict(fit63,Boston[-train1,],n.tree=5000)
plot(pred3,Boston[-train1,"medv"])
abline(0,1)


pred4<-predict(fit64,Boston[-train1,],n.tree=5000)
plot(pred4,Boston[-train1,"medv"])
abline(0,1)

mean((pred3-Boston[-train1,"medv"])^2)#计算测试均方误差
mean((pred4-Boston[-train1,"medv"])^2)#计算测试均方误差
```

**注意**：gbm()中，回归问题distribution="gaussian"，分类问题distribution="bernoulli"，n.trees=5000说明提升法一共需要5000棵树，interaction.depth=4每棵树的深度为4，shrinkage=0.2是压缩参数$\lambda$，默认为0.001。上面两个模型的区别在于压缩参数$\lambda$

变量的重要性度量未涉及，随机森林算法可以计算变量的**相对重要程度**


#缺失值处理

现实任务中常会遇到不完整样本，即样本的某些属性值缺失，如果简单的放弃不完整样本，仅适用无缺失值的样本来进行学习，显然是对数据信息的极大浪费。显然，有必要考虑利用有缺失属性值得训练样本样例来进行学习。

我们需解决两个问题：  
(1)如何在属性值缺失的情况下进行划分属性选择?  
(2)给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分?    

给定数据集D和属性a，令$\tilde{D}$表示属性a上没有缺失值的样本子集。  
对问题(1)，显然我们仅可根据$\tilde{D}$来判断属性a的优劣。  

假定属性a有V个可取值,令$\tilde{D^v}$表示$\tilde{D}$中在属性a上取值为${a^v}$的样本子集。    


假定我们为每个样本$x$赋予一个权重$\omega_{x}$(在决策树开始阶段，跟结点中各样本的权重初始化为1)    
$\tilde{p_{k}}$表示无缺失值样本中第k类所占比例  
$\tilde{r_{v}}$表示无缺失值样本中在属性a上取值$a^v$的样本所占比例  


基于上述定义，我们可以将信息增益率的计算公式推广为：$$Gain(D,a)=\rho(Ent(\tilde{D})-\sum_{v=1}^{V}\tilde{r_{v}}Ent(\tilde{D^v}))$$






其中$$Ent(\tilde{D^v})=-\sum_{k=1}^{Y}\tilde{p_{k}}log_{2}\tilde{p_{k}}$$

对问题(2)
若样本$x$在划分属性a上的**取值已知**，则将$x$划入与其取值对于的子结点，且样本权值在子结点中保持为$\omega_{x}$。  

若样本$x$在划分属性a上的**取值未知**，则将$x$同时划入**所有子结点**，且样本权值在与属性值$a^v$对应的子结点中调整为**$\tilde{r_{v}}$*$\omega_{x}$**;直观上看，就是让同一个样本以不同的概率划入到不同的子结点中去。(C4.5算法就使用了上述解决方案)

****

#多变量决策树

将各个属性看作是一个坐标轴，则d个属性对应的样本，对应着d维空间中的一个点。对样本分类意味着，在这个空间中寻找不同样本之间的分类边界。  

决策树形成的分类边界：轴平行。分类边界由若干个与坐标轴平行的分段构成。这使得结果具有较好的可解释性。  

![](http://i2.tiimg.com/611786/2031150c655d4a76.jpg)

然而如果任务的分类边界较为复杂，必须用很多段才可以凑出来。  

![](http://i2.tiimg.com/611786/d3585ea0487c1468.jpg)

此时决策树相当复杂，进行大量的属性测试，时间开销大。  

若可以使用斜的划分边界，那就省事了。  

多变量决策树，可实现斜划分，非叶结点不再是某个属性，而是属性的线性组合。与传统单变量决策树不同（为每个非叶结点寻找一个最优划分属性），而是建立一个合适的线性分类器。  

![](http://i2.tiimg.com/611786/ff0a7ed6c4871fbf.jpg)