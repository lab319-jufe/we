---
title: "第2章"
author: "廖彩程"
date: "2019/5/27"
output: html_document
---
*学习资源来自于Gareth James的《统计学习导论——基于R应用》*

# 2.1 什么是统计学习 
![Markdown](http://i2.tiimg.com/611786/854ce55de137dcf8.png)

　　受客户委托做统计咨询，为某产品的销量提升提供策略咨询建议，Advertiising数据集记录了该产品在200个不同市场的销售情况及在每个市场中3类广告媒体的预算。虽然并没有提供产品增加的直接证据但是这些信息还是可以帮客户如何控制每个市场在3类媒体广告上的费用支出。该案例的目标是开发一个基于3类广告媒体预算的精确预测销量的模型。 

　　在该案例中，广告预算是输入变量(input variable),sales为输出变量(output variable)。

　　输入变量通常用大写字母X表示，下标区分不同的变量，输入变量又叫预测(predictor)变量、自(independent)变量、属性(feature)变量，甚至变量(variable)。输出变量通常用Y表示，称为响应(response)变量或者因变量(dependent variable)。

　　一般情况下，假设观察到一个定量的相应变量和p个不同的预测变量，记为X1,X2,…。假设这个Y和$X = (X_1, X_2, \cdots, X_p)$有一定的关系，可以表达成一个比较一般的形式 
$$Y = f(X) + \epsilon \tag{2.1}$$
　　f是$X_1, X_2, \cdots, X_p$的函数，它是固定的但未知，$\epsilon$是随机误差项(error term),与X独立，且均值为0。
![Markdown](http://i1.fuimg.com/611786/7dba128bd185670a.png)

　　该图表达了Income(收入)数据集中30个人的Income与years of education(受教育年限)的关系。竖线代表误差项$\epsilon$。总体来看，与拟合线的误差均值接近于0。

　　一般而言，估计函数f会涉及多个输入变量，如下图。作income对year of education和seniority(专业资质)的函数f。这里f 是一个基于观测值估计的二维曲面

![Markdown](http://i2.tiimg.com/611786/3b66b8f3548f3651.png)

　　实际上，统计学习是关于估计f的一系列方法。

## 2.1.1 什么情况下需要估计f

　　主要原因:预测(prediction)和推断(inference)

### 预测

　　许多情况下，输入集X是现成的，但输出Y是不易获得的。由于误差项对的均值是0.则可通过下式预测Y:
$$\hat{Y} = \hat{f}(X)\tag{2.2}$$
　　这里$\hat{f}$表示f的预测，$\hat{Y}$表示Y的预测值。$\hat{f}$是黑箱(black box),表示一般意义下，如果该黑箱能提供准确的预测值Y,则并不十分追求$\hat{f}$的确切形式。
先看一个例子，假设$X_1, X_2, \cdots, X_p$是某个病人的血样特征，该样本在实验室很容易测量，Y测量了病人使用某药物后出现严重不良反应的风险。通过X预测Y是自然的，这样就可以避免将药物用于那些存在不良反应高风险的病人，因为对这类病人而言，这个估计值Y高的。
$\hat{Y}$作为相应变量Y的预测，其精确性依赖于两个量，一个是可约误差(reducible error),另一个是不可约误差(irreducible error)

　　大体上，当所选的$\hat{f}$不是f的一个最佳估计时，对模型估计的不准确也会引起一些误差，这个误差是可约的，因为只要选择更合适的统计学习方法去估计f,提高$\hat{f}$的精确度,就能降低这种误差。然而，因为Y还是一个关于$\epsilon$对的函数，预测任然存在误差，不论我们对f估计的多么准确，我们都不能减少$\epsilon$引起的误差。

　　$\epsilon$包含了对预测Y有用但却不可直接观察的变量，所以残差会大于0。

　　考虑给定的估计$\hat{f}$和一组预测变量X,将产生预测$\hat{Y}=\hat{f}(X)$。假设$\hat{f}$和X是固定的，于是很容易证明
$$\begin{align}
  E(Y-\hat{Y})^2 & = E[f(X)+\epsilon-\hat{f}(X)]^2 \\
  & = \underbrace{[f(X)-\hat{f}(X)]^2}_{\rm 可约误差}+\underbrace{Var(\epsilon)}_{\rm 不可约误差}\tag{2.3}
  \end{align} $$
　　$E(Y-\hat{Y})^2$代表预测量和实际值Y的均方误差或期望平方误差值,$Var(\epsilon)$表示误差项$\epsilon$的方差。

　　本书重点关注估计f的方法，使f有最小的可约误差。不可约误差提供了Y预测精度的一个上界，这个上届在实践中实际上是未知的。

### 推断
　
　　很多情况下，我们对当$X_1, X_2, \cdots, X_p$变化时对Y产生怎样的影响比较感兴趣，我们对估计f的目标是想明白X和Y的关系。这时，f不能当作黑箱看待，而是要知道它的具体形式。

　　在本书中，我们学习的建模例子都是预测、推断或者二者混合。

## 2.1.2 如何估计f
　　运用观测点去训练或者引导我们的方法怎样估计f,这些观察点称为训练数据(training data)

　　目标是：想找到一个函数关系$\hat{f}$对任意观察点(X, Y)都有$\hat{Y}≈\hat{f}(x)$。一般而言，这项任务的大多数统计学习方法都可以分为两类：参数方法和非参数方法。

### 参数方法
　　参数方法是一种基于模型估计的两阶段方法

1. 首先，假设函数f具有一定的形式或形状。例如，一个常用的假设是假设f是线性的，具有如下形式
$$f(X)\,=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p\tag{2.4}$$
　　这是线性模型(linear model)

2. 一旦模型被选定后，就需要用训练数据去拟合(fit)或训练(train)模型。在(2.4)中，需要估计参数$\beta_0,\beta_1,\beta_2,\cdots,\beta_p$。这就是说，要确定参数的值，满足$Y≈\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p$,拟合(2.4)最常用的方法称为(普通)最小二乘法(ordinary least squares)

　　基于模型的方法统称为参数法(parametric);参数法把估计f的问题简化到一组参数。

　　下图给出了一个参数模型的例子，其中数据来自于上图中的Income(数据)
![Markdown](http://i2.tiimg.com/611786/faeead57a303cc3a.png)

　　一个线性拟合如下所示：
$$income≈\beta_0+\beta_1 \times education + \beta \times seniority$$
　　我们发现上图中的线性拟合不够精确，真正的f有一定的曲率，线性拟合无法抓住这些特征，然而，线性拟合任然看起来是一个比较合理的估计，因为它把握了year of education和income之间的正相关关系，还成功捕获了seniority和income之间微弱且不易察觉的正相关关系。

### 非参数方法

　　非参数方法不需要对函数f的形式事先做出明确的假设，追求的是接近数据点的估计。非参数方法较参数方法的优点在于：不限定函数f的具体形式但有一个致命的弱点：无法将估计f的问题简化到仅仅对少数参数进行估计的问题，所以为了获得对f的更为精确的估计，往往需要大量的观测点。  
　　
　　下图表示了用非参数方法对Income数据应用薄板样条(thin-plate spline)估计f的拟合结果。
![Markdown](http://i2.tiimg.com/611786/1b9390dd15c0360f.png)

　　为了拟合一个薄板样条，数据分析师必须指定一个光滑度水平中，也称柔性水平。下图就是使用一个较低的柔性水平用类似的方法拟合的薄板样条，得到一个完整地匹配了每一个观测数据的折皱不平的拟合。同时，也是过拟合。
![Markdown](http://i1.fuimg.com/611786/e2789f3f94a4787c.png)

## 2.1.3 预测精度和模型解释性的权衡
　　有一下几种原因导致我们会选择限定性更强的建模方式。如果建模的主旨在于推断，那么采用结构限定的模型则模型解释性比较强。相反，相当复杂的光滑模型很难解释每一个单独的预测变量是如何影响响应变量的。

　　几种统计学方法在柔性和解释性之间的权衡，一般来讲，当一种方法的柔性增强时，其解释性则减弱。
![Markdown](http://i1.fuimg.com/611786/cb2c928cbe3f6036.png)

　　综述所述，当数据分析的目标是推断时，运用简单又相对欠光滑的统计学方法具有明显的优势。然而在仅仅是对预测感兴趣时，，更精确的预测常常是在欠光滑度的模型上取得的。q欠光滑度模型乍一看上去会感觉违反直觉，然而这正是其抗高光滑模型过拟合缺陷的能力所在。

## 2.1.4 指导学习于无指导学习

　　大部分统计学习问题分为以下两种类型：指导学习和无指导学习

### 指导学习(监督学习)

　　迄今为止，本章已经讨论过的例子都属于指导学习范畴。对每一个预测变量观测值$x_i(i=1,\,\cdots,\,n)$都有相应的响应变量的观测$y_i$。许多传统的学习方法，比如线性回归和逻辑斯谛回归（logistic regerssion）。广义可加模型（GAM）、提升方法和支持向量机（SVM）等比较现代的方法，都属于指导学习范畴。

### 无指导学习(无监督学习)

　　无指导学习则在一定程度上更具挑战性，只有预测变量的观测向量$x_i(i=1,\,\cdots,\,n)$,这些向量没有相应的响应变量$y_i$与之对应。对这类问题拟合线性模型是不可能的，因为缺乏响应变量用于预测。这时，建模工作在某种程度上来看仿佛是在黑暗中的摸索；这种情形就称为无指导。无指导学习还包括：聚类分析，理解变量之间或观测之间的关系；主成分分析，从多个变量中得到低维变量的有效方法等。

### 半指导学习

　　大部分方法可以自然地归为指导学习或非指导学习范畴，但是，有时一种分析应该被归为指导学习还是无指导学习则欠明了，我们称这种问题为半指导学习。例如，假设有n个观测。其中m(m<n)个观测点，可以同时观测到预测变量和响应变量。而对于其余n-m个观测点，只能观测到预测变量但无法观测到响应变量。比如对预测变量的采集相对简单，而相应的响应变量却比较难采集，就会出现这种情况。我们称这种问题为半指导学习（semi supervised learning）问题。
　　
　　指导和无指导的区别在于是否存在与预测变量相对应的响应变量$y_i$指导的统计学习工具主要用于：一是面向预测的统计模型的建立，二是对一个或多个给定的输入(input)估计某个输出(output)。在无指导的统计学习问题中，有输入变量但不指定输出变量，建模的主旨是学习数据的关系和结构。

## 2.1.5 回归与分类问题

　　变量常分为定量和定性两种类型。我们习惯将响应变量为定量的问题称为回归分析问题，将具有定性响应变量的问题定义为分类问题。
根据响应变量是定性的还是定量的来选择所需统计学习方法是数据分析的常规思维，当响应变量是定量是，通常选用线性回归模型，当响应变量是定性变量时，用逻辑斯谛回归。预测变量是定性还是定量，通常对选择模型并不十分重要。 

# 2.2 评价模型精度

　　没有任何一种方法能在各种数据集里完胜其他所有的方法

## 2.2.1 拟合效果检验

　　为评价统计学习方法对某个数据集的效果，需要一些方法评测模型的预测结果与实际观测数据在结果上的一致性。对一个给定的观测，需要定量测量预测的响应值与真实响应值之间的接近程度。在回归中，最常用的评价准则是均方误差(mean squared error,MSE),表达式如下所示：
$$ MSE = \frac {1} {n} \sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2 \tag{2.5}$$
　　$\hat{f}(x_i)$是第i个观测点上应用$\hat{f}$的预测值。如果预测的响应值与真实的响应值很接近，则均方误差会非常小。

式(2.5)中的MSE是用训练数据计算出来的，它的预测精准的程度一般会比较高，我们称它为训练均方误差(training MSE)。
一般而言，我们并不关心这个模型在训练集中的表现如何，而真正的兴趣在于将模型用于测试（test）数据获得怎样的预测精度。
    训练均方误差用来判断拟合的好不好，测试均方误差用来判断预测效果好不好。当模型的光滑度增加时，训练均方误差会减小，但测试均方误差不一定会降低。训练均方误差小，测试均方误差大的现象成为**过拟合**，出现过拟合就意味着需要降低模型的光滑度，以此减小测试均方误差。

　　如果我们掌握了大量的测试数据，可以计算如下函数：
$$ Ave(\hat{f}(x_0) - y_0)^2 \tag{2.6} $$
　　这是测试点的均方预测误差，选择的模型应该力图使测试均方误差(test MSE)尽可能小。

　　选择一个使测试均方误差最小的模型的想法：

　　1. 使用一组没有被用于建立统计学习模型的观测数据做测试数据

　　2. 通过降低训练均方误差(2.5)来选择统计学习模型，但这个想法有一个致命的缺陷：一个模型的训练均方误差最小时，不能保证模型的测试均方误差同时会很小。
![Markdown](http://i2.tiimg.com/611786/89035e634d41863e.png)

　　黑色曲线：真实的f,橙色、蓝色和绿色曲线表示了三种可能的对f的估计。绿色线光滑性最强，与实际数据匹配最好，拟合真正的函数不好，过度曲折了。
灰色曲线：训练均方误差。 红色曲线：测试均方误差，所有的方法都应使得测试均方误差尽可能最小，水平虚线表示的是(2.3)中的不可约误差$Var(\epsilon)$,对应所有方法的最低测试均方误差。

　　当统计学习方法的光滑度增加时，观测到训练均方误差单调递减，测试均方误差呈现U形分布。这是统计学习的一个基本特征。

　　平滑样条曲线的正式术语是自由度(degree of freedom),自由度是一个用于描述曲线光滑度的量。

　　当所建立的模型产生一个较小的训练均方误差，但却有一个较大的测试均方误差，就称为该数据被过度拟合。无论过拟合是否发生，我们总是期望训练均方误差比测试均方误差要小。
![Markdown](http://i2.tiimg.com/611786/61f6473b8112cbc9.png)
　
 　　　　　　真实的f接近与线性

![Markdown](http://i2.tiimg.com/611786/c265d754c15ee4bb.png)

       　　 真实的f是非线性的

## 2.2.2 偏差-方差权衡
　　在给定值$x_0$时，期望测试均方误差能分解成三个基本量的和，分别为：$\hat{f}(x_0)$的方差、$\hat{f}(x_0)$偏差的平方和误差项$\epsilon$的方差，具体而言：
$$ E(y_0 - \hat{f}(x_o))^2 = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2 + Var(\epsilon) \tag{2.7} $$
　　$E(y_0 - \hat{f}(x_o))^2$是模型的期望测试均方误差(expected test MSE),实际上是用大量训练数据重复估计后，又在$x_0$处带入不同的估计所得的平均测试均方误差(average test MSE)
期望测试均方误差可由测试集中每个可能的$x_0$处$E(y_0 - \hat{f}(x_o))^2$的平均来计算。

符号|含义
----|---
X   |测试样本
D   |数据集
$y_D$|X在数据集中的标记
y   |x的真实标记
f   |训练集D学得的模型
f(x:D)|由训练集D学得的模型f对x的预测输出
$\overline{f}(x)$|模型f对x的期望预测输出

对算法的期望泛化误差进行分解：
![Markdown](http://i2.tiimg.com/611786/1b5fe6c72bf4f931.jpg)

　　为使期望测试误差达到最小，需要选择一种统计学习方法使方差和偏差同时达到最小。
　　方差：代表的是用一个不同的训练数据集估计f时，估计函数的改变量。如果一个模型有较大的方差，那么训练数据集微小的变化则会导致$\hat{f}$较大的改变。一般来说，光滑度越高的统计模型有更高的方差。
　　偏差：为了选择一个简单的模型逼近真实函数而被带入的误差。光滑度越高的方法所产生的偏差越小。
一般而言，光滑度更高的方法，所得的模型方差会增加，偏差会减小。
![Markdown](http://i2.tiimg.com/611786/6295d5d2cd60e4d7.png)
　　
　　蓝色曲线：在不同光滑度下偏差的平方，橙色曲线：方差，水平虚线：不可约误差$Var(\epsilon)$ 红色曲线：三者的和
　　
　　蓝色实曲线表示在不同光滑度下偏差的平方，而橙色曲线表示方差，水平虚线表示不可约误差，即$Var(\epsilon)$。 最后，表达测试均方误差的红色曲线是三个量的和。在三个例子中，当模型的光滑度增加时，模型的方差增加，偏差减小。然而，最优测试均方误差所对应的光滑度水平在三个数据集中是不同的。在图 2-12 的左图，偏差迅速减小，使期望测试均方误差急剧减小。另一方面，图2-12的左图，真实的接近于线性，因此当模型的光滑度增加时，偏差只发生了微小的变化，而且测试均方误差在由方差增大所引起的迅速增长前仅出现了轻微的下降。最后由图2-12的右图，由于真实的f是非线性的，随着所选模型光滑度的增加，偏差会急剧减小。随着光滑度的增长，方差也有很小的增加。这时，测试均方误差在由模型光滑度的增长所引起的小增加之前出现了大幅下降。

　　如果一个统计学习模型被称为测试性能好，那么要求该模型有较小的方差和较小的偏差。

## 2.2.3 分类模型
　　
　　最常用的衡量估计$\hat{f}$精度的方法是训练错误率(error rate)，也就是说对训练数据使用估计模型$\hat{f}$所造成的误差比例，如下所示：
$$\frac{1}{n} \sum_{i=1}^{n} {I(y_i \neq \hat{y}_i)} \tag{2.8}$$
　　其中$\hat{y}_i$是使用$\hat{f}$预测数据的第$i$个值。$I(y_i \neq \hat{y}_i)$表示一个示性变量(indicator variable),当$y_i \neq \hat{y}_i$时，值等于1，当$y_i = \hat{y}_i$时，值等于0。 

　　在一组测试观测值$(x_0, y_0)$上的误差计算具有以下形式：
$$Ave(I(y_0 \neq \hat{y}_0)) \tag{2.9}$$
　　其中$\hat{y}_0$是用模型预测的分类标签。一个好的分类器应使用(2.9)表示的预测误差最小。

### 贝叶斯分类器
　　将一个待判的$x_0$分配到下面这个式子最大的那个$j$类上是合理的
$$ Pr(Y = j| X = x_0) \tag{2.10}$$
　　(2.10)是一个条件概率。它是给定了观测向量$x_0$条件下$Y = j$的概率。在一个二分类问题中，只有两个可能的响应值，一个称为类别1，另一个称为类别2.如果$Pr(Y = 1|X = x_0) > 0.5$,贝叶斯分类器将该观测的类别预测为1，否则预测为类别2。
![Markdown](http://i2.tiimg.com/611786/da3ff150cdf7c8c4.png)

　　橙色阴影部分：$Pr(Y = orange|x)$大于50%的点，蓝色区域：概率低于50%的点，紫色的虚线：概率等于50%的点，这条线称为贝叶斯决策边界。

　　贝叶斯分类器将产生最低的测试错误率，称为贝叶斯错误率。在$X = x_0$处的错误率将是$1 - max_jPr(Y=j|X=x_0)。一般来说，整个的贝叶斯错误率是
$$ 1 - E(\operatorname*{\max}_{j} \, Pr(Y = j |X)) \tag{2.11} $$
　　贝叶斯错误率是0.1304，比0大，因为这两个类在一些部分有交叠。贝叶斯错误率类似于之前探讨过的不可约误差。

### k最近邻方法
　　因为很难知道给定X后Y的条件分布，所以有的时候计算贝叶斯分类器是不可能的。它是一种难以达到的黄金标准。
K最近邻(KNN)分类器，给一个正整数K和一个测试观测值$x_0$，KNN分类从识别训练集中K个最靠近$x_0$的点集开始，用$\scr N_0$表示K个点的集合，然后对每个类别J分别用$\scr N_0$中的点估计一个分值作为条件概率的估计，这个值等于j:
$$ Pr( Y = j \,  | \, X = x_0) = \frac{1} {K} \sum_ {i \in \scr N_0 }I(y_i = j) \tag{2.12} $$
　　最后，对KNN方法运用贝叶斯规则将测试观测值$x_0$分到概率最大的类中。
![Markdown](http://i1.fuimg.com/611786/ff7bd321c5752a49.png)

　　尽管这种方法原理简单，但KNN确实能够产生一个对最优贝叶斯分类器近似的分类器。 

![Markdown](http://i1.fuimg.com/611786/f98713e3d4992311.png)

　　k=10,该例中，KNN方法的测试错误率是0.1362，近似与贝叶斯错误率0.1304。

　　K的选择对获得KNN分类器有根本性的影响。

![Markdown](http://i1.fuimg.com/611786/c415a634c09f66e8.png)

　　K=1 ，偏差较低，方差很大，测试错误率为0.1695，k=100，方差较低，偏差很大，错误率为0.1925

　　正如回归设置中，训练错误率和测试错误率之间没有一个确定的关系。一般而言，当使用光滑度较高的分类方法时，训练错误率将减小但测试错误率则不一定很小。

![Markdown](http://i1.fuimg.com/611786/50f56992b506f21d.png)

　　数据所生成的KNN分类器训练错误率(蓝色，200观测点)和测试错误率(橙色，5000观测点)，黑色的虚线表示贝叶斯错误率
当$\frac{1}{K}增加时，方法的柔性增强。在回归设置中，当光滑度增加时，训练错误率会持续递减，但测试误差显式为U形。
选择合适的光滑水平是成功建模的关键。

# 2.3 实验：R语言简介
[R]http://cran.r-project.org/

## 2.3.1 基本命令
```{r}
x <- c(1, 3, 2, 5)#将数字1,3,2,5连在一起，并将它们保存到一个名为x的向量中
x
x = c(1, 6, 2)#也可以用=保存
x
y = c(1, 4, 3)
?funcname#在R窗口里打开一个帮助文件
length(x)
length(y)
x + y
ls()#查看所有的对象列表
rm(x,y)#去除x,y
ls()
rm(list = ls())#消除所有对象
```

```{r}
x <- matrix(data = c(1,2,3,4), nrow = 2, ncol = 2)  #默认按列排序
x
x <- matrix(c(1,2,3,4), 2, 2)
matrix(c(1,2,3,4), 2, 2, byrow = TRUE)#并未给矩阵赋值，意味着这个矩阵仅用于打印在屏幕上，不能为将来计算所用
sqrt(x)#开方
x^2
x <- rnorm(50)#生成服从标准正态分布的随机数
y <- x + rnorm(50, mean = 50, sd = .1)#生成服从正态分布的50个随机数，均值是50，标准差为0.1
cor(x,y)#计算x,y的相关系数
set.seed(1303)#进行输出设置，使代码产生完全相同的随机数，括号里可以是任意整数
rnorm(50)
```
```{r}
set.seed(3)
y <- rnorm(100)
mean(y)
var(y)#计算向量方差
sqrt(var(y))
sd(y)#计算标准差
```
## 2.3.2 图形
### plot()
```{r}
x <- rnorm(100)
y <- rnorm(100)
plot(x, y)
plot(x, y, xlab = "this is the x-axis", ylab = "this is the y-axis", main = "Plot of X vs Y")
pdf("Figure.pdf")#建立一个pdf文件
jpeg("Figure.jpeg")#建立一个jpeg格式的输出文件
dev.off()#指示用R创建图形的工作到此为止
```

```{r}
# seq(a, b)在a和b之间建立一个整数向量
seq(0,1,length = 10)#0,1之间等距的10个数的序列
x <- seq(1, 10)
x
x <- 1 : 10
x
x <- seq(-pi, pi, length = 50)
x
```
### contour()
```{r}
y <- x
y
f <- outer(x, y, function(x, y) cos(y)/(1 + x^2))
contour(x, y, f)
contour(x, y, f, nlevels = 45, add = T)#nlevels等高线的水平的数值
fa <- (f - t(f))/2
contour(x, y, fa, nlevels = 15)
```
### image() 
```{r}
image(x, y, fa)#产生一个有颜色的图形，颜色随Z值的不同而不同，即热地图
persp(x, y, fa)#产生一个三维图
persp(x, y, fa, theta = 30)#
persp(x, y, fa, theta = 30, phi = 20)#参数theta和phi可以控制观测角度
persp(x, y, fa, theta = 30, phi = 70)
persp(x, y, fa, theta = 30, phi = 40)
persp(x, y, fa, theta = 50, phi = 40)
```
## 2.3.3 索引数据
```{r}
A <- matrix(1:16,4)
A
A[2, 3]#选择第二行第列元素
A[c(1,3), c(2,4)]#选择第1行和第3行，第2列和第4列
A[1:3, 2:4]#选择A的1到3行，2到4列
A[1:2, ]
A[, 1:2]
A[1,]#R中把一个单行或者单列称为一个向量
A[-c(1, 3),]#在索引里用一个负号，表示不包含指示的行或列
A[-c(1, 3), -c(1, 3, 4)]
dim(A)#输出一个矩阵的行数和列数
```
## 2.3.4 载入数据
read.table()函数是最基本的方法之一，将数据从文本文件加载到R
write.table()函数可输出数据
```{r}
Auto <- read.table("E:\\319\\tjxxdljyRyy\\data\\Auto.data")
fix(Auto)
Auto <- read.table("E:\\319\\tjxxdljyRyy\\data\\Auto.data", header = T, na.strings = "?")
#header = T ,告知R，文件的第一行包含变量名，na.strings告知R在扫描数据的任何位置只要遇到指定的字符特征或者一个特征集，应该在数据矩阵中对这条数据做缺失标记
#数据被保存为一个csv文件(以“，”分隔的文件)，用read.csv()函数很容易读取
Auto <- read.csv("E:\\319\\tjxxdljyRyy\\data\\Auto.csv", header = T, na.strings = "?")
fix(Auto)
dim(Auto)
names(Auto)#查看该数据的变量名
```
## 2.3.5 其他的图形和数值汇总
```{r}
plot(Auto$cylinders, Auto$mpg)
attach(Auto)
plot(cylinders, mpg)
cylinders <- as.factor(cylinders)#将cylinders因子化，转化为一个定性的变量
#如果要绘制在x轴上的变量是定性的，箱线图将自动通过plot()函数产生
plot(cylinders, mpg)
plot(cylinders, mpg, col = "red")
plot(cylinders, mpg, col = "red", varwidth = T)#箱线图的宽度
plot(cylinders, mpg, col = "red", varwidth = T, horizontal = T)#转换方向
plot(cylinders, mpg, col = 2, varwidth = T, xlab = "cylinders", ylab = "MPG")
# hist()-----绘制直方图， col = 2等价于col = "red"
hist(mpg)
hist(mpg, col = 2)
hist(mpg, col = 2, breaks = 15) #breaks参数表示带宽
# pairs()-----建立一个对任何指定数据集中每一对变量的散点图矩阵
pairs(Auto)
pairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)
plot( horsepower, mpg)
identify(horsepower, mpg, name)  #单击一个图上的某个点将引导打印指定变量上这个点的值。右击这个图将退出identify ()函数当(第一个)鼠标按钮被按下时，identify读取图形指针的位置。然后搜索给定的x和y坐标，寻找最接近指针的点。如果这个点足够接近指针，它的索引将作为调用值的一部分返回 
summary(Auto)   #数值汇总
summary(mpg)
```