---
title: "统计学习基础概念"
output: html_notebook
---

> 文件创建：杨钰萍 2017级 2017年秋/n
> 第一次更新：陈玲倩 2018级 2018年9月23日


数据结构分为向量、矩阵、数组、数据框、因子和列表.请参考R语言实战和Introduction to R for Data Science(https://courses.edx.org/courses/course-v1:Microsoft+DAT204x+2T2017/course/)

如果你使用的不是RStudio，而是VS2017或者RGUI，关于RMarkdown的使用请参考《R语言实战》Ch22

参考：R语言实战、计量经济学、应用线性回归模型、实用多元统计分析

以下部分语句涉及到简单的函数编写及调用，涉及到的数据及包参见下表


R包|MASS|ISLR|基础包
:-:|:-:|:-:|:-:
数据集|Boston|Auto、smarket|state.x77


---



## 拟合效果检验

训练均方误差用来判断拟合的好不好，测试均方误差用来判断预测效果好不好。

当模型的光滑度增加时，训练均方误差会减小，但测试均方误差不一定会降低

训练均方误差小，测试均方误差大的现象成为**过拟合**，出现过拟合就意味着需要降低模型的光滑度，以此减小测试均方误差



左图黄蓝绿代表三种拟合模型，右图灰色曲线代表训练均方误差，红色曲线代表测试均方误差

从三个模型的角度，可以看到，**黄色**模型的训练均方误差和测试均方误差都很大，说明可能存在**欠拟合**情况；**绿色**模型的训练均方误差很小，测试均方误差较大，两者之间的差距很大，说明模型可能存在**过拟合**情况；**蓝色**模型的的训练均方误差和测试均方误差都相对较小，且两者之间的差距也较小，因此在这三个的模型中，**蓝色**模型相对较优。

从右图图线的角度，**灰色**曲线代表了训练均方误差，可以看到随着模型光滑度的递增，训练均方误差越来越小。说明模型的训练均方误差可以通过增加模型的光滑度来改进。**红色**曲线代表了测试均方误差，图线呈**U**形，说明随着模型的光滑度增加，测试均方误差呈先减后增的态势。这就意味着，单一地通过增大模型光滑度，虽然可以减小训练均方误差，使得模型在训练集的表现变优，但是在超过**某一限度**时，会导致测试的均方误差增加，导致模型在测试集的表现不佳，出现过拟合情况。而判断这个限度在哪里，会在后面讨论（交叉验证等方法，还未讲到）


---

## 方差-偏差权衡

**方差**是指用不同的数据集去估计$f(.)$时，估计函数的改变量。对于光滑度低的模型，一般具有低方差性，也即当数据集中的某个点的位置进行偏移时，拟合函数不会有太大的改变。方差简单地说就是模型的**不稳定性**

**偏差**是指为了选择一个简单的模型来逼近真是函数而被带入的误差。一般来说，光滑度越高的模型，偏差越小。偏差简单地说就是模型的**不准确性**

下图中，以打靶为例，左上表示低偏差-低方差（好），右上图表示低偏差-高方差（过拟合），左下图表示高偏差-低方差（欠拟合），右下图表示高偏差-高方差（差）




我们要测试均方误差**最小**，意味着方差与偏差同时很小，但在一个实际系统中，方差和偏差往往是不能兼得的，如果要降低模型的偏差，就会一定程度上提高模型的方差，反之亦然。出现这种情况的根本原因是，我们总是希望用有限的训练样本去估计无限的真实数据。当我们更相信这些数据的真实性时而忽视对模型的先验知识，就会尽量保证模型在训练样本的准确度，这样就可以减少偏差。但是，这样学习到的模型，很可能会失去一定的泛化能力，从而造成过拟合，降低模型在真实数据上的表现，增加模型的不确定性。相反，如果我们更加相信我们对于模型的先验知识，在学习模型的过程中对模型有更多的限制，就可以降低模型的方差，提高模型的稳定性，但也会使模型的偏差增大。

方差与偏差的权衡表现在数学模型上，实际上是模型光滑度的问题。我们可以用下图的方式来判断最优的光滑度。如下图所示是三个数据集的方差-偏差图，蓝色曲线表示不同光滑度下偏差的平方，橙色曲线表示方差，水平虚线表示不可约误差，红色曲线是测试均方误差（也就是前三个量的总和），垂直虚线所对应的光滑度是每一个数据集的最优测试均方误差所对应的模型参数。



但是在现实问题中，$f$一般是未知的，这就意味着我们无法精确地计算测试均方误差、偏差和方差。但是，方差-偏差
权衡需要时刻谨记。



---

## 监督学习和无监督学习

**有监督学习：**回归、分类（区别在于输出的结果是否连续）

**无监督学习：**聚类、主成分

有监督和无监督的区别在于是否存在与预测变量相对应的响应变量$y_i$


---

## 先验概率和后验概率

先验概率是指根据以往经验和分析得到的概率

后验概率是在得到结果的信息后，重新修正的概率，是在考虑了事实后的条件概率，也就是事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小。后验概率的计算要以先验概率为基础。

假如给一些图片,这些图片中有的图上有动物的角,这些图片占了1/10(即**先验概率**),且已知在有角的条件下是犀牛的概率是0.8(类条件概率1,注意这个概率互补的概率是有角条件下不是犀牛的概率),已知在无角条件下是犀牛概率的是0.05(类条件概率2),现在拿起一张图,发现是一张犀牛的图,那么这张图上带角的概率有多大(求后验概率)



由图中公式可知**后验概率**为$P(图片上由动物的角|是犀牛) = 0.8*0.1/(0.8*0.1+0.05*0.9)=0.64$


---

## 参数方法和非参数方法

统计方法分为参数方法和非参数方法，

**参数方法：**基于模型的方法，把估计$f$的问题简化到估计一组参数

缺陷：选定的模型并非与真正的f在形式上是一致的，所以要拟合光滑度更高的模型，但是会带来过拟合问题

**非参数方法：**不需要对模型$f$的形式事先作出明确的假设，追求接近数据点的估计，估计函数在去粗和光滑处理后尽可能与更多数据点接近

优点：不限定函数形式，于是可能在更大的范围选择更适宜$f$形状的估计

缺点：需要大量观测点

下图是几种统计模型在光滑度和解释性之间的权衡
