---
title: "统计学习基础概念"
output: html_document
---

> 文件创建：杨钰萍 2017级 2017年秋
> 第一次更新：陈玲倩 2018级 2018年9月28日


# 前言

    数据结构分为向量、矩阵、数组、数据框、因子和列表.请参考R语言实战和Introduction to R for Data Science(https://courses.edx.org/courses/course-v1:Microsoft+DAT204x+2T2017/course/)
    如果你使用的不是RStudio，而是VS2017或者RGUI，关于RMarkdown的使用请参考《R语言实战》Ch22
    参考：R语言实战、计量经济学、应用线性回归模型、实用多元统计分析
    以下部分语句涉及到简单的函数编写及调用，涉及到的数据及包参见下表
    R包|MASS|ISLR|基础包
    :-:|:-:|:-:|:-:
    数据集|Boston|Auto、smarket|state.x77


---

# 第一讲 统计基础

## 1.1统计学习概述

    统计学习是一套以理解数据为目的庞大工具集。统计学习的工具可分为两大类：有指导的学习和无指导的学习。

### 指导学习（监督学习）

    许多传统的学习方法，比如线性回归和逻辑斯谛回归（logistic regerssion）。广义可加模型（GAM）、提升方法和支持向量机（SVM）等比较现代的方法，都属于指导学习范畴。

### 无指导学习（无监督学习）

    而无指导学习则在一定程度上更具有挑战性，在这种情形中，只有预测变量的观测向量，而没有相应的响应变量。对这类问题拟合线性模型是不可能的，因为缺乏响应变量用于预测。这时，建模工作在某种程度上来看仿佛是在黑暗中的摸索；这种情形就称为无指导。无指导学习还包括：聚类分析，理解变量之间或观测之间的关系；主成分分析，从多个变量中得到低维变量的有效方法等。

### 半指导学习

    大部分方法可以自然地归为指导学习或非指导学习范畴。但是，有时一种分析应被归为指导学习还是无指导学习则欠明了。例如，假设有n个观测。其中m(m<n)个观测点，可以同时观测到预测变量和响应变量。而对于其余n-m个观测点，只能观测到预测变量但无法观测到响应变量。比如对预测变量的采集相对简单，而相应的响应变量却比较难采集，就会出现这种情况。我们称这种问题为半指导学习（semi supervised learning）问题。

    指导和无指导的区别在于是否存在与预测变量相对应的响应变量$y_i$指导的统计学习工具主要用于：一是面向预测的统计模型的建立，二是对一个或多个给定的输入(input)估计某个输出(output)。在无指导的统计学习问题中，有输入变量但不指定输出变量，建模的主旨是学习数据的关系和结构。

### 工资数据（回归问题）

    首先我们考察工资与美国中部大西洋地区男性收入相关的几个因素。根据实验结果可得，当然经验分析也可得出：平均来看，随着age的增长wage增加，这种趋势在60岁左右终止，之后wage开始下降；以及wage与year的函数关系，呈现出缓慢而稳定的增长；wage随受教育水平呈现递增关系。wage数据中的输出变量数据类型是连续型，也称为定量型。这类问题通常称为回归问题。

### 金融市场数据（分类问题） 

    考察股票市场数据集时，该数据集的目标是用过去5天指数的变动比例预测5天后股指的涨跌状态。这个问题中的统计学习模型不是对数值类型的变量预测，而是关注某一天的股市业绩是掉进up桶还是down桶。

### 基因表达数据（聚类问题）

    上述两个应用的数据集中既有输入变量也有输出变量。然而还有另一类不同的重要问题，既观察到的只有输入变量却没有相应的输出变量。例如，在营销领域中，可以采集到现有客户的人口信息。建模的目标是希望理解哪些类型的客户在他们所观察到的属性上彼此相似，并形成一个族群，这就是一个聚类问题。与前面例子的不同在于，这个问题并不打算预测输出变量。

## 1.2拟合效果检验

    为评价统计学习方法对某个数据集的效果，需要一些方法评测模型的预测结果与实际观测数据在结果上的一致性。这时，对一个给定的观测，需要定量测量预测的响应值与真实响应值之间的接近程度。在回归中，最常用的评价准则是均方误差（mean squared error, MSE）,MSE是用训练数据计算出来的，而这些训练数据本来就是用来拟合模型的，所以预测精准的程度会比较高，我们形象地称它为训练均方误差（training MSE）。     
    一般而言，我们并不关心这个模型在训练集中的表现如何，而真正的兴趣在于将模型用于测试（test）数据获得怎样的预测精度。
    训练均方误差用来判断拟合的好不好，测试均方误差用来判断预测效果好不好。当模型的光滑度增加时，训练均方误差会减小，但测试均方误差不一定会降低。训练均方误差小，测试均方误差大的现象成为**过拟合**，出现过拟合就意味着需要降低模型的光滑度，以此减小测试均方误差。

![Markdown](http://i2.tiimg.com/611786/5f8db74e6d07dcfe.png)

    图2-9
    左:由真实函数模拟产生的数据，用黑色曲线表示。三种f的估计:线性回归(橙色曲线)，两条光滑样条拟合(绿色和蓝色曲线)。右:训练均方误差(灰色曲线)，测试均方误差(红色q曲线) ，所有方法都己使测试均方误差尽可能最小。方块分别对应左图的三种拟合中的训练均方误差和测试均方误差。
  
    左图黄蓝绿代表三种拟合模型
    左图中真实的f由黑色曲线表示，橙色、蓝色、绿色曲线表示了三种可能的对f的估计，三种方法都考虑了估计函数形式的光滑度问题。橙色线是线性回归拟合，它是相对固定的形式。蓝色和绿色曲线是由不同的光滑度产生的光滑样条拟合。显然，当光滑度水平增加时，拟合的曲线与实际观测的数据更接近，绿色曲线光滑性最强，与实际数据匹配最好；然而，我们发现绿色线拟合真正的函数（黑色线）不好，绿线过度曲折了。通过调整平滑样条拟合的光滑度水平，对该数据可以产生许多不同的拟合估计。从三个模型的角度，可以看到，**黄色**模型的训练均方误差和测试均方误差都很大，说明可能存在**欠拟合**情况；**绿色**模型的训练均方误差很小，测试均方误差较大，两者之间的差距很大，说明模型可能存在**过拟合**情况；**蓝色**模型的的训练均方误差和测试均方误差都相对较小，且两者之间的差距也较小，因此在这三个的模型中，**蓝色**模型相对较优。

    右图灰色曲线代表训练均方误差关于光滑度的函数，红色曲线代表测试均方误差
    **光滑度**在平滑样条曲线的正式术语是**自由度**(degree of freedom).自由度是一个用于描述曲线光滑度的量。图中橙色、蓝色、绿色的方块所示的均方误差与左图的曲线相对应。限定性强且曲线平坦的模型比锯齿形曲线具有更小的自由度，如上图中的右图所示，线性回归是限定性较强的模型，只有两个自由度。当拟合函数的光滑度增加时，训练均方误差单调递减。该例中真正的f是非线性的，所以用橙色线拟合f时光滑度是不充足的。在三种拟合方法中，绿色曲线具有最小的训练 均方误差，因为它相对应左图中的三条曲线中最大的光滑度。

    从右图图线的角度，**灰色**曲线代表了训练均方误差，可以看到随着模型光滑度的递增，训练均方误差越来越小。说明模型的训练均方误差可以通过增加模型的光滑度来改进。**红色**曲线代表了测试均方误差，图线呈**U**形，说明随着模型的光滑度增加，测试均方误差呈先减后增的态势。这就意味着，单一地通过增大模型光滑度，虽然可以减小训练均方误差，使得模型在训练集的表现变优，但是在超过**某一限度**时，会导致测试的均方误差增加，导致模型在测试集的表现不佳，出现过拟合情况。而判断这个限度在哪里，会在后面讨论（交叉验证等方法，还未讲到）


---

## 2.1 偏差-方差权衡

    **方差**是指用一个不同的训练数据集去估计$f(.)$时，估计函数的改变量。考虑1.2图中绿色和橙色的曲线，光滑度高的绿色曲线与实际观测值很接近，该模型有较高的方差，因为改变其中任何一个数据点将会使估计函数有相当大的变化。相反，橙色的最小二乘线相对来说光滑度不高方差较小，当移动某个观测值只会引起拟合线位置的少许变化。即，对于光滑度低的模型，一般具有低方差性，也即当数据集中的某个点的位置进行偏移时，拟合函数不会有太大的改变。方差简单地说就是模型的**不稳定性**

    **偏差**是指为了选择一个简单的模型来逼近真实函数而被带入的误差。一般来说，光滑度越高的模型（高方差性），偏差越小。偏差简单地说就是模型的**不准确性**
  
    偏差的平方和误差项 的方差，具体而言:
![Markdown](http://i1.fuimg.com/611786/2fb1b16bf7b638ed.png)


    下图中，以打靶为例，左上表示低偏差-低方差（好），右上图表示低偏差-高方差（过拟合），左下图表示高偏差-低方差（欠拟合），右下图表示高偏差-高方差（差）

![Markdown](http://i2.tiimg.com/611786/8c4b99eea55467c7.png)



    我们要测试均方误差**最小**，意味着方差与偏差同时很小，但在一个实际系统中，方差和偏差往往是不能兼得的，如果要降低模型的偏差，就会一定程度上提高模型的方差，反之亦然。出现这种情况的根本原因是，我们总是希望用有限的训练样本去估计无限的真实数据。当我们更相信这些数据的真实性时而忽视对模型的先验知识，就会尽量保证模型在训练样本的准确度，这样就可以减少偏差。但是，这样学习到的模型，很可能会失去一定的泛化能力，从而造成过拟合，降低模型在真实数据上的表现，增加模型的不确定性。相反，如果我们更加相信我们对于模型的先验知识，在学习模型的过程中对模型有更多的限制，就可以降低模型的方差，提高模型的稳定性，但也会使模型的偏差增大。
    方差与偏差的权衡表现在数学模型上，实际上是模型光滑度的问题。我们可以用下图的方式来判断最优的光滑度。
![Markdown](http://i1.fuimg.com/611786/dbd2a274b2009552.png)
![Markdown](http://i1.fuimg.com/611786/03d0280c94c2b466.png)

    在图 2-11中，真实的f实际上是非线性的，因此无论给多少训练观测值，使用线性回归都不可能产生精确的估计。换句话说，在该例中线性回归会引起大的偏差。然而在图 2-10 中，真实的f十分接近线性，如果有足够多的点可供研究的话，线性回归能给出一个精确的估计。一般来说，光滑度越高的方法所产生的偏差越小。
    
    一般而言，使用光滑度更高的方法，所得的模型方差会增加，偏差会减小。这两个量比值的相对变化率会导致测试均方误差整体的增加或减小。有的时候，当提高一种方法的光滑度时，偏差减小的要比模型方差增加的快。这时，期望测试均方误差会下降。另外一些情况下，增加光滑性，对偏差的影响不明显，而模型的方差会显著地增加。当这种情况发生时，测试均方误差会增大。仔细观察图 2-9 ~图 2-11 的右图，可以看到调整光滑度会导致测试均方误差减小的情况发生。

![Markdown](http://i2.tiimg.com/611786/e1b4067804cb859b.png)

          
    图2-12
    分别表示图 2-9 -图 2-11 的三个数据集的平方偏差(蓝色曲线)、方差(橙色曲线)、不可约误差(虚线)、测试均方误差(红色曲线)。垂直的点线表示最小测试均方误差所对应的光滑度。
 
    在图2-12 中的三张图说明了式 (2.7) 在图 2-9 ~图 2-11 中每个例子中的表现，蓝色实曲线表示在不同光滑度下偏差的平方，而橙色曲线表示方差，水平虚线表示不可约误差，即Var( B)。 最后，表达测试均方误差的红色曲线是三个量的和。在三个例子中，当模型的光滑度增加时，模型的方差增加，偏差减小。然而，最优测试均方误差所对应的光滑度水平在三个数据集中是不同的。在图 2-12 的左图，偏差迅速减小，使期望测试均方误差急剧减小。另一方面，图2-12的左图，真实的接近于线性，因此当模型的光滑度增加时，偏差只发生了微小的变化，而且测试均方误差在由方差增大所引起的迅速增长前仅出现了轻微的下降。最后由图2-12的右图，由于真实的f是非线性的，随着所选模型光滑度的增加，偏差会急剧减小。随着光滑度的增长，方差也有很小的增加。这时，测试均方误差在由模型光滑度的增长所引起的小增加之前出现了大幅下降。

    上述图所显现的偏差、方差、和测试均方误差之间的关系是偏差-方差的权衡。如果一个统计学习模型被称为测试性能好，那么要求该模型有较小的方差和较小的偏差。但是在现实问题中，$f$一般是未知的，这就意味着我们无法精确地计算测试均方误差、偏差和方差。但是，偏差-方差权衡需要时刻谨记。


---

## 3.1 先验概率和后验概率

    先验概率是指根据以往经验和分析得到的概率

    后验概率是在得到结果的信息后，重新修正的概率，是在考虑了事实后的条件概率，也就是事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小。后验概率的计算要以先验概率为基础。

    假如给一些图片,这些图片中有的图上有动物的角,这些图片占了1/10(即**先验概率**),且已知在有角的条件下是犀牛的概率是0.8(类条件概率1,注意这个概率互补的概率是有角条件下不是犀牛的概率),已知在无角条件下是犀牛概率的是0.05(类条件概率2),现在拿起一张图,发现是一张犀牛的图,那么这张图上带角的概率有多大(求后验概率)

![Markdown](http://i2.tiimg.com/611786/be2d401dc25262ca.png)

由图中公式可知**后验概率**为$P(图片上由动物的角|是犀牛) = 0.8*0.1/(0.8*0.1+0.05*0.9)=0.64$


---

## 4.1 参数方法和非参数方法
    统计方法分为参数方法和非参数方法，

    **参数方法：**基于模型的方法，把估计$f$的问题简化到估计一组参数

    缺陷：选定的模型并非与真正的f在形式上是一致的，所以要拟合光滑度更高的模型，但是会带来过拟合问题

    **非参数方法：**不需要对模型$f$的形式事先作出明确的假设，追求接近数据点的估计，估计函数在去粗和光滑处理后尽可能与更多数据点接近
  
    优点：不限定函数形式，于是可能在更大的范围选择更适宜$f$形状的估计

    缺点：需要大量观测点

    下图是几种统计模型在光滑度和解释性之间的权衡

![Markdown](http://i2.tiimg.com/611786/d7bcb5487a14aa75.png)


