---
title: "introduction/linear regression"
author: "杨钰萍2017级, 张志伟2019级，2019级陈洁"
output: html_document

---


*latex语法 https://blog.csdn.net/u014630987/article/details/70156489*


# 第一章

## 1.1统计学习概述 

统计学习是一套以理解数据为目的的庞大工具集。统计学习的工具可分为两大类:有指导的学习和无指导的学习。一般而言，有指导的统计学习工具主要有两种用途:一是面向预测的统计模型的建立，二是
对一个或多个给定的输入估计某个输出。在无指导的统计学习问题中，有输入变量但不指定输出变量，建模的主旨是学习数据的关系和结构。


## 1.2记号与简单的矩阵代数
记号中，n表示不同数据点或样本观测的个数；p表示用于预测的变量个数。一般地，$x_{ij}代表第i个观测的第j个变量值,i = 1,2,{\ldots}, n;j = 1,{\ldots},p。在本书中i用于索引样本或观测（从1到n），j用于索引变量（从1到p）。\bf{X}则是(i,j)上元素为x_{ij}的n\times{p}$矩阵即

$$
        {X}=\begin{pmatrix}
        x_{11} & x_{12} & \cdots & x_{1p} \\
        x_{21} & x_{22} & \cdots & x_{2p} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{n1} & x_{n2} & \cdots & x_{np} \\
        \end{pmatrix}
$$
对矩阵不熟悉的读者，可将$\bf{x}$看成$n$行$p$列的表格。
有时会对$\bf{X}$的行进行研究，通常将行记为$x_1,x_2,{\ldots},x_n$。这里$x_i$是长度为$p$的向量，是第$i$个观测的$p$个变量测量，即
$$
        {x_i}=\begin{pmatrix}
        x_{i1} \\
        x_{i2} \\
        \vdots \\
        x_{ip} \\
        \end{pmatrix}
$$
（向量默认用列表示。）例如，对于 Wage数据集，$x_i$是一个长度为 12 的向量，包含第$i$个个体的year(年份)、age(年龄)、wage(工资)和其他变量值。有时也会研究$\bf{X}$的列，将其写作$x_1,x_2\cdots\ x_p$。它们都是长度为$n$的向量，即
$$
        {X_j}=\begin{pmatrix}
        x_{1j} \\
        x_{2j} \\
        \vdots \\
        x_{nj} \\
        \end{pmatrix}
$$
例如，Wage数据集中，$x_1$包含year（年份）的n=3000个观测值。
按这个记号，矩阵$\bf{X}$可以写成
           $$\bf{X}=(x_1\ x_2\cdots\ x_p)$$
或
$$
        {X}=\begin{pmatrix}
        x_{1}^T \\
        x_{2}^T \\
        \vdots \\
        x_{n}^T \\
        \end{pmatrix}
$$

右上角记号$T$表示矩阵或向量的转置。例如${x_i}^T=(x_{i1}\ x_{i2}\cdots\ x_{ip})$,以及
$$
        {X}^T=\begin{pmatrix}
        x_{11} & x_{21} & \cdots & x_{n1} \\
        x_{12} & x_{22} & \cdots & x_{n2} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{1p} & x_{2p} & \cdots & x_{np} \\
        \end{pmatrix}
$$
$y_i$表示待预测的变量(比如wage) 的第$i$个观测值。待预测变量全部 $n$个观测值的集合用如下向量表示:
$$
        y=\begin{pmatrix}
        y_{1} \\
        y_{2} \\
        \vdots \\
        y_{n} \\
        \end{pmatrix}
$$
观测数据集为$\lbrace(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\rbrace$，其中$x_i$都是长度为$p$的向量。（若$p=1$，则$x_i$是标量。）
在本书中，长度为$n$的向量均用小写加粗字母表示，例如：
$$
        \bf a=\begin{pmatrix}
        a_1 \\
        a_2 \\
        \vdots \\
        a_n \\
        \end{pmatrix}
$$
但长度不为$n$的向量则用小写常规字母表示，例如$a$。标量也用小写常规字母表示，例如$a$。在极少数情况下，小写常规字母的两种不同用法会导致含义不明，当出现这一问题时，书中会加以说明。矩阵用加粗大写字母表示，例如$\bf{A}$。随机变量不论维数，一律用大写常规字母表示，例如$A$。
有时需要指出对象的维数。记号$a\in{\Bbb R}$说明某个对象是标量（若长度为$n$,则用$a\in{\Bbb R}^n$表示）。$a\in{\Bbb R}^k$说明某对象为向量，其长度为$k$。$\bf{A}\in{\Bbb R}^{r\times s}$则说明矩阵的维数是$r\times s$。
我们尽可能地避免矩阵代数的使用。但在少数情况下，完全回避矩阵代数会使计算变得缓慢冗长。在这些情况下，理解矩阵乘法的概念是很重要的。$\bf{A}\in{\Bbb R}^{r\times d}$，$\bf{B}\in{\Bbb R}^{d\times s}$。则$\bf{A}$与$\bf{B}$相乘的结果记为$\bf{AB}$。矩阵$\bf{AB}$的第$(i,j)$个元素等于$\bf{A}$中的第$i$行和$\bf{B}$中的第$j$列的对应元素乘积之和。即$(\bf{AB})_{ij}=\sum_{k=1}^d a_{ik}b_{kj}$。






# 第三章 线性回归

$\quad$

## 3.1简单线性回归

$\quad$

* 一元线性回归模型：$y=\beta_0+\beta_1x+\varepsilon$

$\quad$

* 一元线性回归方程：$\hat y=\hat \beta_0+\hat \beta_1x$

$\quad$

### 3.1.1估计系数

$\quad$

用最小二乘法估计$\beta_0$和$\beta_1$时使残差平方和RSS达到最小,$RSS=\sum_{i=1}^n(y_i-\hat y_i)^2$ ,使RSS最小的参数估计值为:

$$
\begin{aligned}
\hat \beta_1 =\frac{\sum\limits_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sum\limits_{i=1}^n(x_i-\bar x)^2}\qquad
\hat \beta_0 = \bar y - \hat \beta_1\bar x
\end{aligned}
$$


### 3.1.2评价回归系数的显著性

$\quad$

1. 建立原假设和备择假设，$H_0:\beta_1 = 0 \ 和 \ H_1:\beta \not=0$


2. 计算t统计量的值，$t=\frac{\hat \beta_1-0}{SE(\beta_1)}$

3. 根据t统计量的值计算P值

* 一般地，用X 表示检验的统计量，当H0为真时，可由样本数据计算出该统计量的值C，根据检验统计量X的具体分布，可求出P值。具体地说：

* 左侧检验的P值为检验统计量X 小于样本统计值C 的概率，即：P = P{ X < C}

* 右侧检验的P值为检验统计量X 大于样本统计值C 的概率：P = P{ X > C}

* 双侧检验的P值为检验统计量X 落在样本统计值C 为端点的尾部区域内的概率的2 倍：P = 2P{ X > C} (当C位于分布曲线的右端时) 或P = 2P{ X< C}(当C位于分布曲线的左端时) 。若X 服从正态分布和t分布，其分布曲线是关于纵轴对称的，故其P 值可表示为P = P{| X| > C} 。

4. 选择显著性水平，与P值相比较，若显著性水平取0.05，当P值小于0.05则拒绝原假设，认为响应变量与预测变量有关.

* 根据$\hat \beta_1$的标准误差，可以计算$\hat \beta_1$的置信区间，$\bigg[\hat \beta_1-2\cdot SE(\hat \beta_1),\hat \beta_1+2\cdot SE(\hat \beta_1)\bigg]$，$SE(\hat \beta_1)^2 = \frac{\sigma^2}{\sum\limits_{i=1}^n(x_i-\bar x)}$


### 3.1.3评价模型的准确性

$\quad$

* 判断模型拟合程度的常用相关量有：残差标准误和R^2统计量。

* RSE越小说明模型拟合程度越好。R^2越大说明模型的拟合程度越好

$$
\begin{aligned}
&RSE = \sqrt{\frac{1}{n-2}RSS},\ RSS=\sum_{i=1}^n(y_i-\hat y_i)^2,。\\
\\
&R^2 = \frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}, \ TSS = \sum_{i=1}^n(y_i-\bar y)^2。\\
\\
&r=Cor(x,y)=\frac{\sum\limits_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sqrt{\sum\limits_{i=1}^n(x_i-\bar x)^2}\sqrt{\sum\limits_{i=1}^n(y_i-\bar y)^2}},\quad r^2=R^2
\end{aligned}
$$

```{r,echo=TRUE,warning=FALSE}
library(MASS)#Boston数据集在MASS包里面
#fix(Boston)  #看一下Boston是一张怎样的数据表
#names(Boston)#看一下Boston数据表中的各个变量名称
#用lm.fit2.1 <-lm(medv ~ lstat, data = Boston)可以得到跟以下两句同样的结果
attach(Boston)
lm.fit2.1 <- lm(medv ~ Boston$lstat) #得到线性回归方程
summary(lm.fit2.1) #查看置信区间，p值，标准误，R^2，F统计量
confint(lm.fit2.1,level = 0.95)#回归系数95%的置信区间
```
$\quad$

##  3.2多元线性回归

$\quad$

* 多元线性回归模型：$y=\beta_0+\beta_1x+ \beta_2x_2+\dots+ \beta_px_p+\varepsilon\\$
$\quad$
* 多元线性回归方程：$\hat y=\hat \beta_0+\hat \beta_1x_1+\hat \beta_1x_2\dots+\hat \beta_px_p$

$\quad$

###  3.2.1估计回归系数（与一元线性回归相似）

$\quad$

###  3.2.2评价模型是否显著

$\quad$

1. 建立原假设和备则假设，$H_0:\beta_1 =\beta_2=\dots=\beta_p= 0 \ 和 \ H_1:至少有一个\beta_j \not=0$

2. 计算F统计量的值，$F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$

3. 根据F统计量计算P值。也可以计算F统计量的值，如果线性回归假设是正确的，$E\{RSS/(n-p-1)\}=\sigma^2$，进一步若$H_0$为真，则有$E\{(TSS-RSS)/P\}=\sigma^2$。因此，当响应变量与预测变量无关，F统计量应该接近1。另一方面，$H_1$为真，预计F大于1，如果n很大，即使统计量只是略大于1，可能也仍然提供了拒绝$H_0$的证据，相反，若n很小，则需要较大的F统计量才能拒绝$H_0$。

4. 选择显著性水平，与P值相比较，若显著性水平取0.05，当P值小于0.05则拒绝原假设，认为至少有一个响应变量与预测变量有关。

5. 除此之外，若我们想要检验系数的特定子集为0，比如$H_0：\beta_{p-q+1}=\beta_{p-q+2}=\dots=\beta_p=0$,此时F统计量变为，$F=\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)}$  $RSS_0$为除q个变量之外的所有变量建立模型的残差平方。

$\quad$

### 3.2.3选定重要变量

$\quad$

* 评价模型的质量的指标主要有：赤池信息量准则AIC，贝叶斯信息准则BIC和调整R^2

* 逐步回归法（向前逐步回归，向后逐步回归，向前向后逐步回归）


```{r,echo=TRUE,warning=FALSE}
lm.fit2.2.3 <- lm(medv ~ . - age, data = Boston) #medv关于除了age以外的所有变量的回归模型
step(lm.fit2.2.3,direction="backward")
```

$\quad$

### 3.2.4模型拟合程度



$\quad$

* 和一元线性回归一样，评价模型拟合程度的指标主要有RSE，和R^2。在多元线性回归中，$RSE = \sqrt{\frac{1}{n-p-1}RSS}$,


* 因为在模型中，增加多个变量，即使事实上无关的变量，也会小幅度条R平方的值，为了排除变量数目的影响计算调整$R^2$,$$adjusted \ R^2 = 1- \frac{(RSS/（n-p-1))}{(TSS/(n-1))}$$

```{r warning=F,echo=TRUE}
lm.fit2.2.1 <- lm(medv ~ lstat + age, data = Boston) #medv关于lstat和age的回归模型
summary(lm.fit2.2.1)
```

## 3.3回归模型中的其他注意事项

$\quad$

### 3.3.1定性预测变量

$\quad$

#### 1. 二值预测变量

$\quad$

* 对gender变量创建一个新变量
$$
\begin{aligned}
&x_i=\left\{\begin{aligned}
&1\quad &女性\\
&0\quad &男性
\end{aligned}
\right.\\
\end{aligned}
$$
* 将哑变量带入回归方程，得到以下模型:
$$
\begin{aligned}
&y_i =\beta_0+\beta_1x_i+\varepsilon_i=\left\{\begin{aligned}
&\beta_0+\beta_1+\varepsilon_i\quad &女性\\
&\beta_0 + \varepsilon_i\quad &男性
\end{aligned}
\right.
\end{aligned}
$$

* 除了0/1编码，我们还可以创建如下哑变量
$$
\begin{aligned}
&x_i=\left\{\begin{aligned}
&1\quad &女性\\
&-1\quad &男性
\end{aligned}
\right.\\
\end{aligned}
$$
* 将哑变量带入回归方程，得到以下模型:
$$
\begin{aligned}
&y_i =\beta_0+\beta_1x_i+\varepsilon_i=\left\{\begin{aligned}
&\beta_0+\beta_1+\varepsilon_i\quad &女性\\
&\beta_0-\beta_1 + \varepsilon_i\quad &男性
\end{aligned}
\right.\\
\end{aligned}
$$
不同编码方式的区别只在于对系数的解释不同

$\quad$

#### 2.定性预测变量有两个以上的水平

$\quad$

* 对种族变量创建两个哑变量变量
$$
\begin{aligned}
&x_{i1}=\left\{\begin{aligned}
&1\quad &亚洲人\\
&0\quad &非亚洲人
\end{aligned}
\right.\\
&x_{i2}=\left\{\begin{aligned}
&1\quad &白种人\\
&0\quad &非白种人
\end{aligned}
\right.\\
\end{aligned}
$$


* 将两个哑变量带入回归方程，得到以下模型:
$$
\begin{aligned}
&y_i =\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\varepsilon_i=\left\{\begin{aligned}
&\beta_0+\beta_1+\varepsilon_i&亚洲人\\
&\beta_0 + \beta_2+\varepsilon_i &白种人\\
&\beta_0 + \varepsilon_i &非裔美国人
\end{aligned}
\right.
\end{aligned}
$$
```{r warning=F,echo=TRUE}
library(ISLR)
attach(Carseats)
head(ShelveLoc)
lm.fit3.3.1 <- lm(Sales~.,data = Carseats)
summary(lm.fit3.3.1)
contrasts(ShelveLoc)  #返回R虚拟变量的编码
```
创建了两个虚拟变量ShelveLocGood，ShelveLocMedium。如果货架位置好，ShelveLocGood为1。如果货架位置中等，ShelveLocMedium为1。若货架位置差，ShelveLocGood，ShelveLocMedium都为0。




$\quad$

### 3.3.2线性模型的扩展

$\quad$

#### 1. 去除可加性假设



$\quad$

$$
\begin{aligned}
y &= \beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_1x_2+\varepsilon\\
&= \beta_0+(\beta_1+\beta_3x_2)x_1+\beta_2x_2+\varepsilon\\
 &=\beta_0+\tilde\beta_1x_1+\beta_2x_2+\varepsilon\\
 \tilde \beta_1&=\beta_1+\beta_3x_2
\end{aligned}
$$
$x_1x_2$称为交互项,$\tilde \beta_1$随$x_2$变化，$x_1$对y的影响与$x_2$有关

$\quad$
```{r warning=F,echo=TRUE}
library(MASS)
lm.fit3.3.2 <- lm(medv ~ lstat + age , data = Boston)
lm.fit3.3.2.1 <- lm(medv ~ lstat + age + lstat : age, data = Boston)
summary(lm.fit3.3.2.1)
AIC(lm.fit3.3.2,lm.fit3.3.2.1)
```


#### 2. 非线性关系(多项式回归)
```{r warning=FALSE,echo=TRUE}
library(car)
lm.fit3.1.2<- lm(mpg ~ horsepower, data = Auto)
lm.fit3.1.2.1<- lm(mpg ~ horsepower + I(horsepower ^ 2), data = Auto)
AIC(lm.fit3.1.2,lm.fit3.1.2.1)
```


### 3.3.3潜在的问题

$\quad$

#### 1. 数据的非线性

$\quad$

* 线性回归模型假定预测变量和响应变量之间的关系是线性的。如果真实关系是非线性的，那么所得出的结论几乎都是不可信的。

* 可以通过残差图判断数据的线性情况，左图残差呈现U形，说明数据是非线性的。如果数据确实为非线性，可以采取对预测变量进行非线性变换，

* 右图是进行非线性变化的结果，残差无明显规律，说明经过非线性变换，提升了模型对数据的拟合程度。

![](http://i2.tiimg.com/611786/77217243e3de9ab1.png)

$\quad$


#### 2. 误差项方差非恒定


* 线性回归模型假定误差项的方差是恒定的，模型中的假设检验，标准误差和置信区间的计算都依赖于这一假设。

* 如左图所示，残差图呈漏斗形，说明误差项方差非恒定，而且误差项的方差随响应值的增加而增加，可以采用凹函数对响应值做变换。

* 右图显示了对响应值进行变换后的残差图，可以看出异方差不明显。

![](http://i2.tiimg.com/611786/f0bd6aa858ca1711.png)
```{r,echo=TRUE,warning= FALSE}
library(ISLR)
attach(Auto)
lm.fit3.1.1 <- lm(mpg ~ horsepower, data = Auto)
par(mfrow = c(2, 2)) #界面变成2*2，放四幅图
plot(lm.fit3.1.1)
```

* 左上图是，用来判断线性性。该图中显示出了一个曲线关系，说明因变量与自变量不满足线性性，可能要在回归模型中加入一个二次项；
* 右上图是“正态Q-Q图”，可以用来判断是否满足正态性假设。图中的点均落在呈45°角的直线上，说明满足正态性假设；
* 左下图是“位置尺度图”，用来判断同方差性。该图中水平线周围的点呈随机分布，说明满足同方差性；
* 右下图是“残差与杠杆图”，可以用来鉴别异常值点(可读性不佳，不展开，后面补充更好的鉴别图像)

* 加权最小二乘法,在每一$\varepsilon_i^2$中加入适当的权数$\omega_i$，以调整各项在平方和中的作用，对较大的$\varepsilon_i^2$赋予较小的权数，对较小的$\varepsilon_i^2$赋予较大的权数，使异方差变为同方差



#### 3. 误差项自相关

* 线性回归模型假定误差项是不相关的，回归系数和拟合值的标准误都基于这个假设。

* 误差项之间的相关系数越大，相邻的残差越可能有相似的值。  


![](http://i2.tiimg.com/611786/e0a41191c24621aa.png)

```{r,echo=TRUE}
states<-as.data.frame(state.x77[,c("Murder","Population","Illiteracy","Income","Frost")])
lm.fit3.1.3 <- lm(Murder ~ ., data = states)
durbinWatsonTest(lm.fit3.1.3)
```

p值大于0.05,不存在自相关,如果存在自相关可以通过差分法解决


#### 4. 离群点


离群点是指yi远离模型预测值的点。红点20是一个典型的离群点。一般认为学生化残差的绝对值大于3的观测点可能是离群点。

![](http://i2.tiimg.com/611786/6aad196858576960.png)  

```{r,echo=TRUE}
lm.fit2.2.2 <- lm(medv ~ ., data = Boston)
outlierTest(lm.fit2.2.2)
```
#### 5. 高杠杆

高杠杆点是是指观测点xi异常的点。红点41具有高杠杆值是一个典型的高杠杆点。从中间的图可以看出，红点的X1,X2都不异常，但它落在数据主体之外，也是高杠杆值。
![](http://i2.tiimg.com/611786/6a138628906870c1.png)

对于一元线性回归，杠杆统计量为:$h_i=\frac{1}{n}+\frac{(x_i-\bar x)^2}{\sum\limits_{i=1}^n(x_i-\bar x)^2}$,当观测点的杠杆值大大超过$\frac{p+1}{n}$,怀疑该点为高杠杆点

```{r,echo=TRUE,warning=FALSE}
hatplot <- function(fit)
{
    p <- length(coefficients(fit)) #p是参数个数
    n <- length(fitted(fit))#n是样本量
    plot(hatvalues(fit), main = "Index Plot of Hat Values")
    abline(h = c(2, 3) * (p + 1) / n, col = "red", lty = 2)
    identify(1:n, hatvalues(fit), names(hatvalues(fit)))#定位函数
}
hatplot(lm.fit2.2.2)
```

#### 6. 共线性


共线性是指两个或更多的预测变量高度相关。


![](http://i2.tiimg.com/611786/08c79f3988a74148.png)





![](http://i2.tiimg.com/611786/b17c772997adf085.png)

* 可以通过预测变量的相关系数矩阵，来判断变量间是否存在共线性。如果三个或更多变量之间存在共线性，则称为多重共线性。

* 评估多重共线性的方法是计算方差膨胀因子。$VIF(\hat \beta_j)=\frac{1}{1-R_{x_j|x_{-j}}^2}$ $R^2_{x_j|x_{-j}}$是$x_j$对所有预测变量回归的$R^2$。
一般当VIF的超过5或10就表示有共线性问题

* 共线性的解决方案有两种，剔除一个问题变量或者将共线变量组合成一个单一的预测变量。






```{r,echo = TRUE}
library(MASS)
lm.fit2.2.3.1 <- lm(medv ~ . - age-rad-tax, data = Boston)
vif(lm.fit2.2.3) 
vif(lm.fit2.2.3.1)
```


## 3.5线性回归与K最近邻法的比较

| | 线性回归 | K最近邻法 |
| :------:| :------:| :------:|
| 优点 | 估计的系数少，系数有简单的解释而且可以进行显著性检验| 不需要对$f(x)$的形式有明确的假设 |




$\quad$

$$
\begin{aligned}
\hat f(x_o) = \frac{1}{k}\sum_{x_i\in\omega}y_i
\end{aligned}
$$
![](http://i1.fuimg.com/611786/2492ed9f3f6b980e.png)

对一个含64个观察值的二维数据集进行KNN回归得到的$\hat f(x)$拟合图,左:k=1,右:k=9






![](http://i2.tiimg.com/611786/349f3271ebad1d04.png)

对一个含100个观测点的一维数据集进行kKNN回归$\hat f(x)$拟合图。真实关系由较粗直线表示,左k=1,右k=9





![](http://i1.fuimg.com/611786/42bafc21f9411499.png)

左:蓝色虚线是对数据的最小二乘拟合。因为f(X) 实际上是线性的(用黑线表示) ，最小二乘回归直线提供了非常好的f(X) 的估计。右:水平虚线代表的最小二乘测试集的 MSE，而绿色的实线对应 KNN 的均方误差 (MSE) , MSE是I/K (对数标度)的函数。线性回归的
MSE比KNN 回归低，这是因为真实的f(X) 是线性的。对 KNN 回归来说，最好的结果对应的K 值很大，与之相对应的 I/K 较小。


![](http://i1.fuimg.com/611786/8602800209827128.png)

第一行真实关系是近似线性关系,第二行非线性程度增加.

![](http://i2.tiimg.com/611786/02f4fae1b22c4317.png)



## 3.6练习题


使用Auto数据集进行多元线性回归。
$\quad$
(a) 作出数据集中的所有变量的散点图矩阵。
$\quad$
(b)用cor()函数计算变量之间的相关系数矩阵。需排除定性变量 name
$\quad$
(c)使用1m ()函数进行多元线性回归，用除了 name 之外的所有变量作为预测变量， mpg作为响应变量。用 summary ( )函数输出结果并分析所得结果。
$\quad$
例如:
$\quad$
i.预测变量和响应变量之间有关系吗?
$\quad$
ii.哪个预测变量与相应变量在统计上具有显著关系?
$\quad$
iii. year (车龄)变量的系数说明什么?
$\quad$
(d) plot ()函数生成线性回归拟合的诊断图。分析拟合中的问题。残差图表明有异常大的离群点吗?杠杆图识别出了有异常高杠杆作用的点吗?
$\quad$
(e) 使用符号*和符号:来进行有交互作用的线性回归模型拟合。是否存在统计显著的交互作用?
$\quad$
(f)对预测变量尝试不同的变换，如 $logX，\sqrt{X}和X^2$并分析结果。


# 第五章 重抽样

\

* 通过反复从训练集中抽取样本，然后对每一个样本重新拟合一个模型。

* 最常用的两种重抽样：交叉验证法和自助法。

* 交叉验证法可以用来估计一种指定的统计学习方法的测试误差,或者为这种方法选择合适的光滑度。

* 自助法常用于为一个参数估计或一个指定的统计方法提供关于准确度的测量。

\

## 5.1 交叉验证法

\

* 测试误差和训练误差之间存在区别,训练误差可能会低估测试误差，所以不能直接用训练误差代替测试误差。

\

### 5.1.1 验证集方法

\

1. 原理：首先随机的将观测集分为两部分，一个为训练集，另一个为测试集。然后在训练集上拟合,再用拟合好的模型在测试集上预测响应变量的值，最后计算均方误差。

![](http://i1.fuimg.com/611786/a14325a0deeb1f66.png)

![](http://i1.fuimg.com/611786/c409261557a6f639.png)

\

2. 缺陷：
* 测试错误率的验证法估计的波动很大；
* 拟合模型只用了一部分数据,意味着验证集错误率可能会高估在整个数据集上拟合模型所得到的测试错误率

```{r,echo=TRUE,warning=FALSE}
library(ISLR)
set.seed(1)
train = sample(392,196)
attach(Auto)
lm.fit1 = lm(mpg~horsepower,data = Auto,subset = train)
mean((mpg-predict(lm.fit1,Auto))[-train]^2)
lm.fit2 = lm(mpg~poly(horsepower,2),data = Auto,subset = train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3 = lm(mpg~poly(horsepower,3),data = Auto,subset = train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
set.seed(2)
print("改变训练集之后")
train = sample(392,196)
lm.fit1 = lm(mpg~horsepower,data = Auto,subset = train)
mean((mpg-predict(lm.fit1,Auto))[-train]^2)
lm.fit2 = lm(mpg~poly(horsepower,2),data = Auto,subset = train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3 = lm(mpg~poly(horsepower,3),data = Auto,subset = train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
```


### 5.1.2 留一交叉验证法

\

1. 原理：

* 首先将一个单独的观测($x_1$,$y_1$)作为验证集，剩下的观测$\{(x_2,y_2),\dots,(x_n,y_n)\}$组成训练集。

* 然后在n-1个训练观测上拟合模型，再用拟合好的模型根据$x_1$预测$\hat y_1$,得到$MES_ \ = \ (y_1-\hat y_1)^2$,重复这个步骤，得到$MES_2, \ \dots, \ MSE_n$

* 最后得到用留一交叉验证法的CV估计：$CV_{(n)} = \frac {1}{n}\sum\limits_{i=1}^nMSE_i$

![](http://i1.fuimg.com/611786/0f315f50339579b9.png)

![](http://i1.fuimg.com/611786/26b39b3d0f32d583.png)

2. 优点：
* 拟合模型用了n-1个数据,观测数几乎于整个数据集中的数据量相等,因此留一交叉验证法比验证集法更不容易高估测试错误率。
* 留一交叉验证法的CV估计不存在波动；

\

3. 缺点：计算量大。(特例:用最小二乘法来拟合线性或者多项式回归模型时,LOOCV所花费的时间将神奇地被缩减至与只拟合一个模型相同,此时测试均方误差为:$CV_n = \frac {1}{n}\sum\limits_{i=1}^n(\frac{y_i-\hat y_i}{1-h_i})^2$)

```{r,echo=TRUE,warning=FALSE}
library(boot)
glm.fit = glm(mpg~horsepower,data = Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta
cv.error = rep(0,5)
for (i in 1:5) {
  glm.fit = glm(mpg~poly(horsepower,i),data = Auto)
  cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
}
cv.error
```


### 5.1.3 k折交叉验证法

1 原理：

* 首先将观测集随机分为K个大小基本一致的折。

* 然后将一个折作为验证集，在剩下的k-1折组成训练集上拟合模型，计算$MSE_1$

* 重复这个步骤K次,得到$MES_2, \ \dots, \ MSE_n$

* 最后得到用K折交叉验证法的CV：$CV_{(K)} = \frac {1}{k}\sum\limits_{i=1}^kMSE_i$

![](http://i1.fuimg.com/611786/302ce8ef4e751458.png)

2. 优点
* 相比留一交叉验证法，计算量小
* 比用验证集方法所得到的测试误差波动要小

\

3. 使用交叉验证法时,其目的可能是向评价某一模型在独立数据运用上的表现，在这种情况下，感兴趣的是测试误差的估计精度。而在其他一些情况，可能是对测试误差曲线的最小值点的位置感兴趣，如对某一统计方法在不同光滑度水平使用交叉验证。

![](http://i1.fuimg.com/611786/034f190534ab4c9a.png)

```{r,echo=TRUE,warning=FALSE}
set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10) {
  glm.fit = glm(mpg~poly(horsepower,i),data = Auto)
  cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]#K要大写,delta向量中两个指是不一样的
}
#cv.error.10
```


### 5.1.4 K折交叉验证的偏差-方差权衡

* 当k < n时，K折CV不仅比LOOCV的计算量要小，而且对测试误差的估计更精确，这里涉及方差-偏差权衡问题

* 在LOOCV中，训练集为n-1;而在K折CV中，训练集为(k-1)n/,比n-1要小。所以LOOCV方法估计的测试误差的偏差要小于K折CV估计的。

* 在LOOCV中，测试误差是平均n个拟合模型的结果，n个拟合模型所使用的训练集几乎一样。而在K折CV中，K个拟合模型所使用的训练集重叠部分相对较小，所以测试误差是平均K个相关性较小的拟合模型的结果。因为高度相关的量的均值要比相关性相对较小的量的均值波动更大，所以用LOOCV法得到的测试误差的方差要比K折CV的大

* 总之，在选择K时要考虑到方差-偏差权衡问题。一般K为5或10，此时测试误差的估计不会有过大的偏差或方差。


### 5.1.5 交叉验证法在分类问题中的应用

* 在响应变量为定性变量的背景下,LOOCV法的测试误差为：$CV_{(n)} \ = \ \frac{1}{n}\sum\limits_{i=1}^nErr_i$, $Err_i=I(y_i\not = \ \hat y_i)$


## 5.2 自助法

* 可以衡量一个指定的估计量或统计学习方法提供关于准确度的测量

假设希望用一笔固定数额的钱对于两个分别收益为X和Y的金融资产进行投资，其中X和Y是随机变量。打算把所有钱的百分比是$\alpha$的部分投资到X，把剩下的1-$\alpha$部分投资到Y。由于这两笔投资的收益存在波动性，所以希望选择一个$\alpha$ ，使得投资的总风险或者说方辈在最小。
换句话说，希望使得 $Var(\alpha + (1-\alpha)Y)$ 最小。使得风险最小的$\alpha$值为



$$
\begin{aligned}
\alpha \ = \ \frac{\sigma_Y^2-\sigma_{XY}}{\sigma_X^2+\sigma_Y^2-2\sigma_{XY}}
\end{aligned}
$$



其中$\sigma_X^2=Var(X),\sigma_Y^2=Var(Y),以及\sigma_{XY}=Cov(X,Y)$。

在现实中，$\sigma_X^2$，$\sigma_Y^2$和$\sigma_{XY}$的值是不知道的。于是可以用包含过去X和Y值的数据集，来计
算这些量的估计$\hat \sigma_X^2,\hat \sigma_Y^2和\hat \sigma_{XY}$。那么就可以估计使得投资的方差最小的$\alpha$值


$$
\begin{aligned}
\hat \alpha \ = \ \frac{\hat \sigma_Y^2-\hat \sigma_{XY}}{\hat \sigma_X^2+\hat \sigma_Y^2-2\hat \sigma_{XY}}
\end{aligned}
$$



![](http://i1.fuimg.com/611786/558e4b495d264a50.png)



自助法估计的标准误差：$SE_\beta(\hat \alpha) \ = \ \sqrt{\frac{1}{B-1}\sum\limits_{i=1}^B(\hat \alpha^{*r}-\frac{1}{B}\sum \limits_{i=1}^B\hat \alpha^{*r^{'}})}$
![](http://i1.fuimg.com/611786/5393437ea6ef1b9e.png)
 
```{r,echo=TRUE,warning=FALSE}
#估计一个感兴趣的统计量的精度
alpha.fn = function(data,index){
  X = data$X[index]
  Y = data$Y[index]
  return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))
}
alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace = T))
boot(Portfolio,alpha.fn,R=1000)
#估计线性回归模型的精度
boot.fn =  function(data,index) return(coef(lm(mpg~horsepower,data=data,subset = index)))
boot.fn(Auto,1:392)
boot.fn(Auto,sample(392,392,replace = T))
boot.fn(Auto,sample(392,392,replace = T))
boot(Auto,boot.fn,1000)
summary(lm(mpg~horsepower,data = Auto))$coef
boot.fn =  function(data,index) return(coef(lm(mpg~horsepower+I(horsepower^2),data=data,subset = index)))
boot(Auto,boot.fn,1000)
summary(lm(mpg~horsepower+I(horsepower^2),data = Auto))$coef
```

## 5.3练习题


复习k折交叉验证法。
$\quad$
(a) 阐述k折交叉验证法的步骤。
$\quad$
(b) k折交叉验证法相对于以下方法的优势和劣势在哪:
$\quad$
(1).验证集方法?
(2).LOOCV方法?