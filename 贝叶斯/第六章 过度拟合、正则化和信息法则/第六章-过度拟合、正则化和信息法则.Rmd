---
title: "第六章 过度拟合、正则化和信息法则"
author: "高全金"
date: "2021/11/15"
geometry: "left=2cm,right=2cm,top=2cm,bottom=2cm"
output: 
      html_document : 
        toc: yes
        toc_float: yes
        toc.depth: 6
        toc_depth: 6
        theme: paper
        df_print: tibble
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	comment = " ",
	prompt = TRUE,
	results = "hold"
)
```

哥白尼是“日心说”的拥护者，因为他坚信，日心说比地心说更加和谐。但实际上，哥白尼建立的日心说模型也十分复杂，它既不比地心说更加和谐，也不比地心说的预测更加准确。所以，从这一点说，哥白尼坚信的“日心说”是缺乏说服力的。不过哥白尼的日心说确实比地心说模型需要的条件要少，在这个意义上来说，似乎比地心说要简单一些。
在预测结果相似的情况下， 我们似乎更乐意选择简单的模型。一个最有名的理论就是“奥克母剃须刀”Ockham razor：**优先选择假设条件较少的模型**。如果日心说和地心说预测效果一致，那么日心说的假设条件较少，确实是优先选择的模型。

同样地，在我们实际应用中，我们常常也会面临这两种选择，**预测效果的准确性和模型的简单程度**。

对于这种选择，最好的一个比喻是尤利西斯，他是荷马奥德赛中的大英雄，在他一次航行中，必须通过一条狭窄的海峡，海峡一边陡峭山壁，山壁上附着着很多野兽，通过海峡时，太靠近山壁就会丧命于野兽口中。但是如果离山壁太远，深海中的野兽也会将其拉下船，葬身大海。所以尤利西斯既不能太远离山壁，也不能太靠近深海。这正如我们建模时两种很危险的错误一样：

（1）**过度拟合** ：模型从现有的数据中学到了过多的信息，导致了其预测能力变差；

（2）**欠拟合**：模型从现有的数据中学到了太少的信息，预测能力也不好。

我们在建模的时候就是要选择一个平衡，既不能被山壁上的野兽吃掉，也不能被大海中的怪物拉入大海。那么怎么做呢？首先是正则化先验概率；其次是使用信息准则。

”寻星之旅“：关于如何寻找一个合适的模型，很多人或许很关注模型中的参数是否有统计学意义，如果一个参数后面跟上了星号”*“，那么会非常兴奋，当然，星号越多越好，越多意味着参数越可信。如同，星际航行中寻找星星那样，越多越令人兴奋。

但是，这样得到的结果并非可靠，**p值的作用并不能判断模型好坏，并不能告你你你的模型是过度拟合还是欠拟合**。一个能够显著提高模型预测的参数并不一定被检验出统计学意义，同样，一个有统计学意义的参数也并不一定能够提高模型预测能力。

# 6.1 参数问题

之前的内容，我们提到了在模型中添加变量能够矫正变量之间隐藏的关系，同样不合适的添加变量可能存在共线性问题。那么，假设在不存在共线性问题的情况下，是不是模型中添加的变量越多越好呢？

显然并不是这样。添加变量越多，一方面会使模型更加复杂，另一方面，添加的变量不管合不合适，都会在一定程度上提高模型的”拟合“程度。比如说我们常用的描述模型拟合程度的R2，**在模型中添加变量总会在一定程度上增大R2的值**。

此外，更复杂的模型虽然拟合效果更好，但是往往预测能力更差。这是因为更多的参数导致模型变得更加灵活，对数据更加敏感，预测效果更不稳定。

## 6.1.1 更多的参数总是提高拟合度


当模型在样本数据中”学到“了过多的信息时，就会出现过度拟合。下面就是一个模型过度拟合的例子。7种灵长类动物中，脑容积和体重的关系。构造数据以及建立线性模型如下：

```{r}
sppnames <- c("afarensis", "africanus", "habilis", "boisei",
              "rudolfensis", "ergaster", "sapiens")
brainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)
masskg <- c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)
d <- data.frame(species=sppnames, brain=brainvolcc, 
            mass=masskg)
m6_1 <- lm(brain ~ mass, data = d)
m6_2 <- lm(brain ~ mass + I(mass^2), data = d)
m6_3 <- lm(brain ~ mass + I(mass^2) +
             I(mass^3), data = d)
m6_4 <- lm(brain ~ mass + I(mass^2) + 
             I(mass^3) + I(mass^4), data = d)
m6_5 <- lm(brain ~ mass + I(mass^2) + 
             I(mass^3) + I(mass^4) + I(mass^5), data = d)
m6_6 <- lm(brain ~ mass + I(mass^2) + 
             I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6), data = d)
```

[![beSOje.png](https://s4.ax1x.com/2022/02/26/beSOje.png)](https://imgtu.com/i/beSOje)

从模型的拟合程度来看，模型中的参数越多，拟合效果越好，其中R2在第6个模型中达到了1，也就是模型能够对数据完美拟合！但是同时也存在一些问题，比如在第6个模型中，脑容积居然出现了小于0的情况，体重在58-60kg的灵长类动物，大脑容积为负？？显然这时不可能的。

但是为什么第6个模型的拟合程度达到了1呢？这时因为数据中有7个观测，而我们的模型中的参数已经达到了7个：

$$
\mu_i = \alpha + \beta_1 m_i +\beta_2 m_i^2 +\beta_3 m_i^3 +\beta_4 m_i^4 +\beta_5 m_i^5 +\beta_6 m_i^6 
$$

也就是，每一个数据都有一个参数来描述。所以对**一个模型，只有你纳入足够多的参数，就可以做到完全拟合**。但是这样的模型对我们的预测没有丝毫帮助。

**模型拟合的本质是一种信息的压缩**。我们把大量观测数据使用几个参数来表示，但是这种压缩不是无损压缩，会造成一定程度上的信息损失。这和文件的压缩是一个道理的。像上面第6种模型，我们的原始数据有7个，而我们的压缩参数也有7个，也就是说我们并没有达到压缩数据的目的，我们只是把原数据换了一种表现形式。所以这种模型也没有任何意义。

## 6.1.2 参数太少也成问题


如果一个模型中使用的参数过少，就会造成模型的欠拟合。欠拟合不仅造成模型对现有数据的拟合效果不好，而且模型的预测效果同样欠佳。模型没能够通过现有观测数据“学到”足够的信息。比如在上面灵长类大脑体积的例子中，如果加入我们的模型中除了一个截距外，再没有其他参数。如下：

$$
v_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha
$$

```{r}
library(rethinking)
m6.7 <-lm(brain ~ 1, data = d)
coef(m6.7) #回归得到的系数
R <- 1 - var(resid(m6.7))/var(d$brain) # R^2
R
plot(brain ~ mass, data = d, type = 'n')
abline(m6.7)
title(paste('R^2 = ',round(R)))
```

得到的模型截距就是脑容量的平均值，和体重完全没有关系。该模型既不能描述样本，也不能进行预测。

在观测样本中移除任何一个点，对该模型的影响都不大，也就是模型对观测十分不敏感。这和上面的过度拟合的模型是正好相反的。


[![bBwiLQ.png](https://s1.ax1x.com/2022/03/06/bBwiLQ.png)](https://imgtu.com/i/bBwiLQ)

# 6.2 信息理论和模型表现

既然参数过多和过少都会导致模型出问题，那么该怎样选择参数，既能避免欠拟合也能避免过度拟合？这时，我们就需要一个对模型评价的标准。

这儿就涉及到一个概念，就是袋外样本偏差。

首先，我们需要建立一个联合概率来判断模型的准确性；其次，我们需要测量实际预测和完美预测之间的距离；第三，还需要建立一个偏差量来估计现有模型到完美预测的相对距离。下面我们就依次对这几方面进行解释

## 6.2.1 开除天气预报员

准确性的定义依赖于所选取的标靶。在选取靶标的时候，我们常常需要考虑下面两个方面：

1）损失-收益分析。如果我们预测错了，我们的付出的代价是多少？如果我们预测对了，我们的收益又会是多少？

2）在具体语境中的准确性。有些预测本身就比较容易一些。所以，即便我们忽略了代价-收益，我们同样需要一种方式来判断“准确性”到底能够在多大程度上提升模型的预测能力。

假如现在有一个城市的天气预测员，他预测了未来十天该城市的天气情况，是雨天还是晴天。

下面是他预测的下雨概率：

[![be2bvD.png](https://s4.ax1x.com/2022/02/27/be2bvD.png)](https://imgtu.com/i/be2bvD)

现在又来了一个预测员，他声称自己是最好的预测员，他预测未来10天全部为晴天，即所有下雨概率为0。

[![be2vVA.png](https://s4.ax1x.com/2022/02/27/be2vVA.png)](https://imgtu.com/i/be2vVA)

**那么根据预测的准确性**，这个新来的预测员确实是最好的预测员。因为根据预测的准确性， 老预测员：3 * 1 + 0.4 * 7 = 5.8天；新预测员：3 * 0 + 7 * 1 = 7天

所以这个新来的预测员赢了！

但是这**仅仅是从预测准确性**上来说的，但是如果我们此时考虑付出-收益的关系，或许我们能够得到完全不同的结论。比如，你非常厌恶雨天，但是也不喜欢平时带伞。如果被雨淋是你的对快乐的影响是-5，带伞的影响是-1。如果你带伞的概率和预测有雨的概率相同，那么要使你的快乐达到最大化，你应该怎样选择一个靠谱的天气预测员呢？

[![beRuGV.png](https://s4.ax1x.com/2022/02/27/beRuGV.png)](https://imgtu.com/i/beRuGV)

也就是说，你按照老预测员的预测，对你快乐的影响是3 * (-1) + 7 * (-0.6) = -7.2；而按照新预测员的预测，对你的快乐影响为3 * (-5) + 7 * 0 = -15。所以，在这种情况下，按照新预测员的预测，你的心情指数大大降低，付出的代价更高。

此外，我们还可以从联合概率来考虑。老预测员的预测的联合概率是1^3 * 0.4^7 = 0.005；而新预测员的联合概率是0^3 * 1^7 = 0。可以看到虽然新预测员的预测准确性的平均概率较高，但是其预测准确性的联合概率很低。

## 6.2.2 信息和不确定性

早在上世纪40年代，就有人研究了关于模型准确性的衡量方式，最初是用在通信领域，比如电报。现在其被称为“信息理论”，广泛应用于基础学科和应用学科中，同时和贝叶斯理论有很多联系。

信息理论的一个基本问题是，当我们知道事情结果时，该事情的不确定性降低了多少？比如，预测未来某天天气时，预测是提前的，天气是不确定的。当该天到来是，天气变成确定的了。这种不确定性的降低就能够衡量我们学到的多少，也就是我们通过观测结果得到了多少信息。如果我们能够给出“不确定性”的精确定义，那么我们就能够衡量预测的难易程度。不确定性的降低就是“信息”的定义。


**信息：由于获知结果而导致的不确定的降低**

不确定性应该具有下面三种属性：

1）不确定性首先应该是**连续的**。如果不是连续的，那么一点很小的概率变化，都会导致不确定性巨大的改变。

2）不确定性大小应该**随着可能发生事件数量的增加而增加**。比如，有两个城市需要预测天气，在第一个城市一年内有一半是雨天，另一半是晴天；在第二个城市，有1/3天是雨天，1/3是晴天，1/3天是冰雹。那么第二个城市天气的不确定性要比第一个城市大，因为该城市有3个可能发生的事件。

3）不确定性应该具有**可加性**。比如我们首先测量了天气（雨天、晴天）和温度（冷、热），那么四种情况组合的不确定性（雨天-冷；雨天-热；晴天-冷；晴天-热）应该是每个情况单独不确定之和。

那么满足上面3种条件的不确定性怎么测量？

**信息熵**（它是一个函数）

虽然它看起来很神秘，但它的定义公式很简单。如果有n种不同的结果，每一个结果i的概率是pi。那么该事件的不确定的定义为：

$$
H(p) = -E \log(p_i) = \ \sum_{i=1}^{n} p_i \log(p_i)
$$
即，概率分布所含有的不确定性是事件发生概率的对数平均。

比如，某天雨天的概率p1=0.3，晴天的概率p2=0.7，那么该天天气不确定性，或者说是信息熵为：

$$
H(p) = - (~p_1 ~log(~p_1~) + p_2~ log(~p_2~)~) \approx 0.61
$$

假如我们生活在迪拜，下雨的可能性很小，p1=0.01，晴天的可能性很大，p2=0.99，那么此时的信息熵为0.06，也就是不确定性变得很低。同样地，如果我们的预测在冬天，雨天p1=0.15，雪天p2=0.15，晴天p3=0.7，那么此时信息熵变为0.82，由于不确定性事件数量的增加，不确定性也增加了。

```{r}
func <- function(p)
  print(-sum(p * log(p)))
func(c(0.3, 0.7))
func(c(0.01, 0.99))
func(c(0.7, 0.15, 0.15))
```

## 6.2.3 从熵到不确定性

信息熵为我们提供了衡量不确定性的方式。H提供了这样一种方法。通过这种方式我们能够很精准地描述达到模型目标有多难。关键在于分布的**散度**：

**散度**：用一个概率分布去描述另一个分布所导致的不确定性

这通常被称为Kullback-Leibler散度，或者简称K-L散度。

如一个事件的两种结果发生的实际可能性为p1=0.3,p2=0.7，而我们认为其发生可能性为q1=0.25，q2=0.75，那么因为我们用q={q1,q2}来描述p={p1,p2}所导致的不确定性的增加是多少呢？根据上面信息熵的理论，我们可以做如下计算：

$$
D_\text{KL}(p, q) = \sum_i p_i (\log(p_i) - \log(q_i)) = \sum_i p_i \log(\frac{p_i}{q_i})
$$
也就是实际值(p)和估计值(q)对数差异的平均值。当p=q时，K-L差异值变为0，即我们用一个概率分布区描述另一个分布没有造成不确定性的增加。

随着估计值q和实际值p之间的增大，K-L差异也在增大。比如，如果从q={0.01，0.99}变为q={0.99,0.01}，那么K-L差异会怎么变化呢？下图给出了答案。
```{r}
q_seq <- seq(0.01, 0.99, 0.01)
q <- data.frame(q_seq)
shang <- function(x){
  shang <- 0.3 * log(0.3 / x) + 0.7 * log(0.7/ (1-x))
  return(shang)
  
}
y <- apply(q,1,shang)
plot(q_seq, y, type = 'l', col = 'blue')
abline(v = q_seq[which.min(y)], lty = 2) # 添加虚线
text(0.33, 1.5,'p = q') #添加文本
```

我们可以通过计算K-L散度来衡量我们估计值和实际值之间的差异，从而选择一个最好的估计。

**交叉熵和散度**

当我们用概率分布q去预测服从另外一个概率分布p的事件时，就产生了一种叫做交叉熵的东西：$H(p,q) = - \sum_i p_i log(q_i)$。

事件服从概率分布p，但我们用概率分布q来预测事件，因此熵增加了。增加的幅度取决于p和q之间的差别。散度是指由于使用q引入的额外熵。因此就是真实事件分布对应的熵$H(p)$与用q来逼近p得到的熵$H(p, q)$之间的差别：

$$
D_{KL}(p,q) =  H(p,q) - H(p) = -\sum_i p_ilog(q_i) - (- \sum_i p_i log(p_i))\\
= -\sum_i p_i(log(q_i) - log(p_i))
$$
所以事实上散度是用来衡量分布q和目标分布p之间的差距，测量单位是熵。

注意：一般来说，H(p,q)不等于H(q,p)。

**散度却决于方向**

假设我们乘坐一艘火箭飞往火星，但我们无法控制在火星的降落地点。现在我们试着预测最后会降落在水面上还是地面上。

通过地球上水面和陆地面积的分布去逼近火星表面的分布p。

对地球，水面和地面的比例分别为q = {0.7，0.3}，假设火星上水面和地面的比例分为为p = {0.01, 0.99}

则用地球分布q来估计火星分布p对应的散度为：

$$
D_{KL}(p,q) =  -\sum_i p_ilog(q_i) - (- \sum_i p_i log(p_i))= \sum_i p_i(log(\frac{p_i}{q_i}) )
$$

结果为：
```{r}
q = c(0.7, 0.3)
p = c(0.01, 0.99)
sum(p * log(p / q))
```

用火星分布p估计地球分布q对应的散度为：

$$
D_{KL}(q,p) =  -\sum_i q_ilog(p_i) - (- \sum_i q_i log(q_i))= \sum_i q_i(log(\frac{q_i}{p_i}) )
$$


```{r}
q = c(0.7, 0.3)
p = c(0.01, 0.99)
sum(q * log(q / p))
```


这说明，用火星分布逼近地球分布确实比相反方向的逼近具有更高的不确定性。原因在于用火星逼近地球时，由于火星表面水域覆盖率比率较低，当我们发现地球表面大部分是水域时会非常吃惊。相反，地球表面水域和陆地都占有一定的比例，所以用地球逼近火星时，虽然我们会期待更多的水域，但是遇到水面和陆地都在预料之中。

## 6.2.4 从散度到偏差


在我们计算K-L散度的时候，我们假设知道真实的概率分布p，但实际上，我们常常无法得到这个p值。不过在我们比较两个不同模型（比如q和r）的时候，真实的p值可以被相减抵消掉。所以我们依旧可以得到q和r的距离，以及哪一个离真实值比较近。如同两个人射箭，虽然我们不知道它们射箭距离靶点的具体距离，但是我们可以判断他们两个的箭哪一个离靶点更近。

通过这种相对距离，我们可以得到模型的偏差：

$$
D(q) = -2\sum_ilog(q_i)
$$

其中，i是第i个观测数据，qi是第i个观测发生的可能性，-2没有明确的意义，只是习惯上的原因。发生的可能性越大，偏离就越小。偏离并没有像K-L差异那样做平均处理，所以它的大小和观测数量有直接的关系。

我们可以计算任何模型的偏差，只需要代入MAP参数估计计算每行观测的对数似然，得到的概率就是对应的q值，然后将这些概率取对数后相加，再乘以-2。

下面是一个简单的例子，依然使用之前的古人类数据。

```{r}
#拟合模型前对体重进行标准化
d$mass.s <- (d$mass - mean(d$mass))/sd(d$mass)
m6.8 <- map(
  alist(
    brain ~ dnorm(mu, sigma),
    mu <- a + b * mass.s
  ),
  data = d,
  start = list(a = mean(d$brain), b = 0, sigma = sd(d$brain)),
  method = 'Nelder-Mead'
)
# 提取MAP估计
theta <- coef(m6.8)
print(theta)

# 计算偏差
dev <- (-2) * sum(dnorm(
  d$brain,
  mean = theta[1] + theta[2] * d$mass.s,
  sd = theta[3],
  log = TRUE
))
dev
```

同时，可以通过使用logLik函数，该函数能够解决其中计算困难的部分：计算对数概率之和。

```{r}
m6.1 <- lm(brain ~ mass, d)

# 走个计算偏差的捷径
(-2) * logLik(m6.1)
```

## 6.2.5 从偏差到袋外样本

样本的偏差大小和R2存在相似的问题，其值受到模型参数数量的影响。而且它描述的是样本内的拟合情况，不能表示预测能力。下面我们把模型扩展到样本外。

如果我们有一定量数据，并根据该数据建立模型，那么这些数据就称之为**训练样本**。模型的参数是根据这些训练样本得到的。我们用得到的模型去预测新的样本，这些样称为为**测试样本**。

我们可以对训练样本和试验样本分别计算偏差D。假设我们有5个模型，其中的参数数量分别为1-5，那么对于训练样本和试验样本的偏差D大小分别是多少呢？使用两个参数的模型来进行1万次模拟，我们得到了下面的图：
[![bl5ynA.png](https://s4.ax1x.com/2022/03/01/bl5ynA.png)](https://imgtu.com/i/bl5ynA)

可以看到，对于训练样本偏离，随着参数增加，一直在降低。而对于测试样本，在样本量比较小的时候（N=20），偏离在3个参数的时候达到最小，随后便开始增大。

同时，还要提醒一下，偏差是用来描述预测准确性的，而不是模型的”真实性“。

虽然训练样本的偏差总是随着预测变量个数的增加而增加，但袋外样本则不一定，结果取决于真实数据发生机制以及用来估计参数的样本量。

以上这些事实为我们理解**收缩先验和信息法则**奠定了基础。

# 6.3 正则化

**导致模型过度拟合的原因是模型在训练样本中”学“到了太多**。如果我们给模型的先验概率分布是很扁平的或者是均匀分布的，那就意味着该先验信息没有提供太多有用的信息，这时先验信息给了**模型太多的自由**，就到导致模型得到一个尽可能包含所有信息的结果，造成过度拟合。

为了避免这种情况，我们就需要给先验信息足够的信息量，避免先验概率分布过于扁平，以此来限制模型的学习，也就是**先验概率正则化**。一个合适的正则化的先验概率既能保证模型充分描述数据本身特征，又不至于过度拟合。当然，如果先验信息正则化过度，就会严重阻碍模型在训练样本中的学习能力，造成模型欠拟合。

如我们建立了以下的线性模型：

$$
y_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta x_i \\
\alpha \sim \text{Normal}(0, 100) \\
\beta \sim \text{Normal}(0, 1) \\
\sigma \sim \text{Uniform}(0, 10)
$$
假设其中观测数据x进行了标准化。其中$\beta$ ~ $Normal(0,1)$的概率分布是比较窄的，几乎就在【-2，2】之间。这就是一种正则化的先验概率，它为模型提供了足够的先验信息，告诉模型斜率应该极有可能在【-2，2】之间。

```{r}
x <- seq(-3,3,length.out = 100)
plot(x,dnorm(x), lty =3, type = 'l', 
     xlab = '参数值',
     ylab = '密度',
     ylim = c(-0.1,2.2))
lines(x,dnorm(x, 0, 0.5), lty = 1)
lines(x,dnorm(x, 0 , 0.2), lty = 1, lwd = 2)
```

【虚线Normal(0,1); 细实线Normal(0,0.5)；粗实线Normal(0,0.2)】

当然，如果我们给模型提供的先验概率是上图中的粗实线，就相当于给模型提供了过多的先验信息，就可能会导致模型欠拟合。

同样地，我们用上面的3种先验概率在进行10000次模拟，来看不同参数情况下，模型预测偏差情况。

[![bl5Rtf.png](https://s4.ax1x.com/2022/03/01/bl5Rtf.png)](https://imgtu.com/i/bl5Rtf)

在样本量N为20的时候，随着先验概率变窄（虚线到粗实线），模型在训练样本中的偏差逐渐增大。这主要是由于先验概率正则化越来越严重，造成模型欠拟合。但是此时，在试验样本中的偏差却是最小的，也就是过度的正则化虽然没有带了较好的模型拟合效果，但是却得到了较好的预测效果。

在样本量N为100的时候，可以看看到，不管是训练样本还是试验样本，3种不同的先验概率下的偏差几乎没有区别，这主要是我们有了足够量的观测数据，这些数据足够使模型抵消掉不同先验概率给结果带来的影响。

先验概率正则化过度造成模型欠拟合，而正则化不足则可能造成模拟拟合过度。**所以合理的正则化非常必要**。如果你有足够的数据量，那么你可以把数据分成训练集和预测集，使用不同的先验概率分布，比较它们的预测偏差情况，选择预测偏差最小的模型。这也就是我们常用的交叉验证，以此可以避免过度拟合和欠拟合。


# 6.4 信息法则

我们现在拿出上面模拟实验的一组数据来比较训练样本偏差和试验样本偏差的差异大小。下图中的竖实线表示它们的差异。


[![blotJK.png](https://s4.ax1x.com/2022/03/01/blotJK.png)](https://imgtu.com/i/blotJK)

【蓝点表示不同参数数量下训练样本偏差，空心点表示试验样本偏差。竖实线表示偏差差异。虚线表示训练样本偏差加上2倍偏差差异的值】

细心的话可以注意到，竖直线的长度大概是横坐标参数数量的2倍。也就是试验样本的偏差（空心点）约等于训练验本偏差（蓝色实心点）加上2倍参数数量。这就是信息准则的一种现象，最常见的信息准则是AIC，其可以表达为：

$$
AIC = D_{train}+2p
$$

其中，p是模型中自由参数的数量。这也就意味着，我们不在需要试验样本来估计偏离了。这给我们实际应用带来了很大的方便。

AIC是最古老的一种信息准则，所以应用AIC有一定的条件限制：


1）先验概率分布是水平的或者先验被样本似然值淹没

2）后验逼近多元高斯

3）样本量N远高于参数个数k。

不过很多时候，我们给出的先验概率分布并不是平的。所以AIC的应用场景有限。但我们还有其他的信息准则，比如$DIC$和$WAIC$

Deviance Information Criterion($DIC$)不再要求先验概率分布是平的，应用范围放宽了。

Widely Applicable Information Criterion($WAIC$)使用范围更加广泛，甚至到后验概率分布也没有特殊要求。

这儿我们详细介绍DIC和WAIC。

## 6.4.1 DIC

像AIC那样，$DIC$应用也十分广泛，很多软件包都提供$DIC$的计算。它**不要求先验概率扁平分布**。不过如果后验概率不是正态分布，可能会造成严重的结果偏倚。

$DIC$是从训练样本偏差的后验概率分布中计算得到的。什么是偏差的后验概率分布？我们知道模型的参数有后验概率分布，由于偏差是通过模型参数计算得到的，所以偏差也应该有一个后验概率分布。只不过通常我们认为的偏差只是一个该分布的MAP（最大后验概率）值。在这儿我们以D来表示偏差的后验概率分布。我们对从参数的后验概率分布中抽样，每抽一次，计算一次偏差值。假设我们进行了10000次抽样，我们就得到了10000个偏差值。

我们以$\overline{D}$表示D的平均值，$\hat{D}$表示参数后验分布均值对应的偏差，也就是现将抽取的参数值取平均，然后将这些均值代入得到相应的偏差
$\hat{D}$

这时我们可以计算$DIC$为：

$$
DIC = \overline{D} + (\overline{D} - \hat{D}) = \overline{D} + p_D
$$

其中$\overline{D} - \hat{D} =p_D$和计算AIC中参数数量类似，表示模型拟合训练样本时的灵活度。灵活度过高，会导致过度拟合。$p_D$有时候也称之为惩罚项，表示训练样本偏差和测试样本偏离的偏差大小。对于先验概率分布扁平的模型，DIC就变成了AIC，因为此时距离大小就是模型的参数数量。

## 6.2.4 WAIC

由于$WAIC$不要求后验概率正态分布，所以通常比$DIC$要更加准确。$WAIC$的计算不像$DIC$那样对参数的后验概率分布整体抽样，而是点对点，观测对观测地单独计算。

对于训练样本中的第i个观测，我们可以从参数后验概率中抽取一组参数组合，计算$y_i$，多次抽样后得到$y_i$的平均值$Pr(y_i)$，然后我们把训练样本中所有观测点的$Pr(y_i)$做对数加和，表示如下：

$$
lppd = \sum_{i=1}^N logPr(y_i)
$$


这就构成了计算$WAIC$的第一部分————对数逐点预测密度。

lppd进一步化简：

$$
lppd = \sum_{i=1}^N logPr(y_i)=\sum_i^nlog\big(\int(p(y_i|\theta)p_{post}\theta)d\theta\big)\\
其中\int(p(y_i|\theta)p_{post}\theta)d\theta
\approx\frac{1}{N}\sum_{j=1}^N p(y_i|\theta^{(j)})\\
因此lppd = \sum_{i=1}^N \big(log(\sum_{j=1}^Np(y_i|\theta^{(j)}) - log(N)\big)
$$

计算$WAIC$的第二部分是参数有效数量$p_{WAIC}$，上面我们用$Pr(y_i)$表示了每个点$y_i$的平均值，这儿我们用$V(y_i)$表示每个点$y_i$的方差大小。

$p_{WAIC}就是所有点的$V(y_i)$的加和，表示如下：

$$
p_{WAIC} = \sum_{i=1}^N V(y_i)
$$

WAIC的完整定义为：

$$
WAIC = -2(lppd-p_{WAIC})
$$

通过$WAIC$的值可以估计试验样本的偏离大小。

$WAIC$的计算要求各个观测点之间相互独立，对于有些数据，比如时间序列数据，可能不能满足相互独立的条件，所以使用的时候应当注意。

下面通过具体的例子来看$WAIC$是怎么计算得到的，考虑下面通过map拟合的简单回归。

```{r}
library(rethinking)
library(ggpubr)
data(cars)
m <- map(
  alist(
    dist ~ dnorm(mu, sigma),
    mu <- a + b * speed,
    a ~ dnorm(0, 100),
    b ~ dnorm(0,10),
    sigma ~ dunif(0, 30)
  ),data = cars
)
post <- extract.samples(m, n = 1000)
ggtexttable(head(post))
```

现在我们将需要每个后验样本集s对应的观测i的对数似然：
```{r}
dim(cars)
```

```{r}
n_samples <- 1000
ll <- sapply(1:n_samples, function(s){
  mu <- post$a[s] + post$b[s] * cars$speed
  dnorm(cars$dist, mu, post$sigma[s], log = TRUE)
})
dim(ll)
ll[,1]
```
具体解释以下每一行的含义，以第一次循环为例：
```{r}
mu1 <- post$a[1] +post$b[1] * cars$speed
print(mu1)
dnorm(cars$dist, mu1, post$sigma[1], log = TRUE)
```
这里会得到一个50行，1000列的对数似然函数矩阵，矩阵的行对应观测，列对应后验样本。

现在计算相应的lppd，贝叶斯偏差。

（1）我们需要对矩阵的每行取平均，但由于ll是对数后的概率密度，所以需要先用一个指数变换将其还原为原尺度；

（2）然后再对原尺度下的概率密度求和，然后再取对数。我们需要对每一行都进行相似的操作，可以通过log_sum_exp函数很容易实现这一系列操作：指数变换、求和、取对数；

（3）最后只需要将该值减去样本量的对数值即可。

```{r}
n_cases <- nrow(cars)
lppd <- sapply(1:n_cases,function(i) log_sum_exp(ll[i,]) - log(n_samples))
print(lppd)

# 计算第一行的lppd
step1_num <- exp(ll[1,])
step2_num <- log(sum(step1_num))
step3_num <- step2_num - log(n_samples)
print(step3_num)

```
最后键入sum(lppd)就能够得到之前定义的lppd值了。

下面计算有效个数$p_{WAIC}$，这更加直白，只需要计算每个观测对应的样本方差，然后将这些方差相加即可：

```{r}
pWAIC <- sapply(1:n_cases, function(i) var(ll[i,]))
```

通过sum(pWAIC)得到之前定义的$p_{WAIC}$，代码如下：

```{r}
-2 * (sum(lppd) - sum(pWAIC)) 
```
```{r}
WAIC(m)
```

## 6.4.3 用DIC和MAIC估计偏差

到这儿终于把一些重要的内容说完了，回到我们最初的问题上。如果对同一组数据，我们建立了多个模型，那么怎么选择一个最好的模型，怎么比较它们的准确性呢？如果我们仅仅根据训练样本集来判断，可能会倾向于选择更加复杂的模型。

一个比较好的方法是使用信息准则，通过$AIC$、$DIC$和$WAIC$来估计试验样本中的平均偏离以及模型预测准确性。其中$AIC$适用于简单模型且先验概率分布偏平类型的情况；$DIC$和$WAIC$都是通过在后验概率中抽样来计算的，使用范围也更广一些。除了信息准则，还应该对先验概率进行必要则正则化，降低过度拟合出现的风险。

下图给出的是使用信息准则$DIC$和$WAIC$，以及正则化的先验概率的模型在试验样本中的偏离情况。


[![b1E2VA.png](https://s4.ax1x.com/2022/03/01/b1E2VA.png)](https://imgtu.com/i/b1E2VA)


其中黑线标志未正则化的扁平先验概率分布模型，蓝线表示正则化的先验概率。可以看到，正则化对降低试验样本中的预测偏离有显著效果，同时DIC和WAIC都准确的识别出了模型最小偏离是的参数数量。所以，在我们建立和选择模型的时候，**应该同时考虑对先验概率正则化，以及使用信息准则来选择最适合模型**。

# 6.5 使用信息法则

那么怎样根据信息准则来选择模型？是不是我们选择一个模型AIC，DIC或者WAIC值最小的那个，然后丢掉其他模型就可以了呢？这种做法是不妥的。因为被丢到的模型并非一无是处，仍然含有很多模型信息是有用的。这个时候我们就需要进行**模型比较**和**模型平均**。

1）模型比较：这儿的模型比较并不是只比较DIC或者WAIC，而是结合其他其他方面，比如模型的参数估计和后验概率预测情况。

2）模型平均：模型平均并不是对模型参数进行简单的平均，而是根据不同模型权重，利用所有模型进行后验概率预测，可以理解为对预测的平均。

下面我们来具体看模型比较和模型平均。

## 6.5.1 模型比较

我们还是以灵长类母乳数据为例，来研究模型比较。这比较之前，务必去除数据中的不完整观测数据，确保各个模型使用的观测数据都相同。不然，纳入观测数据少的模型通常会得到较小的偏离以及较小的AIC/DIC/WAIC值。

我们要研究母乳(kcal.per.g)和大脑新皮质(neocortex)以及体重(mass)的关系，我们拟合了4个模型，分别是：只有截距项；只纳入一个新皮质；只纳入一个体重；同时纳入新皮质和体重两个因素。

```{r}
data(milk)
d <- milk[complete.cases(milk),] #处理缺失值
d$neocortex <- d$neocortex.perc/100  # 改变新皮质的标度
```

这里需要介绍一种将标准差限定在正数范围内的方法。这里的技巧是估计$\sigma$的对数值，取对数后可以是任何实数。之后只有在似然函数内对其进行指数变换即可。


```{r}
a_start <- mean(d$kcal.per.g)
sigma_start <- log(sd(d$kcal.per.g))

# 只有截距项模型
m6.11 <- map(
    alist(
        kcal.per.g ~ dnorm(a, exp(log_sigma))
    ),
    data = d,
    start = list(a = a_start,
                 log_sigma = sigma_start)
)
# 新皮质
m6.12 <- map(
    alist(
        kcal.per.g ~ dnorm(mu, exp(log_sigma)),
        mu <- a + bn*neocortex
    ),
    data = d,
    start = list(a = a_start,
                 bn = 0,
                 log_sigma = sigma_start)
)
# 体重
m6.13 <- map(
    alist(
        kcal.per.g ~ dnorm(mu, exp(log_sigma)),
        mu <- a + bm*log(mass)
    ),
    data = d,
    start = list(a = a_start,
                 bm = 0,
                 log_sigma = sigma_start)
)
# 新皮质和体重
m6.14 <- map(
    alist(
        kcal.per.g ~ dnorm(mu, exp(log_sigma)),
        mu <- a + bn*neocortex + bm*log(mass)
    ),
    data = d,
    start = list(a = a_start,
                 bn = 0,
                 bm = 0,
                 log_sigma = sigma_start)
)

```

对上面四个模型就行WAIC的比较。可以通过WAIC(m6.14)查看单个模型的WAIC的值：

```{r}
WAIC(m6.14)
```

上面的第一个值是WAIC。注意它是一个负数，没有说偏差不能是负数，越小越好。

第二个值是lppd。第三个值是$p_{WAIC}$

最后一行se是WAIC的标准差

一旦有了模型的WAIC之后，就可以通过该值对模型进行排序。rethinking包提供了一个非常方便的函数原来按照WAIC的值进行排序：

```{r}
(milk.models <- compare(m6.11, m6.12, m6.13, m6.14))
```

表格有6列：

（1）WAIC是每个模型的WAIC值；依据WAIC越小模型越好的法则，可见模型m6.14是最好的；

（2）$p_{WAIC}$是有效参数的个数的估计。该估计提供了关于每个模型拟合样本的灵活度的信息；

（3）$d_{WAIC}$是最大WIAC和最小WAIC之间的差距；

（4）weight是每个模型的**Akaike**权重，这些值是变换后的信息法则值；

（5）SE是WAIC估计的标准差。WAIC是一个估计，在样本量N足够大的情况下，标准差能够很好地近似其对应的不确定性；

（6）dSE是$d_{WAIC}$对应的标准差。因为第一个模型对应的dSE是NA，因为它和自身的差距总是0。

可以对这些值进行可视化，这样就可以更加直观的展示结果：

```{r}
plot(milk.models, SE = TRUE, dSE = TRUE)
```

【黑点是训练样本内每个模型的偏离；空心点是WAIC值；三角点是模型与WAIC值最小模型之间WAIC的差值】

从WAIC的值来看，第四个模型m6.14是最小的。同时还要注意每个模型都有一个权重weight，它的含义是在当前这些模型中，哪一个模型能够做出最好预测的概率大小。比如模型m6.14的权重是0.94，就意味着它有94%的可能性是预测准确的那个模型。但是要注意，它不表示这个模型的预测准确性是94%，如果我们加入了一个更好的模型比较，那么m6.14的权重可能迅速降低到50%以下。

上面我们只是简单比较了不同模型的WAIC的大小，我们说模型比较不仅是比较WAIC一类的信息准则，还要比较各个模型的参数情况。通过比较参数，我们能够知道为什么一个模型有较小的WAIC，以及某个参数在不同的模型中是否稳定。比如，在模型中加入或者删除一个变量，会对模型中另一个变量产生影响吗？

下面对上述4个模型的参数进行比较：

```{r}
coeftab(m6.11,m6.12,m6.13,m6.14)
```
可以看到，当二者都在模型中的时候bn和bm的估计都离0比较远。

通过可视化，可以更加清晰地看到这一点：

```{r}
plot(coeftab(m6.11, m6.12, m6.13, m6.14))
```

## 6.5.3 模型平均

在上面的分析中，我们知道模型m6.14有94%的可能性是四个模型中最好一个，但是还有6%的可能性不是最好的。

但是设想一下，如果该最好模型的权重为0.51，那么我们只选择该模型，把其他3个模型都丢弃掉？显然不是。这个时候我们就需要综合考虑其他模型了。根据每一个模型的权重，去平均它们在预测中的作用。那么该怎么做呢？

1）分别计算每个模型的信息准则，如WAIC

2）计算每个模型的权重

3）计算每个模型的模拟结果

4）根据每个模型权重大小，进行预测


下面我们先对模型m6.14进行模拟作图，为了计算不同权重模型的预测，我们可以使用ensemble()函数来实现：

```{r}
nc.seq <- seq(0.5,0.8, length.out = 30)
d.predict <- list(
  kcal.per.g = rep(0,30),
  neocortex = nc.seq,
  mass = rep(4.5, 30)
)

pred.m6.14 <- link(m6.14, data = d.predict)

mu <- apply(pred.m6.14, 2, mean)
mu.PI <- apply(pred.m6.14, 2, PI)

#作图
plot(kcal.per.g ~ neocortex, d, col=rangi2)
lines(nc.seq,mu, lty = 2)
lines(nc.seq, mu.PI[1,], lty = 2)
lines(nc.seq, mu.PI[2,], lty = 2)

milk.ensemble <- ensemble(m6.11,m6.12,m6.13,m6.14, data = d.predict)

mu <- apply(milk.ensemble$link, 2, mean)
mu.PI <- apply(milk.ensemble$link, 2, PI)
lines(nc.seq, mu)
shade(mu.PI, nc.seq)
```

虚线是模型m6.14单独估计的回归线和参数区间。实线是各个模型平均估计的回归线，阴影部分是各个模型平均估计的参数区间。由于m6.14模型占的权重不大，所以在本例子中，即便是各个模型平均之后，参数估计的变化也不大。但是如果各个模型权重相差不大，那么进行模型平均之后的预测结果可能会与只纳入最好模型的预测结果差别很大了。

# 6.6 总结

总结一下就是：什么是过度拟合，怎样用**正则化先验概率**和**信息准则**来避免过度拟合。

正则化先验概率可以减低建模中的过度拟合，信息准则可以评估过度拟合程度大小。












